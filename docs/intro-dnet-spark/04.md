# 四、用户定义的函数

当我们执行 Apache Spark 代码时。NET 中，我们在 Java 虚拟机中调用方法和类，Apache Spark 根据我们的需求读取、写入、聚集和转换我们的数据。这是完全可能的，也很常见。NET 应用程序永远看不到实际的数据，JVM 处理所有的数据修改。如果 Apache Spark 拥有完成处理所需的所有类和方法，这没什么问题。然而，当我们需要做一些 Apache Spark 不支持的事情时，我们该怎么办呢？答案是用户定义函数(UDF)和用户定义聚合函数(UDAFs)。UDF 和 UDAFs 允许我们将数据放回。然后运行我们在. NET 中能想到的任何处理。

## 一个例子

稍后我们将访问用户定义的聚合函数；UDAFs 处理聚合数据，而不是对数据帧的每一行进行操作。如果您想在不使用本机 Apache Spark 代码的情况下实现自己的 Group By、Sum 或 Count，那么您应该编写一个 UDAF。

在清单 [4-1](#PC1) 中，我们可以看到我们在哪里。NET 代码对每一行进行操作，并返回一个新值，我们传入一列，并返回一个新列。

```cs
let main argv =

    let spark = SparkSession.Builder().GetOrCreate()
    let udfIntToString = Microsoft.Spark.Sql.Functions.Udf<int, string>(fun (id) -> "The id is " + id.ToString())
    let dataFrame = spark.Sql("SELECT ID from range(1000)")
    dataFrame.Select(udfIntToString.Invoke(dataFrame.["ID"])).Show()
    0

Listing 4-2Calling a .NET UDF from Apache Spark in F#

```

```cs
var spark = SparkSession.Builder().GetOrCreate();

Func<Column, Column> udfIntToString = Udf<int, string>(id => IntToStr(id));

var dataFrame = spark.Sql("SELECT ID from range(1000)");

dataFrame.Select(udfIntToString(dataFrame["ID"])).Show();

string IntToStr(int id)
{
    return $"The id is {id}";
}

Listing 4-1Calling a .NET UDF from Apache Spark in C#

```

清单 [4-1](#PC1) 和 [4-2](#PC2) 的输出如下:

```cs
+-------------------------------------+
|System.String <Main>b__0_0(Int32)(ID)|
+-------------------------------------+
|                          The id is 0|
|                          The id is 1|
|                          The id is 2|
|                          The id is 3|
|                          The id is 4|
|                          The id is 5|
|                          The id is 6|
|                          The id is 7|
|                          The id is 8|
|                          The id is 9|
|                         The id is 10|
|                         The id is 11|
|                         The id is 12|
|                         The id is 13|
|                         The id is 14|
|                         The id is 15|
|                         The id is 16|
|                         The id is 17|
|                         The id is 18|
|                         The id is 19|
+-------------------------------------+
only showing top 20 rows

```

## 体系结构

的方式。NET UDFs 的工作方式类似于。NET 驱动程序的工作原理是，Java DotNetRunner 类启动。NET 代码，并打开一个网络套接字，用于代理请求的来回传递。Apache Spark 创建了 UDF 进程，它碰巧认为这是一个 Python 进程。Apache Spark 等待创建一个套接字，然后 Apache Spark 向下发送数据，并期待一些数据作为响应。Apache Spark 拥有这一功能已经有一段时间了，并且是用于 Python 和 R 支持的相同过程。如果你还记得。因为 Apache Spark JVM 进程启动了。NET 进程，我们现在有了 JVM。NET 进程，现在是第二个。NET 辅助进程，用于接收 UDF 的数据并调用。NET 代码。

这里需要理解一些复杂性，因为。NET 辅助进程加载。NET 应用程序作为库，并使用反射来查找要作为 UDF 执行的代码。这意味着尽管我们有我们的。NET 进程中，任何 UDF 工作都是在一个单独的进程中执行的，因此 UDF 之外的任何初始化可能都不会发生。任何共享状态都将丢失，因此请继续在 UDF 中处理您需要的内容。在清单 [4-3](#PC4) 中，我们将查看 UDF 以及任何共享状态是如何丢失的。在清单 [4-4](#PC5) 中，我们看到在 F#中我们如何需要显式地将一个变量标记为可变的，这在 F#中通常是不推荐的，即使这样共享的状态也会丢失；具有讽刺意味的是，用 F#编写 UDF 的最简单的方法是遵循没有共享状态的指导。

```cs
let mutable SharedState = 100

[<EntryPoint>]
let main argv =

    let spark = SparkSession.Builder().GetOrCreate();

    let dataFrame = spark.Sql("SELECT ID FROM range(1000)")
    SharedState = 991923
    let addUdf =  Microsoft.Spark.Sql. DataFrameFunctions.VectorUdf<Int64DataFrameColumn, Int64DataFrameColumn>(fun (column) -> column.Add(SharedState));
    dataFrame.Select(dataFrame.["ID"], addUdf.Invoke(dataFrame.["ID"])).Show();

    0

Listing 4-4Explicitly creating a mutable shared variable in F#, the altered, changed state is not available in the UDF process

```

```cs
private static int AddAmount = 100;

private static Int64DataFrameColumn Add100(Int64DataFrameColumn id)
{
    return id.Add(AddAmount);
}

static void Main(string[] args)
{
    var spark = SparkSession.Builder().GetOrCreate();

    var d = spark.Sql("SELECT ID FROM range(1000)");

    AddAmount = 991923;

    var addUdf = VectorUdf<Int64DataFrameColumn, Int64DataFrameColumn>((id) => Add100(id));
    d.Select(d["ID"], addUdf(d["ID"])).Show();
}

Listing 4-3Shared state is lost when running UDFs in C#

```

这里的输出显示即使总的来说。NET 进程中，我们将变量 AddAmount 设置为 991923，新列使用 AddAmount 初始化为 100 的值。

```cs
» spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner ./microsoft-spark-2.4.x-0.12.1.jar dotnet ./Listing4-3.dll
+---+-----------------------------+
| ID|Int32 <Main>b__0_1(Int32)(ID)|
+---+-----------------------------+
|  0|                          100|
|  1|                          101|
|  2|                          102|
|  3|                          103|
|  4|                          104|
|  5|                          105|
|  6|                          106|
|  7|                          107|
|  8|                          108|
|  9|                          109|
| 10|                          110|
| 11|                          111|
| 12|                          112|
| 13|                          113|
| 14|                          114|
| 15|                          115|
| 16|                          116|
| 17|                          117|
| 18|                          118|
| 19|                          119|
+---+-----------------------------+
only showing top 20 rows

```

关于路径有几个重要的部分。第一个是工作进程需要从 [`https://github.com/dotnet/spark/releases`](https://github.com/dotnet/spark/releases) 下载并放在一个目录中。然后应该有一个名为`"DOTNET_WORKER_PROCESS"`的环境变量，它指向目录。在这个目录中，应该有一个名为`Microsoft.Spark.Worker`的可执行文件。

当我们运行我们的。NET 应用程序，然后我们可以使用`"dotnet run --project"`并传递项目 csproj/fsproj 文件或包含项目文件的目录，应用程序将启动。对于 UDF 代码，有一个汇编加载器，这意味着你要么需要调用`"dotnet run Listing4-3.dll"`和 dll 的完整路径，要么与 dll 在同一个目录中。为 UDF 找到正确的路径可能很棘手，但是如果您有一个包含已编译应用程序的目录，并且您从那里运行该应用程序，那么 UDF 程序集加载器应该会找到正确的代码。如果汇编加载程序在运行 UDF 时找不到要加载的 dll，您将看到一条带有`System.IO.FileNotFoundException exception`的错误消息。

如果你没有微软。Spark.Worker 应用程序或`DOTNET_WORKER_PROCESS`配置为 Apache Spark 可以找到进程并执行程序，那么您将得到错误，UDF 将不会运行。

## 表演

Apache Spark 针对性能进行了优化。它使用的文件格式(如 Parquet)针对性能进行了优化，通常使用列数据格式来帮助高效地处理大型数据，跳过当前进程不需要的列。在进程间传递数据会增加性能开销，这是不可避免的。Apache Spark 中最初的 UDF 支持使用了所谓的 Python Pickling，这是一种序列化和反序列化通过连接发送的数据的方法。Python 酸洗是一种非常昂贵的发送数据的方式，并且在行级别上工作，因此如果您有一个 UDF 从包含数百列的数据集中读取一列，那么每一列都会被酸洗并通过连接发送。将数据保存在 JVM 端的应用程序和将数据保存到另一个进程的应用程序之间的性能差异非常显著。

Apache Arrow 的创建是为了使在进程间共享数据更加有效。与 Pickling 不同，Apache Arrow 是一种列格式，因此只有 UDF 使用的列在进程间传输。

如果你不是用 Scala 或 Java 编写，出于性能原因，一般建议总是避免 UDF，但是如果你需要使用它们，并且你关心性能，那么你有一些选择。人们过去常用的一种变通方法是用 Scala 或 Java 编写 UDF 并注册它们，但使用 PySpark 调用 UDF。清单 [4-5](#PC7) 展示了如何在 C#中做到这一点，清单 [4-6](#PC8) 展示了如何在 F#中做到这一点。注意，我们需要一个 Java 类，它位于已经添加到 Apache Spark 实例的“classpath”中的 JAR 文件中，以便执行下面的两个示例。

```cs
    let spark = SparkSession.Builder().GetOrCreate()
    spark.Udf().RegisterJava("java_function

", "com.company.ClassName")
    let dataFrame = spark.Sql("SELECT ID, java_function(ID) as java_function_output FROM range(1000)")
    dataFrame.Select(Microsoft.Spark.Sql.Functions.CallUDF("java_udf", dataFrame.["ID"])).Show();

Listing 4-6Registering a Java UDF and calling that from Spark SQL and from the DataFrame API in F#

```

```cs
var spark = SparkSession.Builder().GetOrCreate();

spark.Udf().RegisterJava("java_function", "com.company.ClassName");

var dataFrame = spark.Sql("SELECT ID, java_function(ID) as java_function_output FROM range(1000)");
dataFrame.Select(CallUDF("java_udf", dataFrame["ID"])).Show();

Listing 4-5Registering a Java UDF and calling that from Spark SQL and from the DataFrame API in C#

```

### 酸洗

如果我们满足于不必担心我们的性能。NET 的 Apache Spark 应用程序，我们必须使用 UDF 来实现我们的目标，我们可以使用 pickling 来调用 UDF。清单 [4-7](#PC9) 和 [4-8](#PC10) 分别显示了如何在 C#和 F#中使用酸洗。我们使用本地类型定义一个函数并直接调用它。尽管在代码中没有使用旧式的酸洗来明确调用这一点，但我们需要意识到，任何数据都将被酸洗，并且对于大型数据集来说可能会很慢。

```cs
let spark = SparkSession.Builder().GetOrCreate()
    let dataFrame = spark.Sql("SELECT ID FROM range(1000)")
    let add100 = Functions.Udf<System.Nullable<int>, int>(fun input -> if input.HasValue then input.Value + 100 else 100 )
    dataFrame.Select(add100.Invoke(dataFrame.["ID"])).Show()
    0

Listing 4-8A pickling UDF in F#

```

```cs
static void Main(string[] args)
{
    var spark = SparkSession.Builder().GetOrCreate();

    var dataFrame = spark.Sql("SELECT ID FROM range(1000)");

    var add100 = Udf<int?, int>((input) => input + 100 ?? 100);

    dataFrame.Select(add100(dataFrame["ID"])).Show();
}

Listing 4-7A pickling UDF in C#

```

### 阿帕契箭

为了提高 UDF 的性能，Apache Spark 实现了对使用 Apache Arrow 在进程间共享数据的支持。这意味着不再需要旧式的酸洗，当我们将一个列传递给 UDF 时，只有这一个列在两个过程之间传递。使用 Apache Arrow 的 UDF 有几种不同的名称，您可能会看到它们被称为矢量化 UDF 或 Pandas UDFs。来创建它们。NET for Apache Spark，我们需要使用 Apache 中定义的列类型，如`Int64DataFrameColumn`或`StringDataFrameColumn`，而不是创建一个接受本机类型的函数。Arrow 获取 Microsoft.Spark 引用的包。

清单 [4-9](#PC11) 和 [4-10](#PC12) 展示了如何创建一个`VectorUDF`，它非常类似于酸洗 UDF，除了我们需要定义类型`VectorUDF`的功能。

```cs
open Microsoft.Data.Analysis
open Microsoft.Spark.Sql
open System

[<EntryPoint>]
let main argv =

    let spark = SparkSession.Builder().GetOrCreate();

    let dataFrame = spark.Sql("SELECT ID FROM range(1000)");

    let add100 = DataFrameFunctions.VectorUdf<Int64DataFrameColumn, Int64DataFrameColumn, Int64DataFrameColumn>(fun first second -> first.Add(second))

    dataFrame.Select(add100.Invoke(dataFrame.["ID"], dataFrame.["ID"])).Show()
    0

Listing 4-10Using the DataFrameFunctions to create a VectorUDF in F#

```

```cs
using Microsoft.Data.Analysis;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.DataFrameFunctions;

namespace Listing4_9
{
    class Program
    {
        static void Main(string[] args)
        {
            var spark = SparkSession.Builder().GetOrCreate();

            var dataFrame = spark.Sql("SELECT ID FROM range(1000)");

            var add100 = VectorUdf<Int64DataFrameColumn, Int64DataFrameColumn, Int64DataFrameColumn>((first, second) => first.Add(second));

            dataFrame.Select(add100(dataFrame["ID"], dataFrame["ID"])).Show();
        }
    }
}

Listing 4-9Using the DataFrameFunctions to create a VectorUDF in C#

```

## 用户定义的聚合函数(UDAFs)

UDAF 类似于矢量化 UDF，因为它们也使用 Apache Arrow 格式，但是 udaf 不是在单个操作中接收整个列并对其进行操作，而是对分组数据集进行操作，每个分组集都被发送到 UDAF 进行处理。结果是每个组都有一个我们定义的输出。

如果我们看表 [4-1](#Tab1) ，我们有一些数据，我们将在名称列上分组。

表 4-1

抽样资料

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

名字

 | 

购买

 | 

费用

 |
| --- | --- | --- |
| 爱德华 | 三明治 | $4.95 |
| 撒拉 | 喝 | $2.95 |
| 爱德华 | 筹码 | $1.99 |
| 爱德华 | 喝 | $3.45 |
| 撒拉 | 三明治 | $8.95 |

我们这里有两个自然组，一个是 Ed，一个是 Sarah。例如，如果我们在 name 列上创建一个 UDAF 组，并传入 Name 和 Cost 列，那么我们的 UDAF 将被一个名为`RecordBatch`的对象调用，将有两个`RecordBatch's`，看起来像表 [4-2](#Tab2) 和 [4-3](#Tab3) 。

表 4-3

第二批记录

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

名字

 | 

费用

 |
| --- | --- |
| 撒拉 | $2.95 |
| 撒拉 | $8.95 |

表 4-2

第一批记录

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

名字

 | 

费用

 |
| --- | --- |
| 爱德华 | $4.95 |
| 爱德华 | $1.99 |
| 爱德华 | $3.45 |

这使我们能够检查传入的所有列，并创建我们的聚合。在清单 [4-11](#PC13) 中，我们看看如何在 C#中使用 UDAF 来处理这些批处理。在我们看过 C#中的例子后，我们将在清单 [4-17](#PC19) 中浏览 F#中的例子。

```cs
static void Main(string[] args)
{
    var spark = SparkSession.Builder().GetOrCreate();

    var dataFrame = spark.Sql(

        "SELECT 'Ed' as Name, 'Sandwich' as Purchase, 4.95 as Cost UNION ALL SELECT 'Sarah', 'Drink', 2.95 UNION ALL SELECT 'Ed', 'Chips', 1.99 UNION ALL SELECT 'Ed', 'Drink', 3.45  UNION ALL SELECT 'Sarah', 'Sandwich', 8.95");

    dataFrame = dataFrame.WithColumn("Cost", dataFrame["Cost"].Cast("Float"));

    dataFrame.Show();
    var allowableExpenses = dataFrame.GroupBy("Name").Apply(new StructType(new[]
        {
            new StructField("Name", new StringType()),new StructField("TotalCostOfAllowableExpenses", new FloatType())
        }), TotalCostOfAllowableExpenses
    );

    allowableExpenses.PrintSchema();
    allowableExpenses.Show();
}

private static RecordBatch TotalCostOfAllowableExpenses(RecordBatch records)
{
    var purchaseColumn = records.Column("Purchase") as StringArray;
    var costColumn = records.Column("Cost") as FloatArray;

    float totalCost = 0F;

    for (int i = 0; i < purchaseColumn.Length; i++)
    {
        var cost = costColumn.GetValue(i);
        var purchase = purchaseColumn.GetString(i);

        if(purchase != "Drink" && cost.HasValue)
            totalCost += cost.Value;
    }

    int returnLength = records.Length > 0 ? 1 : 0;

    return new RecordBatch(
        new Schema.Builder()
            .Field( f => f.Name("Name").DataType(ArrowStringType.Default))
            .Field( f => f.Name("TotalCostOfAllowableExpenses").DataType(Apache.Arrow.Types.FloatType.Default))
            .Build(),
        new IArrowArray[]
        {
            records.Column("Name"),
            new FloatArray.Builder().Append(totalCost).Build()
        }, returnLength);
}

Listing 4-11How to process batches in a UDAF

```

如果我们将它分解，在清单 [4-12](#PC14) 中，我们创建了一个数据帧；这通常通过读入一些数据来完成。然后，我显式地将 Cost 列转换为 float。当我们使用 Apache Arrow 格式时，每个数据类型都必须完全正确。如果有任何错误，数据将不会被正确地序列化和反序列化，Apache Spark 将会崩溃，因此确保您知道每一列是什么数据类型将会对您有所帮助。

```cs
var dataFrame = spark.Sql(
                "SELECT 'Ed' as Name, 'Sandwich' as Purchase, 4.95 as Cost UNION ALL SELECT 'Sarah', 'Drink', 2.95 UNION ALL SELECT 'Ed', 'Chips', 1.99 UNION ALL SELECT 'Ed', 'Drink', 3.45  UNION ALL SELECT 'Sarah', 'Sandwich', 8.95");

dataFrame = dataFrame.WithColumn("Cost", dataFrame["Cost"].Cast("Float"));

Listing 4-12Create a DataFrame and cast the cost column to float

```

在清单 [4-13](#PC15) 中，我们看到如何通过调用现有`DataFrame`上的`GroupBy`函数来创建一个新的`DataFrame`。在我们的 GroupBy 调用中，我们还定义了将从 UDAF 接收的数据帧的结构。当我第一次开始查看 UDAFs 时，我的印象是这是传递给 UDAF 的数据帧的模式，但事实上这是 UDAF 将返回的模式。

```cs
var allowableExpenses = dataFrame.GroupBy("Name").Apply(new StructType(new[]
                {
                    new StructField("Name", new StringType()),new StructField("TotalCostOfAllowableExpenses", new FloatType())
                }), TotalCostOfAllowableExpenses
            );

Listing 4-13Calling GroupBy on our existing DataFrame

```

在清单 [4-14](#PC16) 中，我们可以看到如何在我们的 UDAF 中接收记录批，并且我们可以检索我们需要的任何列。

```cs
var purchaseColumn = records.Column("Purchase") as StringArray;
var costColumn = records.Column("Cost") as FloatArray;

Listing 4-14Retrieving rows from the RecordBatch

```

当我们检索列时，我们得到的是数组，我们可以迭代到例子中；请记住，我们收到的行是针对每个唯一组的。

在清单 [4-15](#PC17) 中，我们有自己的定制处理，在这一点上，您可以做任何对您的应用程序有意义的处理。在这个例子中，我们遍历所有的行，并对任何不是“饮料”的购买成本求和。

```cs
float totalCost = 0F;

for (int i = 0; i < purchaseColumn.Length; i++)
{
    var cost = costColumn.GetValue(i);
    var purchase = purchaseColumn.GetString(i);

    if(purchase != "Drink" && cost.HasValue)
        totalCost += cost.Value;
}

Listing 4-15Processing RecordBatch’s to include our custom logic

```

需要记住的关键一点是，我们根本不必关心 name 列。分组全部由 Apache Spark 处理，所以我们可以保持一个运行总数。这意味着为每个组计算不同的值很简单，但反过来，这意味着我们无法在不同的组之间共享状态。

在清单 [4-16](#PC18) 中，我们返回 RecordBatch 的数据。在这种情况下，我们返回允许项目的名称和总成本。您可以在这里返回您喜欢的任何内容，但是您将不能在每个组中返回一行以上的内容。

```cs
int returnLength = records.Length > 0 ? 1 : 0;

            return new RecordBatch(
                new Schema.Builder()
                    .Field( f => f.Name("Name").DataType(ArrowStringType.Default))
                    .Field( f => f.Name("TotalCostOfAllowableExpenses").DataType(Apache.Arrow.Types.FloatType.Default))
                    .Build(),
                new IArrowArray[]
                {
                    records.Column("Name"),
                    new FloatArray.Builder().Append(totalCost).Build()
                }, returnLength);
        }

Listing 4-16Returning data to Apache Spark from the UDAF

```

我们返回的数据是 Apache Arrow 格式的，因此，我们需要使用`Schema.Builder`来创建模式和字段，然后将数据作为一个`IArrowArray`对象的数组传入。在这个例子中，对于数据，我们精确地按照传递给我们的方式传递 name 列，但是对于 cost 列，我们使用`Builder`创建一个新的`FloatArray`，并追加总成本。遵循这种模式意味着对于组中的每个项目，我们会收到许多行，但只返回一行，即聚合数据。

这一开始可能会很混乱，但关键是对于 UDAF，您每次接收每个组的一组记录，并且您为正在处理的组中的所有条目返回一个记录。

在清单 [4-17](#PC19) 中，我们有相同的例子，但是在 F#中。

```cs
open Apache.Arrow
open Apache.Arrow.Types
open Microsoft.Spark.Sql
open Microsoft.Spark.Sql.Types

let totalCostOfAllowableItems(records: RecordBatch): RecordBatch =

    let nameColumn  = records.Column "Name" :?> StringArray
    let purchaseColumn = records.Column "Purchase" :?> StringArray
    let costColumn = records.Column "Cost" :?> FloatArray

    let shouldInclude (purchase) = purchase <> "Drink"

    let count() =
        let mutable costs : float32 array  = Array.zeroCreate purchaseColumn.Length
        for index in 0 .. purchaseColumn.Length - 1 do
            costs.SetValue((if shouldInclude (purchaseColumn.GetString(index)) then costColumn.GetValue(index).Value else float32(0)), index)

        costs |> Array.sum

    let returnLength = if records.Length > 0 then 1 else 0

    let schema = Schema.Builder()
                     .Field(
                        Field("Name", StringType.Default, true))
                     .Field(
                        Field("TotalCostOfAllowableExpenses", FloatType.Default, true)
                        )
                        .Build()

    let data: IArrowArray[] = [|
        nameColumn
        (FloatArray.Builder()).Append(count()).Build()
    |]

    new RecordBatch(schema, data, returnLength)

[<EntryPoint>]
let main argv =

    let spark = SparkSession.Builder().GetOrCreate();

    let dataFrame = spark.Sql("SELECT 'Ed' as Name, 'Sandwich' as Purchase, 4.95 as Cost UNION ALL SELECT 'Sarah', 'Drink', 2.95 UNION ALL SELECT 'Ed', 'Chips', 1.99 UNION ALL SELECT 'Ed', 'Drink', 3.45  UNION ALL SELECT 'Sarah', 'Sandwich', 8.95")
    let dataFrameWithCost = dataFrame.WithColumn("Cost", dataFrame.["Cost"].Cast("Float"))

    dataFrameWithCost.Show()

    let structType = StructType ([|
        StructField("Name", StringType())
        StructField("TotalCostOfAllowablePurchases", FloatType())
        |])

    let categorized = dataFrameWithCost.GroupBy("Name").Apply(structType, totalCostOfAllowableItems)
    categorized.PrintSchema();
    categorized.Show();

    0

Listing 4-17User-Defined Aggregate Function in F#

```

## 调试用户定义的函数

因为我们有 Apache Spark 启动一个或多个独立进程来处理用户定义函数的概念，这意味着很难在 Visual Studio 中进行调试。那个。NET for Apache Spark 项目包括微软。被触发时调用。NET `Debugger.Launch()`方法，该方法暂停进程并显示附加调试器的提示，您可以在这里选择 Visual Studio 实例。不幸的是，尽管我找不到任何文档来证实这一点，但我无法让`Debugger.Launch()`方法在 macOS 或 Linux 上做任何事情，所以除非你在 Windows 上，否则你可能会发现不可能在调试器中调试 UDF 或 UDAFs。相反，您需要退回到创建日志文件和将详细信息写到磁盘这样的事情。使用`Console.WriteLine()`甚至没有任何用处，因为输出被 Apache Spark 吞噬了，没有显示出来。

要启用`Debugger.Launch()`，您可以将它添加到您的 UDF 或 UDAF 代码中，这将触发 UI 来允许您选择调试器，或者您可以将环境变量“DOTNET_WORKER_DEBUG”设置为 1。当工作进程启动时，如果环境变量存在并被设置为 1，那么工作进程会为您调用`Debugger.Launch()`。

## 摘要

仅仅关于用户定义的函数和用户定义的集合函数，就有很多东西需要理解。关键的要点是，在可能的情况下，我们应该完全避免函数的用户代码。如果我们可以完全不用代理任何数据就完成我们的处理，那么这将是最快和最简单的。如果我们关心性能，但是需要我们的自定义代码，那么我们应该避免旧的酸洗 UDF，并确保我们使用 VectorUDF 类。