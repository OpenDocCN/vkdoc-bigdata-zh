# 九、结构化流

在这一章中，我们将看一个如何创建流应用程序的例子。Apache Spark 的结构化流 API 允许您使用 DataFrame API 来表达您的 Apache Spark 作业。您不是使用静态数据集，而是使用基于 Apache Spark 的可伸缩、容错的流处理引擎来处理微批量数据。

我们将创建的应用程序将做两件事。首先，它将检查特定条件下的每条消息，并允许我们的应用程序发出警报；其次，它将收集 5 分钟内收到的所有数据，汇总数据，并保存数据，以便可以在仪表板中显示。

## 我们的流示例

在本章的示例中，我们将使用 Apache Kafka 主题，通过 Debezium 连接器使用变更数据捕获(CDC)从 Microsoft SQL Server 读取变更。除了微软 SQL Server，这些都是开源产品。配置 Microsoft SQL Server、Apache Kafka 和 Debezium 超出了本章的范围，但是我们将解释如何解析 Apache Kafka 消息。

对整个过程的概述是

1.  数据被写入 SQL Server 数据库。

2.  SQL Server 的变更数据捕获功能可生成变更数据。

3.  Debezium 阅读这些更改，并发布到 Apache Kafka 主题。

4.  我们的应用程序读取 Apache Kafka 主题并处理数据。

应该注意的是，尽管我们将使用 Apache Kafka，但是 Apache Spark 可以从许多不同的源进行流式传输，尽管连接细节和解析不同，但是 Apache Spark 中的处理是相同的。

## 树立榜样

要自己运行该示例，您将需要一个支持变更数据捕获的 SQL Server 实例、一个与 Kafka Connect 一起运行的 Apache Kafka 实例以及用于 SQL Server 的 Debezium 连接器。在清单 [9-1](#PC1) 中，我们展示了 SQL Server 中我们将用作源表的表。

有关配置 SQL Server 变更数据捕获的更多信息，请参见 [`https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server`](https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server) *。*关于卡夫卡和德贝兹姆，见 [`https://debezium.io/documentation/reference/connectors/sqlserver.html`](https://debezium.io/documentation/reference/connectors/sqlserver.html) *。*

```cs
CREATE TABLE dbo.SalesOrderItems
(
     Order_Item_ID      INT IDENTITY(1,1) NOT NULL PRIMARY KEY,
     Order_ID           INT NOT NULL,
     Product_ID         INT NOT NULL,
     Amount INT         NOT NULL,
     Price_Sold         FLOAT NOT NULL,
     Margin FLOAT       NOT NULL
     Order_Date         DATETIME NOT NULL
)

Listing 9-1The source SQL Server table

```

一旦创建了表并在数据库和表上启用了 CDC，我们就可以从 Debezium connector for SQL Server 创建一个连接。Debezium 将为我们创建主题。在本例中，主题将被称为“sql.dbo.SalesOrderItems ”,因为我们在 Debezium 连接器配置中将数据库的名称配置为“sql ”,然后添加表的模式和名称来构建完整的主题名称。如果您创建了一个到单独表的连接，那么该表的名称将用于创建主题。

当数据被写入 SQL Server 表时，Debezium 会读取任何更改并创建一个 JSON 消息，该消息会被发送到 Apache Kafka。在清单 [9-2](#PC2) 中，我们展示了一个示例消息，稍后我们将需要使用 DataFrame API 对其进行解析。

```cs
{
    "schema": {
        "type": "struct",

        ],
        "optional": false,
        "name": "sql.dbo.SalesOrderItems.Envelope"
    },
    "payload": {
        "before": null,
        "after": {
            "Order_ID": 1,
            "Order_Item_ID": 737,
            "Product_ID": 123,
            "Amount": 10,
            "Price_Sold": 1000.23,
            "Margin": 0.99
        },
        "source": {
            "version": "1.3.0.Final",
            "connector": "sqlserver",
            "name": "sql",
            "ts_ms": 1602915585290,
            "snapshot": "false",
            "db": "Transactions",
            "schema": "dbo",
            "table": "SalesOrderItems",
            "change_lsn": "0000002c:00000c60:0003",
            "commit_lsn": "0000002c:00000c60:0004",
            "event_serial_no": 1
        },
        "op": "c",
        "ts_ms": 1602915587594,
        "transaction": null
    }
}

Listing 9-2A sample Apache Kafka message from the Debezium SQL Server connector. The schema section has been removed to keep the size of the listing reasonable

```

JSON 消息由一个模式、包含前后数据的有效负载和源信息(如事务时间)组成。在本例中，payload 部分有一个空的“before”对象，因为这是一个插入。如果是更新，那么 before 部分就会有数据。

## 流媒体应用

在清单 [9-3](#PC3) 和 [9-4](#PC4) 中，我们将展示如何使用 SparkSession 从 Apache Kafka 主题创建 DataFrame。我们对连接和主题信息进行了硬编码，但是您可能会从命令行参数或配置文件中读取这些信息。

```cs
let spark = SparkSession.Builder().GetOrCreate();

let rawDataFrame = spark.ReadStream()
                   |> fun stream -> stream.Format("kafka")
                   |> fun stream -> stream.Option("kafka.bootstrap.servers", "localhost:9092")
                   |> fun stream -> stream.Option("subscribe", "sql.dbo.SalesOrderItems")
                   |> fun stream -> stream.Option("startingOffset", "earliest")
                   |> fun stream -> stream.Load()

Listing 9-4Creating a DataFrame from an Apache Kafka topic in F#

```

```cs
var spark = SparkSession.Builder().GetOrCreate();

var rawDataFrame = spark.ReadStream().Format("kafka")
    .Option("kafka.bootstrap.servers", "localhost:9092")
    .Option("subscribe", "sql.dbo.SalesOrderItems")
    .Option("startingOffset", "earliest").Load();

Listing 9-3Creating a DataFrame from an Apache Kafka topic in C#

```

我们传入的“startingOffset”选项决定了查询开始时的起始点。有关可用选项及其描述的完整列表，请参见 [`https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html`](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html) 。

我们在这里创建的“rawDataFrame”将读取 Apache Kafka 主题，以便可以在 DataFrame 上构建正确的列，但此时不会包含任何数据，事实上，如果我们尝试执行`rawDataFrame.Show()`，我们将会得到一条错误消息，显示为`"Queries with streaming sources must be executed with writeStream.start()"`。然而，我们可以做一个 PrintSchema `()`，DataFrame 模式应该是这样的:

```cs
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

```

数据在“值”列中，是我们在清单 [9-3](#PC3) 中看到的 JSON 消息的二进制表示。

## 粉碎 JSON 文档

因为 JSON 在 DataFrame 列中，并且我们已经有了 DataFrame 中的数据，所以我们想使用 Apache Spark 分解 JSON 文档并检索我们想要的实际列。我们需要提供一个允许 Apache Spark 读取 JSON 的模式。在清单 [9-5](#PC6) 和 [9-6](#PC7) 中，我们展示了如何创建`StructType`模式定义。

```cs
let messageSchema() =
    StructType(
        [|
            StructField("schema", StringType())
            StructField("payload", StructType(
                [|
                    StructField("after", StructType(
                        [|
                            StructField("Order_ID", IntegerType())
                            StructField("Product_ID", IntegerType())
                            StructField("Amount", IntegerType())
                            StructField("Price_Sold", FloatType())
                            StructField("Margin", FloatType())
                        |]
                    ))
                    StructField("source", StructType(
                        [|
                            StructField("version", StringType())
                            StructField("ts_ms", LongType())
                         |]
                    ))
                |]
            ))
        |]
    )

Listing 9-6Creating a StructType schema definition in F#

```

```cs
var jsonSchema = new StructType(
    new List<StructField>
    {
        new StructField("schema", new StringType()),
        new StructField("payload", new StructType(
            new List<StructField>
            {
                new StructField("after", new StructType(
                    new List<StructField>
                    {
                        new StructField("Order_ID", new IntegerType()),
                        new StructField("Product_ID", new IntegerType()),
                        new StructField("Amount", new IntegerType()),
                        new StructField("Price_Sold", new FloatType()),
                        new StructField("Margin", new FloatType())
                    })),
                new StructField("source", new StructType(new List<StructField>
                {
                    new StructField("version", new StringType()),
                    new StructField("ts_ms", new LongType())
                }))
            }))
    }

);

Listing 9-5Creating a StructType schema definition in C#

```

需要注意的重要一点是，您只需要提供您感兴趣的文档部分的细节。例如，文档的 schema 部分对我们理解文档很有用，但是我们不能在 Apache Spark 中使用它，因为我们需要 schema 来读取 schema，所以在我们的`StructType` schema 中，我们将整个部分标记为`StringType`,所以它被存储为一个字符串，我们可以选择读取或不读取它。对于我们感兴趣的文档部分，即交易时间“ts_ms”和订单细节，我们确实需要提供一个特定的模式。应该注意的是，如果您遗漏了一列，那么 Apache Spark 将会忽略它，但是如果您提供了不正确的数据类型，那么整行都将为空，即使文档中的其他值具有正确的数据类型。

## 创建数据帧

在清单 [9-7](#PC8) 和 [9-8](#PC9) 中，我们将获取我们创建的指向 Apache Kafka 主题的数据帧以及我们的`StructType`模式，并创建一个数据帧，Apache Spark 将 JSON 文档分解成我们可以使用的实际数据帧列。

```cs
let parsedDataFrame = rawDataFrame
                        |> fun dataFrame -> dataFrame.SelectExpr("CAST(value as string) as value")
                        |> fun dataFrame -> dataFrame.WithColumn("new", Functions.FromJson(Functions.Col("value"), messageSchema().Json))
                        |> fun dataFrame -> dataFrame.Select("new.payload.after.*", "new.payload.source.*")
                        |> fun dataFrame -> dataFrame.WithColumn("timestamp", Functions.Col("ts_ms").Divide(1000).Cast("timestamp"))

Listing 9-8Shredding the JSON document into DataFrame Columns in F#

```

```cs
var parsedDataFrame = rawDataFrame
    .SelectExpr("CAST(value as string) as value")
    .WithColumn("new", FromJson(Col("value"), messageSchema.Json))
    .Select("value", "new.payload.after.*", "new.payload.source.*")
    .WithColumn("timestamp", Col("ts_ms").Divide(1000).Cast("timestamp"));

Listing 9-7Shredding the JSON document into DataFrame Columns in C#

```

这里要注意的是，我们采用二进制“值”列并将其转换为字符串，然后我们使用`FromJson`函数，并结合我们的模式。`FromJson` Apache Spark 函数将为我们模式中的每个属性创建一列，我们使用 JSON 路径“new.payload.after.*”选择数据，这将为模式中“after”对象下指定的每个类型提供一列，名称将是属性名，如“Order_ID”和“Margin”。

Microsoft Change Data Capture 提供的时间戳需要除以 1000，以便我们可以将其转换为具有中正确日期和时间的时间戳。

此时，我们仍然在使用从 Apache Kafka 主题创建的原始 DataFrame。在没有任何数据的情况下，我们还没有开始从任何地方传输任何数据。

## 开始流

在清单 [9-9](#PC10) 和 [9-10](#PC11) 中，我们要做的下一件事是启动一个流并使用`ForeachBatch`方法，它将为 Apache Spark 结构化流提供给我们的应用程序的每个微批处理运行一次。我们将使用这个微批处理来检查每一行，如果销售的商品符合特定条件，就会触发警报。

```cs
let handleStream (dataFrame:DataFrame, _) : unit  =
    dataFrame.Filter(Functions.Col("Margin").Lt(0.10))
    |> fun failedRows -> match failedRows.Count() with
        | 0L -> printfn "We had no failing rows"
            failedRows.Show()
        | _ -> printfn "Trigger Ops Alert Here"

let operationalAlerts = parsedDataFrame.WithWatermark("timestamp", "30 seconds")
    |> fun dataFrame -> dataFrame.WriteStream()
    |> fun stream -> stream.ForeachBatch(fun dataFrame batchId -> handleStream(dataFrame,batchId))
    |> fun stream -> stream.Start()

Listing 9-10Using ForeachBatch to process each micro-batch looking for specific conditions in F#

```

```cs
var operationalAlerts = parsedDataFrame
    .WriteStream()
    .Format("console")
    .ForeachBatch((df, id) => HandleStream(df, id))
    .Start();

private static void HandleStream(DataFrame df, in long batchId)
{
    var tooLowMargin = df.Filter(Col("Margin").Lt(0.10));

    if (tooLowMargin.Count() > 0)
    {
        tooLowMargin.Show();
        Console.WriteLine("Trigger Ops Alert Here");
    }

}

private static void HandleStream(DataFrame df, in long batchId)
{
    var tooLowMargin = df.Filter(Col("Margin").Lt(0.10));

    if (tooLowMargin.Count() > 0)
    {
        tooLowMargin.Show();
        Console.WriteLine("Trigger Ops Alert Here");
    }
}

Listing 9-9Using ForeachBatch to process each micro-batch looking for specific conditions in C#

```

在`HandleStream`中，我们展示了我们可以开始使用 DataFrame API 和我们期望的熟悉方法，例如 Filter 和 show，来构建我们的应用程序，就像我们编写批处理模式应用程序一样。

这里需要注意的两件事是`WithWatermark`函数和`WriteStream`函数。`WithWatermark`函数允许 Apache Spark 确保迟交的消息不会被丢弃。在这个系统中，我们只关心最近的数据，所以如果任何消息在 30 秒后到达，那么它们可能会被丢弃。如果这是一个关键的业务流程，那么您可能会增加保证消息传递的时间。您选择的时间长度是在使用更多内存、更长的窗口(您更有可能收到所有消息)和更少的内存(如果存在基础结构问题或其他问题，消息可能会丢失)之间进行权衡。

第二个函数是`WriteStream`，它启动实际的流处理，并导致任何写入 Apache Kafka 主题的消息被引入 Apache Spark 实例并进行处理。在我们调用`WriteStream`之前，我们不会收到任何实际消息。

这是我们的流应用程序的第一部分，在这里我们实时处理消息并采取一些行动。我们展示的操作非常简单，但是您可以运行更复杂的命令，包括加入静态数据集甚至其他流，因此支持加入流到流作业。

## 汇总数据

在清单 [9-11](#PC12) 和 [9-12](#PC13) 中，我们展示了应用程序的第二部分将获取一段时间内收到的所有消息，并聚合数据以便显示在仪表板中。这是流式应用程序的另一个常见用例，因为它允许业务用户实时查看趋势，而不必等待每小时甚至每天的批处理过程来运行和更新他们的仪表板和报告。

```cs
let totalValueSoldByProducts = parsedDataFrame.WithWatermark("timestamp", "30 seconds")
                                |> fun dataFrame -> dataFrame.GroupBy(Functions.Window(Functions.Col("timestamp"), "5 minute"), Functions.Col("Product_ID")).Sum("Price_Sold")
                                |> fun dataFrame -> dataFrame.WithColumnRenamed("sum(Price_Sold)", "Total_Price_Sold_Per_5_Minutes")
                                |> fun dataFrame -> dataFrame.WriteStream()
                                |> fun stream -> stream.Format("parquet")
                                |> fun stream -> stream.Option("checkpointLocation", "/tmp/checkpointLocation")
                                |> fun stream -> stream.OutputMode("append")
                                |> fun stream -> stream.Option("path", "/tmp/ValueOfProductsSoldPer5Minutes")
                                |> fun stream -> stream.Start()

Listing 9-12Aggregating time slices of data in real time using F#

```

```cs
var totalByProductSoldLast5Minutes = parsedDataFrame.WithWatermark("timestamp", "30 seconds")
    .GroupBy(Window(Col("timestamp"), "5 minute"), Col("Product_ID")).Sum("Price_Sold")
    .WithColumnRenamed("sum(Price_Sold)", "Total_Price_Sold_Per_5_Minutes")
    .WriteStream()
    .Format("parquet")
    .Option("checkpointLocation", "/tmp/checkpointLocation")
    .OutputMode("append")
    .Option("path", "/tmp/ValueOfProductsSoldPer5Minutes")
    .Start();

Listing 9-11Aggregating time slices of data in real time using C#

```

在这个例子中，同样，我们有了`WithWatermark`，但是在我们调用`WriteStream`之前，我们还在数据帧上有一个聚合。一般的方法是，在调用`WriteStream`之前，我们定义我们想要对数据做什么，然后 Apache Spark 将负责运行聚合，然后为我们写出数据。

等待流数据出现是使用 Apache Spark 的一种不同方式，并且可能更难解决它为什么不能按预期工作的问题，因此通常更容易的是让 DataFrame 操作使用静态数据集，然后将代码复制到您的流应用程序。

当我们聚合数据时，我们还使用 Apache Spark `Window`函数，该函数接受包含可以使用的时间戳的列的名称，以及应该聚合数据的时间长度。在本例中，我使用了“5 分钟”，这意味着对于我们接收数据的每 5 分钟时间段，我们将运行聚合并将数据保存到文件系统。

## 查看输出

在清单 [9-11](#PC12) 和 [9-12](#PC13) 中，在使用`WriteStream`启动流之后，我们指定我们想要将数据作为 parquet 写入并附加到任何已经存在的数据。如果我们运行该程序并将一些数据写入 SQL Server 表，我们应该会看到以下格式的数据被写入磁盘:

```cs
+------------------------------------+----------+-------------------------+
|                         window|Product_ID|Total_Price_Sold_Per_5_Minutes|
+------------------------------------+----------+-------------------------+
|[2020-10-16 06:05:00, 2020-10-16 06:10:00]|    123|       12002.759765625|
|[2020-10-16 06:55:00, 2020-10-16 06:00:00]|    123|       6001.3798828125|
|[2020-10-16 06:00:00, 2020-10-16 06:05:00]|    123|      9002.06982421875|
|[2020-10-16 06:10:00, 2020-10-16 06:15:00]|    123|      9002.06982421875|
+------------------------------------+----------+-------------------------+

```

当我们调用 StartStream 时，会在后台创建一个线程，处理会移动到该线程上。这意味着现有的 main 函数将会结束，我们的应用程序将会停止，所以我们需要确保我们的应用程序和流一样长。在清单 [9-13](#PC15) 和 [9-14](#PC16) 中，我们展示了如何使用 stream `AwaitTermination`方法保持流程活动，直到流停止。

```cs
[|
 async {

     operationalAlerts.AwaitTermination()
 }
 async{
    totalValueSoldByProducts.AwaitTermination()
 }
 |]
    |> Async.Parallel
    |> Async.RunSynchronously
    |> ignore

Listing 9-14Keeping the process alive until the streams terminate in F#

```

```cs
Task.WaitAll(
    Task.Run(() => operationalAlerts.AwaitTermination()),
    Task.Run(() => totalByProductSoldLast5Minutes.AwaitTermination())
);

Listing 9-13Keeping the process alive until the streams terminate in C#

```

如果我们运行我们的应用程序，那么我们将会看到大约每 5 分钟写入一次聚合，并且在屏幕上显示任何没有通过测试的行。要运行该应用程序，您需要为您的 Apache Spark 版本传递“Spark-SQL-Kafka”JAR 文件的名称。

## 摘要

在本章中，我们已经了解了如何使用 DataFrame API 来创建流应用程序，这种应用程序使用两种常见模式，一种是单独处理每个批处理并采取一些措施，另一种是使用 Apache Spark 来聚合流数据，以便在报告和仪表板中使用。

与 DataFrame API 类似，结构化流 API 提供了一个易于使用的接口，这在技术上很难做好，而 Apache Spark 使它几乎无缝。