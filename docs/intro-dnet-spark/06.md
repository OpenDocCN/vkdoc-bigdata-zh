# 六、Spark SQL 和 Hive 表

在这一章中，我们将看看 Apache Spark SQL API。SQL API 允许我们编写符合 ANSI SQL:2003 子集的查询，这是 SQL 数据库查询语言的标准。SQL API 意味着我们可以将数据存储在文件中，可能存储在数据湖中，并且我们可以编写访问数据的 SQL 查询。

在 Apache Spark 之前，Apache Hive 是由脸书创建的，作为一种对存储在 Hadoop 甚至 Hadoop 分布式文件系统(HDFS)中的数据运行 SQL 查询的方式。Apache Hive 由一个“metastore”和一个查询引擎组成，metastore 是一组关于文件的元数据，允许开发人员读取它们，就像它们是数据库中的表一样，查询引擎将 SQL 查询转换为可以对存储在 HDFS 中的文件执行的 map/reduce 作业。

当 Apache Spark 第一次发布时，它有 RDD API，没有 SQL 支持，但是当 Apache Spark 2.0 发布时，它包括了一个 SQL 解析器和到 Apache Hive metastore 的连接。这意味着 Apache Spark 能够使用自己的“catalyst”引擎运行 SQL 查询，同时使用 Apache Hive metastore 来存储读写文件所需的元数据。

## 什么是 SQL API

当我们使用。对于 Apache Spark API，我们通常有数据帧。我们要么从文件中读取它们，要么创建新的文件，但这些是我们工作的操作单元，将它们传递给 Apache Spark，转换并再次写回。

我们可以使用 SQL 来访问我们存储的任何数据，而不是直接从文件中读取或向文件中写入数据，我们已经在这些数据中创建了指向这些文件的元数据。

在清单 [6-1](#PC1) 中，我们将看到如何获取一个 CSV 文件，将其注册为 Hive metastore 中的一个表，然后使用 SQL 查询读取该文件的内容。

```cs
var spark = SparkSession.Builder().GetOrCreate();
spark.Sql("CREATE TABLE Users USING csv OPTIONS (path './Names.csv')");

spark.Sql("SELECT * FROM Users").Show();

Listing 6-1Create a table in the Hive metastore, pointing to a file on disk

```

当我们执行这个程序时，我们看到文件的内容:

```cs
» ./RunListing.sh 6 01
+------+
|   _c0|
+------+
|    Ed|
|  Bert|
|  Mary|
|Martha|
+------+

```

Apache Spark DataFrame 和 SQL APIs 与大多数现代数据库系统有一个相似的特性，即它会生成一个计划，并且有一种方法可以查看为 Apache Spark 如何执行查询而生成的计划。在清单 [6-2](#PC3) 中，我们将查看通过运行前面的 SQL 语句以及通过使用 DataFrame API 读取同一文件生成的计划，我们将看到生成的实际计划是相同的。

```cs
var spark = SparkSession.Builder().GetOrCreate();
spark.Sql("CREATE TABLE Users USING csv OPTIONS (path './Names.csv')");

spark.Sql("SELECT * FROM Users").Explain();
spark.Read().Format("csv").Load("./Names.csv").Explain();

Listing 6-2Comparing plans from the SQL and DataFrame API

```

当我们执行这个程序时，我们看到以下输出:

```cs
== Physical Plan ==
*(1) FileScan csv default.users[_c0#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string>
== Physical Plan ==
*(1) FileScan csv [_c0#22] Batched: false, Format: CSV, Location: InMemoryFileIndex[..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string>

```

除了表名(包含在 SQL 版本的计划中)之外，它们都是相同的，这表明您是通过 DataFrame API 还是 SQL API 访问数据。您仍然会得到相同的执行路径。

## 在上下文之间传递数据

当我们调用`SparkSession.Sql`时，结果在一个数据帧中，所以将数据从 SQL API 传递到您的代码，在那里您可以运行您的标准数据帧调用，这是一个运行 select 语句的问题。如果我们想走另一条路，即获取一个数据帧并使其在 SQL 上下文中可用，那么我们需要执行一个步骤，以便将数据识别为 hive 目录中的一个表。

有许多方法可以让 SQL 使用数据帧。首先，我们可以在 Apache Hive 中创建一个托管表。`DataFrameWriter`对象有一个名为`SaveAsTable`的方法。当我们调用它时，DataFrame 被写成一组 parquet 文件，并被添加到 Apache Hive 目录中。清单 [6-3](#PC5) 和 [6-4](#PC6) 展示了如何获取一个数据帧并将其写成一个 Apache Hive 管理的表。

```cs
let spark = SparkSession.Builder().Config("spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation", "true").GetOrCreate()

    spark.CreateDataFrame([|10;11;12;13;14;15|])
        |> fun dataFrame -> dataFrame.WithColumnRenamed("_1", "ID")
        |> fun dataFrame -> dataFrame.Write().SaveAsTable("saved_table")

    spark.Sql("select * from saved_table").Show()

Listing 6-4Writing a DataFrame as a managed Apache Hive table in F#

```

```cs
var spark = SparkSession.Builder().Config("spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation", "true").GetOrCreate();

var dataFrame = spark.CreateDataFrame(new [] {10, 11, 12, 13, 14, 15}).WithColumnRenamed("_1", "ID");

dataFrame.Write().Mode("overwrite").SaveAsTable("saved_table");
spark.Sql("select * from saved_table").Show();

Listing 6-3Writing a DataFrame as a managed Apache Hive table in C#

```

当我们执行这些程序时，我们可以看到数据帧的内容:

```cs
+---+
| ID|
+---+
| 12|
| 15|
| 14|
| 13|
| 11|
| 10|
+---+

```

如果我们看一下输出，我们会发现即使我们创建的数据帧中的数字是以升序排列的，它们现在也是以随机顺序显示的。这是因为当我们写出数据帧时，它在物理上被保存到许多 Parquet 文件中，每个分区一个文件。正如我们在第 [5](05.html) 章中看到的，我们得到多个文件是因为工作在执行者之间被分割的方式。

如果我们看一下文件系统，我运行程序的文件夹有一个 spark-warehouse 目录，其中有一个与我们的表“saved_table”同名的文件夹，最后是一组五个 Parquet 文件。

还有第二件要注意的事情，当我创建`SparkSession`时，我必须传递一个选项，如果文件已经存在，该选项将允许`SaveAsTable`方法物理地覆盖目录中的文件；仅仅在`DataFrameWriter`上设置`Mode("overwrite")`是不够的。但是请注意，如果您使用的是 Apache Spark 3.0 或更高版本，那么您必须删除此选项，因为它会导致 Apache Spark 在配置设置不再有效时抛出异常。

我应该在这里指出，本地 spark-warehouse 来自运行 Apache Spark 的本地实例。在一个环境中，而不是在您的开发人员机器上，我们将正确配置 Apache Hive 仓库，或者用 Databricks 或 AWS Glue 包含一个 Apache Hive 仓库，或者您可以部署和管理自己的 metastore。当然，最终目标并不是在每个开发人员的机器上都有一组 Parquet 文件。

下面四种使数据帧可用于 SQL 查询的方法是数据帧上的以下方法:

*   **CreateTempView**–创建数据帧的临时视图。如果视图已经存在，这将失败。临时视图仅对当前 SparkSession 可用。

*   **createorreplacetenview**–创建数据帧的临时视图。如果视图已经存在，这不会失败。临时视图仅对当前 SparkSession 可用。

*   **CreateGlobalTempView**–创建数据帧的临时视图。如果视图已经存在，这将失败。临时视图可用于当前 SparkSession 和集群上的任何其他 SparkSession。

*   **CreateOrReplaceGlobalTempView**–创建数据帧的临时视图。如果视图已经存在，这不会失败。临时视图可用于当前 SparkSession 和集群上的任何其他 SparkSession。

*   其中,`DataFrameWriter.SaveAsTable`方法被用来在 Apache Hive 中创建一个托管表，其中的数据被物理地写成一组 parquet 文件。这些方法在现有数据上创建视图，因此您不需要将数据写入磁盘作为中间步骤。

*   不同方法的变化是为了允许视图被 Apache Spark 实例的其他用户读取，考虑一下 Databricks 工作区，其中许多用户连接，作业作为不同的用户运行，您可以在会话之间共享数据。如果视图是一个全局视图，那么当我们从中选择时，我们需要用全局视图数据库的名称“global_temp”作为它的前缀，所以如果我们创建一个名为“global_temp_view”的全局视图，我们可以在 SQL 上下文中运行这个查询来读取它:`"select * from global_temp.global_temp_view"`。

*   Create 和 CreateOrReplace 之间的区别决定了您是否可以覆盖现有视图，或者如果视图已经存在，是否会引发异常。

在清单 [6-5](#PC8) 和 [6-6](#PC9) 中，我们展示了如何在 C#和 F#中使用这四个函数。

```cs
let spark = SparkSession.Builder().GetOrCreate()

let dataFrame = spark.CreateDataFrame([|10;11;12;13;14;15|]).WithColumnRenamed("_1", "ID")

dataFrame.CreateTempView("temp_view")
printfn "select * from temp_view:"
spark.Sql("select * from temp_view").Show()

dataFrame.CreateOrReplaceTempView("temp_view")
printfn "select * from temp_view:"
spark.Sql("select * from temp_view").Show()

dataFrame.CreateGlobalTempView("global_temp_view")
printfn "select * from global_temp.global_temp_view:"
spark.Sql("select * from global_temp.global_temp_view").Show()

dataFrame.CreateOrReplaceGlobalTempView("global_temp_view")
printfn "select * from global_temp.global_temp_view:"
spark.Sql("select * from global_temp.global_temp_view").Show()

0

Listing 6-6Using the Create View methods on a DataFrame in F#

```

```cs
var spark = SparkSession.Builder().GetOrCreate();

var dataFrame = spark.CreateDataFrame(new [] {10, 11, 12, 13, 14, 15}).WithColumnRenamed("_1", "ID");

dataFrame.CreateTempView("temp_view");
Console.WriteLine("select * from temp_view:");
spark.Sql("select * from temp_view").Show();

dataFrame.CreateOrReplaceTempView("temp_view");
Console.WriteLine("select * from temp_view:");
spark.Sql("select * from temp_view").Show();

dataFrame.CreateGlobalTempView("global_temp_view");
Console.WriteLine("select * from global_temp.global_temp_view:");
spark.Sql("select * from global_temp.global_temp_view").Show();

dataFrame.CreateOrReplaceGlobalTempView("global_temp_view");
Console.WriteLine("select * from global_temp.global_temp_view:");
spark.Sql("select * from global_temp.global_temp_view").Show();

Listing 6-5Using the Create View methods on a DataFrame in C#

```

运行这两个程序时，它们会显示以下输出:

```cs
select * from temp_view:
+---+
| ID|
+---+
| 10|
| 11|
| 12|
| 13|
| 14|
| 15|
+---+

select * from temp_view:
+---+
| ID|
+---+

| 10|
| 11|
| 12|
| 13|
| 14|
| 15|
+---+

select * from global_temp.global_temp_view:
+---+
| ID|
+---+
| 10|
| 11|
| 12|
| 13|
| 14|
| 15|
+---+

select * from global_temp.global_temp_view:
+---+

| ID|
+---+
| 10|
| 11|
| 12|
| 13|
| 14|
| 15|
+---+

```

## Spark 会话目录

Spark 会议。Catalog 是一个对象，它允许我们检查和修改存储在 Hive metastore 中的元数据。我们可以创建表并列出数据库、表、视图和函数，还可以检查和删除这些相同的对象。在清单 [6-7](#PC11) 和 [6-8](#PC12) 中，我们将使用 SQL 查询创建一个新的数据库，然后查询数据库中的表列表，并使用 catalog 函数检查表上的列。清单使用了我已经生成的一个拼花文件；拼花文件包括三列。

```cs
let spark = SparkSession.Builder().GetOrCreate()
spark.Sql("CREATE DATABASE InputData")
spark.Catalog.SetCurrentDatabase "InputData"
spark.Catalog.CreateTable("id_list", "./ID.parquet")

let getTableDefinition =
    let getColumn(column:Row) =
       sprintf "%s\t%s" (column.[0].ToString()) (column.[2].ToString())

    let getColumns(dbName:string, tableName:string) =
        spark.Catalog.ListColumns(dbName, tableName)
                               |> fun c -> c.Collect()
                               |> Seq.map(fun column -> getColumn(column))
                               |> String.concat "\n"

    let getTable (table:Row) =
        let databaseName = table.[1].ToString()
        let tableName = table.[0].ToString()

        let tableHeader = sprintf "Database: %s, Table: %s" databaseName tableName
        let columnDefinition = getColumns(databaseName, tableName)

        sprintf "%s\n%s" tableHeader columnDefinition

    let tableDefinition =
        spark.Catalog.ListTables "InputData"
        |> fun t -> t.Collect()
        |> Seq.map (fun table -> getTable(table))

    tableDefinition

PrettyPrint.print getTableDefinition

0

Listing 6-8Working with Hive databases and tables in F#

```

```cs
var spark = SparkSession.Builder().GetOrCreate();
spark.Sql("CREATE DATABASE InputData");

spark.Catalog.SetCurrentDatabase("InputData");
spark.Catalog.CreateTable("id_list", "./ID.parquet");

var tables = spark.Catalog.ListTables("InputData");

foreach (var row in tables.Collect())
{
    var name = row[0].ToString();
    var database = row[1].ToString();

    Console.WriteLine($"Database: {database}, Table: {name}");
    var table = spark.Catalog.ListColumns(database, name);
    foreach (var column in table.Collect())
    {

        var columnName = column[0].ToString();
        var dataType = column[2].ToString();
        var nullable = (bool) column[3];
        var nullString = nullable ? "NULL" : "NOT NULL";

        Console.WriteLine($"{columnName}\t{dataType}\t{nullString}");
    }
}

Listing 6-7Working with Hive databases and tables in C#

```

这些程序的输出是

```cs
Database: inputdata, Table: id_list
Id        bigint
Age       bigint
halfAge   double

```

能够探索对象在实践中非常有用，这方面的一个真实例子是我参与的一个项目，该项目涉及从许多源系统接收数据文件，在这些源系统中，我们接收到的数据模式可能会在没有通知的情况下发生变化。我们开发的是一种比较现有模式和传入文件的模式的方法，并确定模式是否可以改进，或者我们是否必须手动修复模式以解决不兼容问题。

`ListDatabases`、`ListTables`和`ListColumns`方法对于探索哪些对象存在是有用的，但是 catalog 也有一些其他的函数来检查对象是否存在，比如`DatabaseExists`、`FunctionExists`和`TableExists`，如果对象存在则返回一个布尔值。

该目录还允许我们删除使用`DropTempView`和`DropGlobalTempView`创建的临时视图。

如果我们知道一个对象存在，我们可以使用`GetDatabase`、`GetFunction`和`GetTable`，它们不返回数据帧，而是一级对象，让我们访问它们的属性。

*   **get Database**–返回一个具有属性描述、名称和位置 Uri 的数据库对象

*   **GetFunction**–返回一个函数对象，它具有数据库、描述、名称、类名和 IsTemporary 属性

*   **GetTable**–返回一个 Table 对象，该对象具有属性 Database、Description、Name、IsTemporary 和 TableType

## 摘要

在本章中，我们了解了 Apache Spark 如何拥有运行 SQL 查询的接口，我们如何从这些 SQL 查询中访问数据帧，以及我们如何管理 SQL 查询引擎可用的表的元数据。

Apache Spark SQL 有一个非常好的特性——完整的 SQL 解析器和函数集；要查看最新的可用函数，请访问位于 [`https://spark.apache.org/docs/latest/api/sql/`](https://spark.apache.org/docs/latest/api/sql/) 的 Apache Spark 文档，记住您可以从。NET 使用`SparkSession.Sql`。