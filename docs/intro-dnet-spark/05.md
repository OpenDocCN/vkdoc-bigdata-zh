# 五、数据帧 API

在本章中，我们将了解 DataFrame API，这是我们将与一起使用的核心 API。NET for Apache Spark。Apache Spark 有几个不同的 API，弹性分布式数据集(RDD)和 DataFrame APIs，用于处理。我们将介绍什么是 API 以及为什么 RDD API 在中不可用。净，这是好的；DataFrame API 给了我们所需要的一切。

## RDD API 与 DataFrame API

弹性分布式数据集(RDD) API 提供了对 RDD 的访问。rdd 是对海量数据文件的抽象，通过对文件进行分区并将处理分散到不同的计算节点上来实现。当 Apache Spark 第一次出现时，RDD API 是唯一可用的 API，使用 Apache Spark 就是使用 RDD API。

DataFrame API 是一个更高层次的抽象，基于分布在 rdd 之上的数据列。`Column`对象包括许多方法，我们可以用它们来更有效地编写数据处理代码。在清单 [5-1](#PC1) 中，我们有一个 Scala RDD 示例，它解析 Apache web 服务器日志，按照用户列对数据进行分组，并计算字节数和请求数。完整的示例来自 Apache Spark 安装，位于 examples/src/main/Scala/org/Apache/Spark/examples/log query . Scala 文件中。该示例使用`map`、`reduceByKey`和`collect`将 UDF 应用于 RDD。在清单 [5-2](#PC2) 中，我们有一个 C#版本的例子，不使用 RDD API，而是使用 DataFrame API 和类似`GroupBy`、`Agg`、`Sum`和`Count`的方法。

```cs
static void Main(string[] args)
{
    Console.WriteLine("Hello World!");
    var regex = @"^([\d.]+) (\S+) (\S+) \[([\w\d:/]+\s[+\-]\d{4})\] ""(.+?)"" (\d{3}) ([\d\-]+) ""([^""]+)"" ""([^""]+)"".*";
    var spark = SparkSession.Builder().AppName("LogReader").GetOrCreate();
    var dataFrame = spark.Read().Text("log.txt");

    dataFrame
        .WithColumn("user", RegexpExtract(dataFrame["value"], regex, 3))
        .WithColumn("bytes", RegexpExtract(dataFrame["value"], regex, 7).Cast("int"))
        .WithColumn("uri", RegexpExtract(dataFrame["value"], regex, 5))
        .Drop("value")
        .GroupBy("user", "uri")
        .Agg(Sum("bytes").Alias("TotalBytesPerUser"), Count("user").Alias("RequestsPerUser"))
        .Show();
}

Listing 5-2The same example rewritten for the DataFrame API in C#

```

```cs
object LogQuery {
  def main(args: Array[String]) {

    val sparkConf = new SparkConf().setAppName("Log Query")
    val sc = new SparkContext(sparkConf)

    val dataSet =
      if (args.length == 1) sc.textFile(args(0)) else sc.parallelize(exampleApacheLogs)
    // scalastyle:off

    val apacheLogRegex =
      """^([\d.]+) (\S+) (\S+) \[([\w\d:/]+\s[+\-]\d{4})\] "(.+?)" (\d{3}) ([\d\-]+) "([^"]+)" "([^"]+)".*""".r
    // scalastyle:on
    /** Tracks the total query count and number of aggregate bytes for a particular group. */
    class Stats(val count: Int, val numBytes: Int) extends Serializable {
      def merge(other: Stats): Stats = new Stats(count + other.count, numBytes + other.numBytes)
      override def toString: String = "bytes=%s\tn=%s".format(numBytes, count)
    }

    def extractKey(line: String): (String, String, String) = {
      apacheLogRegex.findFirstIn(line) match {
        case Some(apacheLogRegex(ip, _, user, dateTime, query, status, bytes, referer, ua)) =>
          if (user != "\"-\"") (ip, user, query)
          else (null, null, null)
        case _ => (null, null, null)
      }
    }

    def extractStats(line: String): Stats = {
      apacheLogRegex.findFirstIn(line) match {
        case Some(apacheLogRegex(ip, _, user, dateTime, query, status, bytes, referer, ua)) =>
          new Stats(1, bytes.toInt)
        case _ => new Stats(1, 0)
      }
    }

    dataSet.map(line => (extractKey(line), extractStats(line)))
      .reduceByKey((a, b) => a.merge(b))
      .collect().foreach{
        case (user, query) => println("%s\t%s".format(user, query))}

    sc.stop()
  }
}
// scalastyle:on println

Listing 5-1Example Scala program using the RDD API

```

列表 [5-1](#PC1) 的输出:

```cs
(10.10.10.10,"FRED",GET http://images.com/2013/Generic.jpg HTTP/1.1)  bytes=621  n=2

```

清单 [5-2](#PC2) 的输出显示相同的数据；然而，输出是一个 DataFrame，我称之为 Show on 来显示数据，而不是作为本地对象的数据(strings、int 等)。)在 Scala 中。

```cs
+------+--------------------+-----------------+---------------+
|  user|                 uri|TotalBytesPerUser|RequestsPerUser|
+------+--------------------+-----------------+---------------+
|"FRED"|GET http://images...|              621|              2|
+------+--------------------+-----------------+---------------+

```

处理时，有两件关于 RDD API 的重要事情需要了解。NET for Apache Spark。首先是 RDD API 无法从。NET，也没有计划让他们可用。RDD 有可能在年实现。NET，但使用 RDD API 从。NET 将意味着必须使用我们在上一章看到的酸洗 UDF 来编写，这将是缓慢的。第二件事是我们不能忽略 RDD API，因为 DataFrame API 和诸如`GroupBy`和`Agg`之类的方法是对 RDD API 的抽象。当您调用 DataFrame API 时，代码进行 RDD 调用，在集群上执行的是 RDD API。

在 Apache Spark 1.x 中，Python 和 Scala/Java 之间的性能差异很大，因为您想在 Python 中执行的每个操作都需要将每一行“腌制”或序列化/反序列化到 Python 中进行处理。在 Apache Spark 2.x 时间框架中，DataFrame API 意味着 Python 程序可以调用 Column 上的 Scala 方法，这些方法反过来调用 RDD 函数，并将数据留在 Java 虚拟机(JVM)端。将数据留在 JVM 端意味着 Python 和 Scala/Java 之间的性能差异非常相似。正是这个 DataFrame API 使得编写成为可能。NET 代码具有相似的性能，所以 RDD API 在。因为性能不会很好，开发人员的体验也不会很好。

## 行动和转变

在我们研究 DataFrame API 并深入研究我们可以用 DataFrame API 做的所有事情之前，我们必须理解动作和转换之间的区别。转换是可能应用于数据帧的东西，而动作将所有先前的转换应用于数据帧。在清单 [5-3](#PC5) 中，我展示了一个 Spark SQL 语句，它将在运行时失败，但是因为没有动作，程序成功完成。

```cs
spark.Sql("select assert_true(false)")

Listing 5-3A successfully completing query that should fail

```

将 false 传递给`assert_true`应该会使程序崩溃，但是当我们运行它时，程序完成了。如果我们添加一个类似于`Show`、`Collect`、`Take`、`Count`、`First`的动作，那么当我们执行程序时，我们会得到一个失败。清单 [5-4](#PC6) 显示了导致运行时评估和后续失败的相同语句。

```cs
spark.Sql("select assert_true(false)").Show()

Listing 5-4An action terminates the statement, which causes the application to crash

```

当操作执行时，会有一个失败。但是，异常显示出错的方法是`"showString"`:

`Unhandled exception. System.Exception: JVM method execution failed: Nonstatic method '` *showString* `' failed for class '6' when called with 3 arguments ([Index=1, Type=Int32, Value=20], [Index=2, Type=Int32, Value=20], [Index=3, Type=Boolean, Value=False], )`

当您得到一个错误，并且异常的细节显示了一个方法时，它通常会分散您对错误的原始原因的注意力，因此重要的是要认识到当您调用一个转换时，它可能是正确的，也可能是不正确的。

在这一点上，人们很可能认为调试一个大型的失败的 Apache Spark 应用程序是不可能的。然而，也不全是坏事。有些操作是经过验证的，例如文件的架构或查询中的列。即使清单 [5-5](#PC7) 没有动作，仍然会有一个失败，因为我们试图使用的列不存在。

```cs
spark.Sql("SELECT ID FROM Range(100)").Select("UnknownColumn")

Listing 5-5Failure will occur without an action under certain circumstances

```

在这种情况下，执行了足够多的 Spark SQL 语句，因此 Apache Spark 知道列 UnknownColumn 无效，因此它将失败并出现异常。

## 数据帧 API

在本节中，我们将开始进一步探索我们可以用 DataFrame API 做什么。有一些定义相当好的类应该研究，这样我们就可以有效地使用 DataFrame API。我们将从`DataFrameReader`开始，它是我们用来将数据读入 Apache Spark 的类，然后我们将看看如何在不读取数据的情况下创建`DataFrames`，然后是`DataFrameWriter`，这是我们如何将处理结果再次写出来，最后更详细地看一下`Column`对象，这在 Apache Spark 中处理数据时非常重要。

### 数据帧阅读器

`DataFrameReader`是允许我们读取文件和数据源的类，然后我们可以用 Apache Spark 处理这些文件和数据源。我们使用一个`SparkSession`到达`DataFrameReader`，清单 [5-6](#PC8) 展示了如何从`SparkSession`中读取并创建一个`DataFrame`，在清单 [5-7](#PC9) 中，我们展示了 F#中的一个`DataFrameReader`。

```cs
let spark = SparkSession.Builder().GetOrCreate()
let reader = spark.Read()
            |> fun reader -> reader.Format("csv")
            |> fun reader -> reader.Option("header", true)
            |> fun reader -> reader.Option("sep", "|")

let dataFrame = reader.Load("./csv_file.csv")
dataFrame.Show()

Listing 5-7Using the DataFrameReader to read data in F#

```

```cs
var spark = SparkSession.Builder().GetOrCreate();

DataFrameReader reader =
    spark.Read().Format("csv").Option("header", true).Option("sep", ",");

var dataFrame = reader.Load("./csv_file.csv");

dataFrame.Show();

Listing 5-6Using the DataFrameReader to read data in C#

```

在 Apache Spark 中，理解我们创建的许多对象(如果不是全部的话，比如 DataFrameReader)是不可变的是很重要的，因此如果您做了类似清单 [5-8](#PC10) 中的事情，我们将不会修改原始对象，也不会得到想要的效果。

```cs
var spark = SparkSession.Builder().GetOrCreate();
var reader = spark.Reader();
reader.Option("header", true);
reader.Option("sep", "|");
reader.Csv("path.csv).Show();

Listing 5-8Each object is immutable, so unless we use method chaining, we could reference the wrong object

```

如果我们运行这段代码，我们在阅读器上设置的选项就会丢失。如果我们想保留它们，那么我们应该使用清单 [5-6](#PC8) 中的方法链接。

#### CSV、检察官、Orc 与负载

有两种方法可以让 Apache Spark 以物理方式读取文件；第一种是在`DataFrameReader`上使用`Load`方法，第二种是调用`Format()`，然后调用`Load()`。清单 [5-9](#PC11) 展示了如何调用特定于格式的方法，清单 [5-10](#PC12) 展示了如何指定格式并调用`Load`。

```cs
Spark.Read().Format("csv").Load("/path/to/.csv")

Listing 5-10Specifying the format of the file and using Load

```

```cs
spark.Read().CSV("/path/to/CSV")

Listing 5-9Using the custom format methods on the DataFrameReader

```

使用 DataFrameReader 的每种方法最终都会得到相同的结果，因此您可以选择使用哪种方法。当我不知道在编写代码时将加载哪种数据格式，或者不知道是否将在运行时提供类型信息(可能是我们随文件一起接收的一些元数据)时，我通常使用 Format/Load 方法。

默认情况下，格式设置为 parquet，所以如果您想使用 Load 方法，那么您要么需要加载一个 parquet 文件，要么首先调用 format。Apache Spark 本身支持表 [5-1](#Tab1) 中列出的格式。不过，也可以为其他数据源添加 JAR 文件，并使用 Format/Load 方法加载文件。清单 [5-11](#PC13) 给出了一个例子，它使用 Format 方法指定一个 avro 文件，清单 [5-12](#PC14) 给出了一个例子，它使用 Format 方法指定一个 Excel XLSX 文件。

表 5-1

Apache Spark 中的本地文件类型支持

<colgroup><col class="tcol1 align-left"></colgroup> 
| 文件类型 |
| 文本 |
| 数据 |
| 镶木地板 |
| 妖魔 |
| 数据库编程 |

有两点需要注意。首先，每种文件类型在`DataFrameReader`上都有一个方法，比如`Text()`、`JSON()`、`Parquet()`等等，这些方法允许加载文件。其次，尽管 Java 数据库连接(JDBC)是一种连接到类似于 ODBC 或 ADO.NET 的数据库的方式，但我们仍然认为它是 Apache Spark 中的一种文件类型，因为 DataFrameReader 对象使用它，并且使用它和使用 DataFrameReader 从文件中获取数据没有区别。

```cs
spark.Format("avro").Load("com.crealytics.spark.excel")

Listing 5-12Using Format to read from an Excel XLSX file

```

```cs
spark.Format("avro").Load("/path/to/avro.avro");

Listing 5-11Using Format to read from an avro file

```

因为 Apache Spark 没有提供这两种格式，所以我们需要向 Apache Spark 传递额外的参数，告诉它加载包含可以处理这两种格式的代码的 JAR 文件。

#### DataFrameReader 选项

在读取文件时，有许多考虑因素和选项可供我们用来加载文件。例如，在一个 CSV 文件中，文件分隔符是什么，是否有标题行？要查看哪些选项适用于哪种文件类型，可以访问`DataFrameReader`的 Apache Spark 文档，访问`csv`、`text`、`json`、`parquet`等文件类型方法，方法描述包含了可用选项的列表: [`https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#csv-scala.collection.Seq-`](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html%2523csv-scala.collection.Seq-) 。

文档显示了可用的选项以及默认值。如果我们以 CSV 为例，我们可以看到今天有 28 个具体选项；默认分隔符(sep)是“”，默认编码是“UTF-8”。

设置选项时，您可以逐个指定它们，也可以传入一个您希望传入的所有选项的`"Dictionary<string, string>"`。当您需要传入一个不是字符串的值类型时，使用字符串表示，比如“true”，Apache Spark 会将它转换为正确的类型。

#### 推断模式与手动指定的模式

有些格式，比如 avro 或 parquet，除了数据之外，还包含了作为定义良好的元数据的模式。JSON 或 CSV 等其他文件格式不包含模式定义，因此 Apache Spark 可以尝试推断模式。对于 CSV 文件，默认情况下不推断模式，但是您可以使用`Option("inferSchema", "true")`来推断模式。JSON 文件总是试图推断模式，除非手动指定了模式。不可能不传入模式，而让 Apache Spark 不推断模式。

有两种方法可以将模式传递给 DataFrameReader，第一种是在 SQL Server 或 Oracle 等传统数据库系统中传递我们称之为 DDL 的内容；我们在清单 [5-13](#PC15) 中演示了这一点。第二种方法是传递所谓的`StructType`，它是模式定义。如果您做任何比 hello world 类型的 Apache Spark 应用程序更复杂的事情，那么您很可能会再次遇到`StructType`类型；清单 [5-14](#PC17) 展示了如何在 C#中将 StructType 传递给 Apache Spark，清单 [5-15](#PC18) 展示了如何在 F#中将 StructType 传递给 Apache Spark。

```cs
var spark = SparkSession.Builder().GetOrCreate();
            var dataFrame = spark.Read().Option("sep", ",").Option("header", "false")
                .Schema("greeting string, first_number int, second_number float")
                .CSV("csv_file.csv");

            dataFrame.PrintSchema();
            dataFrame.

Listing 5-13Passing a DDL string to Apache Spark to specify the schema

```

运行清单 [5-13](#PC15) 产生以下输出:

```cs
let dataFrame = SparkSession.Builder().GetOrCreate()
                |> fun spark -> spark.Read()
                |> fun reader ->
                    reader.Schema
                        (StructType
                            ([| StructField("greeting", StringType())
                                StructField("first_number", IntegerType())
                                StructField("second_number", FloatType()) |]))

                |> fun reader -> reader.Option("sep", ",").Option("header", "false").Csv("csv_file.csv")

dataFrame.PrintSchema()
dataFrame.Show()

Listing 5-15Passing

a StructType to the DataFrameReader to manually specify the schema in F#

```

```cs
var spark = SparkSession.Builder().GetOrCreate();

var schema = new StructType(new List<StructField>()
{
    new StructField("greeting", new StringType()),
    new StructField("first_number", new IntegerType()),
    new StructField("second_number", new FloatType())
});

var dataFrame = spark.Read().Option("sep", ",").Option("header", "false")
    .Schema(schema)
    .Csv("csv_file.csv");

dataFrame.PrintSchema();
dataFrame.Show();

Listing 5-14Passing a StructType to the DataFrameReader to manually specify the schema in C#

```

```cs
root
 |-- greeting: string (nullable = true)
 |-- first_number: integer (nullable = true)
 |-- second_number: float (nullable = true)

+--------+------------+-------------+
|greeting|first_number|second_number|
+--------+------------+-------------+
|   hello|         123|        987.0|
|      hi|         456|        654.0|
+--------+------------+-------------+

```

### 创建数据帧

在一些罕见的情况下，您希望从代码中创建一个`DataFrame`，而不是将数据读入 Apache Spark。有几种创建数据帧的方法。我们可以调用`CreateDataFrame`，也可以使用`SparkSession`运行一些 Spark SQL 来创建一个数据帧，或者使用`SparkSession`创建一个`DataFrame`，其中包含一组使用`Range`方法的连续数字。

#### 创建数据帧

第一种方法是使用`CreateDataFrame`，我们可以向它传递一个特定类型的列表或数组，这将创建一个由数组或列表中的值组成的单个列。或者，我们可以传入一个数组或一个列表`GenericRow`，这将允许我们创建多个列。清单 [5-16](#PC19) 展示了传入单一类型的数组，这创建了一个具有单个列的 DataFrame，还传入了一个 GenericRows 的列表，这也需要将模式指定为 StructType，清单 [5-17](#PC20) 展示了如何使用 F#创建 data frame。

```cs
let spark = SparkSession.Builder().GetOrCreate()

spark.CreateDataFrame([| "a"; "b"; "c" |]).Show()

spark.CreateDataFrame([| true; true; false |]).Show()

spark.CreateDataFrame([| GenericRow([| "hello"; 123; 543.0 |])
                         GenericRow([| "hi"; 987; 456.0 |]) |],
                      StructType
                          ([| StructField("greeting", StringType())
                              StructField("first_number", IntegerType())
                              StructField("second_number", DoubleType()) |]

                          )).Show()

Listing 5-17Creating DataFrames using CreateDataFrame in F#

```

```cs
var spark = SparkSession.Builder().GetOrCreate();

spark.CreateDataFrame(new [] {"a", "b", "c"}).Show();
spark.CreateDataFrame(new [] {true, true, false}).Show();

var schema = new StructType(new List<StructField>()
{
    new StructField("greeting", new StringType()),
    new StructField("first_number", new IntegerType()),
    new StructField("second_number", new DoubleType())
});

IEnumerable<GenericRow> rows = new List<GenericRow>()
{
    new GenericRow(new object[] {"hello", 123, 543D}),
    new GenericRow(new object[] {"hi", 987, 456D})
};

spark.CreateDataFrame(rows, schema).Show();

Listing 5-16Creating DataFrames in C#

```

运行清单 [5-16](#PC19) 和 [5-17](#PC20) 会产生以下输出:

```cs
+---+
| _1|
+---+
|  a|
|  b|
|  c|
+---+

+-----+
|   _1|
+-----+
| true|
| true|
|false|
+-----+

+--------+------------+-------------+
|greeting|first_number|second_number|
+--------+------------+-------------+
|   hello|         123|        543.0|
|      hi|         987|        456.0|
+--------+------------+-------------+

```

请注意，前两个数据帧有一个名为“_1”的列。英寸 NET for Apache Spark 中，`CreateDataFrame`的类型化版本将列表或数组转换成`GenericRow`并创建一个名为“_1”的`StructType`,在将它们传递给 Apache Spark 之前传递列表或数组的数据类型。

一旦有了`DataFrame`，就可以使用`WithColumnRenamed`重命名列，并用新名称替换现有的列名。这在清单 [5-18](#PC22) 中有所展示。

```cs
spark.CreateDataFrame(new [] {"a", "b", "c"}).WithColumnRenamed("_1", "ColumnName").Show();

Listing 5-18Renaming a column using WithColumnRenamed

```

#### Spark SQL

创建数据帧的第二种方法是将一些 SQL 传递给 Apache Spark，如果可以的话，它将创建一个数据帧。清单 [5-19](#PC23) 展示了一些可以传递给 Apache Spark 以生成数据帧的示例 SQL 语句。

```cs
var spark = SparkSession.Builder().GetOrCreate();
spark.Sql("SELECT ID FROM Range(100, 150)").Show();
spark.Sql("SELECT 'Hello' as Greeting, 123 as A_Number").Show();
spark.Sql("SELECT 'Hello' as Greeting, 123 as A_Number union SELECT 'Hi', 987").Show();

Listing 5-19Example Spark SQL statements that will generate DataFrames

```

运行时，输出显示了创建的所有行:

```cs
+---+
| ID|
+---+
|100|
|101|
|102|
|103|
|104|
|105|
|106|
|107|
|108|
|109|
|110|
|111|
|112|
|113|
|114|
|115|
|116|
|117|
|118|
|119|
+---+
only showing top 20 rows

+--------+--------+
|Greeting|A_Number|
+--------+--------+
|   Hello|     123|
+--------+--------+

+--------+--------+
|Greeting|A_Number|
+--------+--------+
|   Hello|     123|
|      Hi|     987|

```

#### 范围方法

从创建数据帧的最后一个选项。NET 将对 SparkSession 对象使用 Range 方法。Range 方法接受一个整数值，Range 返回一个数据帧，其中包含从 0 到传入值之间的所有值，或者您可以给出一个起始值和结束值，您将返回一个包含这两个值之间的所有值的数据帧。清单 [5-20](#PC25) 展示了范围的两种用法，然后我们展示了清单 [5-20](#PC25) 的输出。

```cs
var spark = SparkSession.Builder().GetOrCreate();
spark.Range(5).Show();
spark.Range(10, 12).Show();

Listing 5-20Calling Range on the SparkSession to create a DataFrame

```

这样的输出是

```cs
+---+
| id|
+---+
|  0|
|  1|
|  2|
|  3|
|  4|
+---+

+---+
| id|
+---+
| 10|
| 11|
+---+

```

### 数据帧写入器

`DataFrameWriter`是我们用来再次写回数据的类。它与`DataFrameReader`的相似之处在于，你可以使用`Csv()`、`Parquet()`等使用特定的格式来书写，或者指定格式并使用`Format()`和`Save()`方法。我们直接从 DataFrame 进入 DataFrameWriter，并在清单 [5-21](#PC27) 中展示了一个编写 DataFrame 的例子。

```cs
var spark = SparkSession.Builder().GetOrCreate();
var dataFrame = spark.Range(100);

dataFrame.Write().Csv("output.csv");
dataFrame.Write().Format("json").Save("output.json");

Listing 5-21The DataFrameWriter

```

DataFrameWriter 的工作方式与 DataFrameReader 非常相似。如果您想改变数据的写入方式，那么有一组选项可供您使用。例如，在编写 CSV 文件时，您可以控制分隔符、标题、编码等。在 DataFrameWriter 文档中可以找到编写文件时可以设置的所有可用选项，并查看每种编写方法，如`csv()`、`json()`等:[、`https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameWriter.html#csv-java.lang.String-`、](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameWriter.html%2523csv-java.lang.String-)。

#### 数据帧写入器模式

当我们写数据时，我们可以选择如果有现有数据会发生什么。我们可以选择将数据添加到任何现有数据的末尾。我们可以选择覆盖任何现有数据。如果数据已经存在，我们可以选择什么都不做，最后，如果数据已经存在，我们可以选择引发错误。如果数据已经存在，最后一个出错的模式是默认模式。在清单 [5-22](#PC28) 中，我们展示了所有写模式的一个例子。

```cs
var spark = SparkSession.Builder().GetOrCreate();
var dataFrame = spark.Range(100);

dataFrame.Write().Mode("overwrite").Csv("output.csv");
dataFrame.Write().Mode("ignore").Csv("output.csv");
dataFrame.Write().Mode("append").Csv("output.csv");
dataFrame.Write().Mode("error").Csv("output.csv");

Listing 5-22Apache Spark DataFrameWriter write modes

```

请注意，最后一行将导致异常，因为文件已经存在，如果文件已经存在，“error”将抛出异常。

#### 分区依据

当我们写数据时，我们也可以选择一列或多列来划分数据。这意味着，如果我们有一个看起来像表 [5-2](#Taba) 的数据帧，并且我们选择按年份和国家列进行分区，我们将最终得到每个国家每年一个文件。

表 5-2

抽样资料

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

**国家**

 | 

**年**

 | 

**金额**

 |
| --- | --- | --- |
| 联合王国 | Two thousand and twenty | Five hundred |
| 联合王国 | Two thousand and twenty | One thousand |
| 法国 | Two thousand and twenty | Five hundred |
| 法国 | One thousand nine hundred and ninety | One hundred |
| 联合王国 | One thousand nine hundred and ninety | One hundred |

在清单 [5-23](#PC29) 中，数据被写入，但是按照国家和年份进行分区，我们最终得到的是五个独立的文件，每个国家/年份组合一个文件。比如英国，2020 文件的路径是“output . CSV/Year = 2020/Country = UK/part-randomguid . CSV”。

```cs
var spark = SparkSession.Builder().GetOrCreate();
var dataFrame = spark.CreateDataFrame(new List<GenericRow>()
    {
        new GenericRow(new object[] {"UK", 2020, 500}),
        new GenericRow(new object[] {"UK", 2020, 1000}),
        new GenericRow(new object[] {"FRANCE", 2020, 500}),
        new GenericRow(new object[] {"FRANCE", 1990, 100}),
        new GenericRow(new object[] {"UK", 1990, 100})
    },
    new StructType(
        new List<StructField>()
        {
            new StructField("Country", new StringType()),
            new StructField("Year", new IntegerType()),
            new StructField("Amount", new IntegerType())
        }

    ));

dataFrame.Write().PartitionBy("Year", "Country").Csv("output.csv");

Listing 5-23Partitioning the data when writing it out

```

如果我们在读取数据时像这样对数据进行分区，并希望在 Apache Spark 中过滤数据，如果我们可以对分区的列进行过滤，那么读取效率会高得多。例如，如果我们使用`"spark.Read().Csv("output.csv").Filter("Year = 2020 AND Country = 'UK'").Show();"`，那么分区将被使用，以便只有分区中匹配过滤器的数据将被读入。如果你有很多数据，但只需要其中的一小部分，那么这可以使阅读非常有效。

#### 控制文件名

当我们使用 Apache Spark 写入数据并指定文件和文件名时，例如“c:\temp\output.csv”或“/tmp/output.csv”，我们将得到一个名为“output.csv”的文件夹，在该文件夹中有一个或多个遵循“part-part number-randomguid-jobid . format”命名过程的文件，例如“part-00003-de 71 ce 5c-63aa-4bd 9-863 c-9696 F9 f 86849 . c 0”

最终得到的单个文件的数量取决于您拥有的数据量以及这些数据的分区数量。如果你必须只有一个文件，你可以通过在调用`DataFrameWriter`之前在`DataFrame` *上做一个`Coalesce()`来控制你最终有多少个文件。`Coalesce`将允许您指定写出数据时使用多少分区。*

控制文件名是不可能的，虽然这可能有点混乱和烦人，但这不是一个实际问题。我们写出数据，当我们读回数据时，我们传入文件夹的名称，如果使用分区，Apache Spark 将负责查找目录或任何子目录中的任何文件。

### 列和函数

我们将在本章介绍的 DataFrame API 的最后一部分是 Column 类。与 RDD API 的 map/reduce 类型操作相比，Column 类使得 DataFrame API 如此易于使用。`Column`是方法可用于实际处理数据的地方。请记住，DataFrame API 是基于数据列的，因此很自然地，`Column`类应该是我们处理数据的核心。

`Column`类是一个属于`Microsoft.Spark.Sql.Functions`的静态成员，你既可以使用`Function`类到达`Column`，比如`Functions.Column`，也可以在 C#中使用静态导入“使用静态`Microsoft.Spark.Sql.Functions;`”。Column 也有别名`Col`，所以如果你看到`Column`或者`Col`，它们是可以互换的。

清单 [5-24](#PC30) ，C#和 5-25，F#展示了我们如何使用`Column`来处理一个`DataFrame`中的数据。

```cs
let spark = SparkSession.Builder().GetOrCreate()
let dataFrame = spark.Range(100L)

dataFrame.Select(Functions.Column("ID")).Show()
dataFrame.Select(Functions.Col("ID")).Show()

dataFrame.Select(Functions.Column("ID").Name("Not ID")).Show()
dataFrame.Select(Functions.Col("ID").Name("Not ID")).Show()

dataFrame.Filter(Functions.Column("ID").Gt(100)).Show()
dataFrame.Filter(Functions.Col("ID").Gt(100)).Show()

Listing 5-25Using a Column or Col object in F#

```

```cs
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;

namespace Listing5_24
{
    class Program
    {
        static void Main(string[] args)
        {
            var spark = SparkSession.Builder().GetOrCreate();
            var dataFrame = spark.Range(100);

            dataFrame.Select(Column("ID")).Show();
            dataFrame.Select(Col("ID")).Show();

            dataFrame.Select(Column("ID").Name("Not ID")).Show();
            dataFrame.Select(Col("ID").Name("Not ID")).Show();

            dataFrame.Filter(Column("ID").Gt(100)).Show();
            dataFrame.Filter(Col("ID").Gt(100)).Show();
        }
    }

}

Listing 5-24Using a Column or Col object in C#, including the static using statement to bring the Functions into scope

```

当我们想要访问一个列时，我们可以使用`Function.Col`或`Function.Column`，或者数据帧本身可以使用列名来索引，比如`dataFrame["ColumnName"]`。

要确切了解您可以对列执行什么操作，以及哪些功能在中可用。NET for Apache Spark，可以访问列( [`https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.column?view=spark-dotnet`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.column%253Fview%253Dspark-dotnet) )和函数( [`https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.functions?view=spark-dotnet`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.functions%253Fview%253Dspark-dotnet) )的文档页面。

## 摘要

DataFrame API 是我们如何使用 Apache Spark 读取、处理和写入数据的核心。DataFrame API 是我们如何使用 Apache Spark 的核心。NET，理解什么是数据帧，如何读入数据，使用列和函数进行处理，并再次写回数据，是我们如何以编程方式使用 Apache Spark 的核心。

在下一章中，我们将看看如何通过使用配置单元表使用 SQL 查询来获得 Apache Spark 的强大功能。这种访问 Apache Spark 的不同方法是它吸引许多人的部分原因。想用 Scala/Python/R/编程的人。NET 可以做到这一点，想使用 SQL 的人也可以使用它。我发现自己主要编写代码，但使用 SQL 来探索数据或帮助迁移现有的遗留 SQL 解决方案。