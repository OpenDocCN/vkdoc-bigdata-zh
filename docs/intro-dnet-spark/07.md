# 七、Spark 机器学习 API

在这一章中，我们将看看 Spark 的机器学习 API 或 MLLib API。MLLib API 由基于 RDD 的 API 和较新的 DataFrame API 组成。API 的 DataFrame 版本被称为 ML API，因为对象存在于 org.apache.spark.ml 名称空间中。从这里开始，我们将使用 ML API 这个术语来指代 MLLib API 的 DataFrame 版本。就像。NET for Apache Spark 项目支持 DataFrame API，但不支持 RDD API，迄今为止只有 Spark ML API 有任何实现。

ML API 最初发布时并不是核心项目的一部分，到目前为止只是通过外部贡献来实现，所以它不像其他 API 那样完整。随着时间的推移，ML API 将变得越来越完整，但截至今天，实现的 ML 对象屈指可数。这意味着我们在用编写机器学习应用程序时有一些不同的选择。NET for Apache Spark。

第一选择是我们使用。NET 并使用微软的 ML.NET 库，这意味着你可以使用 C#或 F#创建 ML 模型。为了从 Apache Spark 访问 ML.NET，我们将使用一个用户定义的函数(UDF)将数据传递给 ML.NET 模型。这种方法的缺点是所有数据都必须通过 UDF 传递，但是如果您想用. NET 编写所有代码，这可能是目前最好的选择。

第二个选择是，如果我们没有所需的一切。NET，但可以部分地创建或执行我们的模型，我们可以在。NET，然后保存我们的进度并调用 Scala 或 Python Apache Spark 程序来读取输出。NET 并完成处理。如果您已经拥有 Scala 或 Python 中的现有模型，并且希望将代码移植到. NET 中，那么这种选择会更好。

中实现机器学习应用程序的最后选择。NET 就是自己实现自己需要的对象。根据您要实现的内容，这可能很简单，也可能很难实现。在附录 B 中，我们展示了如何实现可以在项目中使用的对象，或者将这些对象贡献给。用于 Apache Spark 项目的. NET。

## 库命名

具体到命名，最初基于 RDD 的机器学习 API 被命名为 MLLib。在 Apache Spark 2.0 中，创建了“Spark ML”库，该库虽然不是官方名称，但用于指代 DataFrame API，Scala 中的对象是在 org.apache.spark.ml 包中创建的，其中 MLLib 对象以前位于 org.apache.spark.mllib 中。MLLib API 包括 RDD API 和 DataFrame API 的代码，但我们在中实现的对象。至少现在，Apache Spark . NET 将来自 org.apache.spark.ml 包。

当查看 Apache Spark 文档时，要小心的是；两个包中会有同名的对象，例如 org . Apache . spark . ml lib . feature .`Word2Vec`对象，它与 org . Apache . spark . ml . feature .`Word2Vec`对象是分开的，这可能会导致一些混淆，您期望看到一组在对象的 MLLib RDD 版本中不存在的参数，反之亦然。

## 实现的对象

ML API 中创建的第一组对象来自 org.spark.ml.feature API，实现的对象是

*   斗式提升机

*   计数器/计数器模块

*   特征散列器

*   哈希

*   IDF/IDFModel

*   Tokenizer

*   Word2Vec/Word2VecModel

*   SQL 转换器

*   停用词去除器

对于更多的对象，有几个未决的拉请求，所以我希望这个列表将保持增长，虽然速度很慢，但速度稳定，直到我们之间的功能对等。NET 的 Apache Spark 和 Scala 和 Python。

要查看正在实现的 org.apache.spark.ml.feature 对象的进度，请参见本期 GitHub 跟踪进度: [`https://github.com/dotnet/spark/issues/381`](https://github.com/dotnet/spark/issues/381) 。

## 参数

参数是构建机器学习应用程序的基础部分。了解如何控制模型以实现最佳可能结果，以及了解使用哪些参数来构建模型以便可以复制该模型，对于在生产中运行机器学习应用程序是必不可少的。如果不了解如何使用机器学习应用程序做出决策，可能会产生一些严重的后果，包括可能的监管措施。欧盟 GDPR 法律包括一个关于机器学习的特定部分，称为第 22 条，其中包括一个注释，说明必须有可能提供用于做出决策的逻辑。

当我们在 Spark 中使用 ML 对象时，每个对象通常带有许多参数，有两种方法可以访问这些参数。首先，对象本身通常有一组 getters 和 setters。例如，如果我们查看表 [7-1](#Tab1) 中的`Word2Vec`对象，我们可以看到每个参数的 Get*、Set*和参数名。

表 7-1

Word2Vec 上的 getter/setter 参数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

得到

 | 

一组

 | 

参数名称

 |
| --- | --- | --- |
| GetInputCol | SetInputCol | 输入控制 |
| GetOutputCol | SetOutputCol | 输出控制 |
| GetVectorSize | 服务规模 | 向量大小 |
| getpincount | SetMinCount | minCount |
| getmaxsentexcelength | setmaxsentexcelength | maxsentexcelength |
| GetNumPartition | 集合分区 | 数字分区 |
| 获取种子 | 集种子 | 种子 |
| GetStepSize | SetStepSize | 步长 |
| GetWindowSize | SetWindowSize | windows size(windows size) |
| GetVectorSize | 服务规模 | 向量大小 |
| GetMaxIter | SetMaxIter | 马克西特 |

实际上，这意味着我们可以使用提供的 getter 和 setter 方法来控制参数，或者我们可以使用方法 Set 并将参数传递给 object。在清单 [7-1](#PC1) 中，我们展示了一个如何使用 getter 和 setter 方法或者一个`Param`对象来设置特定参数的例子。我们还引入了`ExplainParams`方法，它打印所有可用的参数，包括任何文档、当前值以及默认值(如果有的话)。

```cs
var word2Vec = new Word2Vec();
word2Vec.SetSeed(123);

Console.WriteLine(word2Vec.ExplainParams());

Listing 7-1Controlling an object’s parameters

```

运行此命令会产生以下输出:

```cs
inputCol: input column name (undefined)
maxIter: maximum number of iterations (>= 0) (default: 1)
maxSentenceLength: Maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks up to the size (> 0) (default: 1000)
minCount: the minimum number of times a token must appear to be included in the word2vec model's vocabulary (>= 0) (default: 5)
numPartitions: number of partitions for sentences of words (> 0) (default: 1)
outputCol: output column name (default: w2v_cabb3eadcb81__output)
seed: random seed (default: -1961189076, current: 123)
stepSize: Step size to be used for each iteration of optimization (> 0) (default: 0.025)
vectorSize: the dimension of codes after transforming from words (> 0) (default: 100)
windowSize: the window size (context words from [-window, window]) (> 0) (default: 5)

```

我们可以看到种子的值被设置为 123。在清单 [7-2](#PC3) 中，我们使用一个`Param`对象和`Set`方法来指定参数值。

```cs
var seedParam = new Param(word2Vec, "seed", "Setting the seed to 54321");
word2Vec.Set(seedParam, 54321L);

Console.WriteLine(word2Vec.ExplainParams());

Listing 7-2Using a Param object to set a parameter value

```

清单 [7-2](#PC3) 产生以下输出，我们可以看到种子现在是 54321:

```cs
inputCol: input column name (undefined)
maxIter: maximum number of iterations (>= 0) (default: 1)
maxSentenceLength: Maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks up to the size (> 0) (default: 1000)
minCount: the minimum number of times a token must appear to be included in the word2vec model's vocabulary (>= 0) (default: 5)
numPartitions: number of partitions for sentences of words (> 0) (default: 1)
outputCol: output column name (default: w2v_cabb3eadcb81__output)
seed: random seed (default: -1961189076, current: 54321)
stepSize: Step size to be used for each iteration of optimization (> 0) (default: 0.025)
vectorSize: the dimension of codes after transforming from words (> 0) (default: 100)
windowSize: the window size (context words from [-window, window]) (> 0) (default: 5)

```

最后，在清单 [7-3](#PC5) 中，我们没有创建新的`Param`对象，而是要求`Word2Vec`对象给我们一个名为“seed”的参数，然后我们可以用它来设置参数。

```cs
var seed = word2Vec.GetParam("seed");
word2Vec.Set(seed, 12345L);
Console.WriteLine(word2Vec.ExplainParams());

Listing 7-3Using a Param object supplied by the Word2Vec object to set a parameter value

```

我们可以在输出中看到，参数值被设置为 12345:

```cs
inputCol: input column name (undefined)
maxIter: maximum number of iterations (>= 0) (default: 1)
maxSentenceLength: Maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks up to the size (> 0) (default: 1000)
minCount: the minimum number of times a token must appear to be included in the word2vec model's vocabulary (>= 0) (default: 5)
numPartitions: number of partitions for sentences of words (> 0) (default: 1)
outputCol: output column name (default: w2v_cabb3eadcb81__output)
seed: random seed (default: -1961189076, current: 12345)
stepSize: Step size to be used for each iteration of optimization (> 0) (default: 0.025)
vectorSize: the dimension of codes after transforming from words (> 0) (default: 100)
windowSize: the window size (context words from [-window, window]) (> 0) (default: 5)

```

关键的一点是，当我们使用`Param`对象和`Set`方法时，数据类型没有被验证，因此有可能将参数设置为不正确的类型。除非你保存你的对象或者尝试使用它，否则你不会知道。这通常比在每个对象上使用提供的 getters 和 setters 更安全。`Param`对象的 Scala 版本有一种验证参数的方法，但是在。NET，我们只是需要小心。

如果你想把一个参数重置回它原来的默认值，你可以使用`Clear`方法，如清单 [7-4](#PC7) 所示。

```cs
var seed = word2Vec.GetParam("seed");
word2Vec.Set(seed, 12345L);
Console.WriteLine(word2Vec.ExplainParams());

word2Vec.Clear(seed);
Console.WriteLine(word2Vec.ExplainParams());

Listing 7-4Clearing any parameters which have previously been set

```

## 保存/加载对象

Spark 中的每一个核心物体。ML 名称空间包括一个名为`Save`的方法和一个名为`Load`的静态方法。`Load`和`Save`方法保存对象的副本，包括任何运行时信息，然后允许它们被读回内存。这对于机器学习应用程序特别有用，因为我们可能希望在一组数据上创建和训练一个模型，然后保存这些对象，以便以后可以重用它们来使用该模型或运行预测。在清单 [7-5](#PC8) 中，我们看到了正在使用的`Load`和`Save`方法。请注意，虽然它们在同一个进程中，但是对象可以保存在一个进程中，并加载到另一个进程中。语言是不相关的，所以您可以在。NET，保存它，然后从 Scala 加载并使用它。

```cs
bucketizer.SetInputCol("input_column");
bucketizer.Save("/tmp/bucketizer");

bucketizer.SetInputCol("something_else");

var loaded = Bucketizer.Load("/tmp/bucketizer");
Console.WriteLine(bucketizer.GetInputCol());
Console.WriteLine(loaded.GetInputCol());

Listing 7-5The Load and Save methods

```

当我们运行它时，我们可以看到原始的`Bucketizer`，它的 inputColumn 被设置为“其他”，仍然有效，但是新加载的`Bucketizer`具有原始的值“input_column”。

```cs
something_else
input_column

```

## 可辨认的

这些对象通常还会实现`Identifiable`，这意味着当您创建一个新的对象时，您可以选择指定一个惟一的字符串来标识该对象的特定实例。如果没有指定唯一的字符串，则会为您生成一个。您可以稍后使用这个唯一的字符串来标识对象的确切实例。当您创建`Param`对象时，您需要标识 param 将属于的对象，这可以通过传递对象本身或传递字符串标识符来完成。在清单 [7-6](#PC10) 中，我们展示了如何将一个唯一的字符串传递给一个 Spark。ML 对象以及以后如何引用这个唯一的字符串。

```cs
var tokenizer = new Tokenizer();
Console.WriteLine(tokenizer.Uid());

tokenizer = new Tokenizer("a unique identifier");
Console.WriteLine(tokenizer.Uid());

Listing 7-6The uid of a Spark.ML object instance

```

它的输出是

```cs
tok_34a2ad14b80a
a unique identifier

```

## TF-以色列国防军

中实现的 ML 对象。NET 与 Spark 在 Spark 中的对象数量相差甚远。ML；然而，已经有足够的功能来运行有用的机器学习应用程序。在本节中，我们将构建一个“术语频率，逆文档频率”或 TF-IDF 的工作示例，这是一种在一组文档中搜索某些文本并找到相关文档的方法。TF-IDF 基于这样一个事实:如果您只是对术语进行通配符搜索，那么您将会找到存在该术语但不太相关的文档。TF-IDF 衡量一个词在一个特定文档中的常见程度，与所有文档中有多少术语以及这些搜索术语的相关性。例如，一本书可能在每一页上都写有“第 xx 页”,但是这个术语与文档并不十分相关。然而，如果有一个文档讨论页面是如何布局的，那么这个文档中的单词 page 将会非常相关。

TF-IDF 在这篇维基百科文章中有详细讨论: [`https://en.wikipedia.org/wiki/Tf%E2%80%93idf`](https://en.wikipedia.org/wiki/Tf%25E2%2580%2593idf) *。*使用 TF-IDF 的高级流程是

1.  获取一些文档作为来源。

2.  将文件读入数据帧。

3.  使用分词器将文档拆分成单词。

4.  使用 HashingTF 构建一个向量，其中包含每个单词的哈希。

5.  *创建 IDF，通过“拟合”每个词的 hash 来创建 IDFModel，即给每个词或术语一个频数和相对重要性。*

6.  获取一些搜索词，并将它们转换成数据帧。

7.  使用分词器将搜索词拆分成单词。

8.  *使用 HashingTF 构建一个向量，包含搜索词中每个单词的散列。*

9.  *使用 IDFModel 转换搜索词，赋予它们与文档中相同的相对权重。*

10.  *将数据集连接在一起，通过计算两者的余弦相似度并根据匹配程度对结果进行排序，计算出搜索词与文档的接近程度。要理解我们为什么用余弦相似度与 TF-IDF，看看这篇优秀的博文:* [`https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/`](https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/) *。*

对于这个例子，我将使用*莎士比亚全集*，然后找到与特定搜索词相关的文档。我们需要的数据有几个来源，但我下载了这个回购( [`https://github.com/severdia/PlayShakespeare.com-XML`](https://github.com/severdia/PlayShakespeare.com-XML) )，包括 XML 格式的所有作品的副本，这使得阅读每首诗或剧本的文本和标题以及许可许可证变得很简单。

完整的示例应用程序在清单 7-examples sharp 和清单 7-examples sharp 中。

在清单 [7-7](#PC12) (C#)和 7-8 (F#)中，我们以 XML 格式读取每个文档，并解析 XML 以检索作品的文本和标题。

```cs
let createXmlDoc(path: string) =
    let doc = XmlDocument()
    doc.Load(path)
    doc

let parseXml(doc: XmlDocument) =
    let selectSingleNode node =
        Option.ofObj (doc.SelectSingleNode(node))

    let documentTitle =
        match selectSingleNode "//title" with
        | Some node -> node.InnerText
        | None -> doc.SelectSingleNode("//personae[@playtitle]").Attributes.["playtitle"].Value

    match selectSingleNode "//play" with
    | Some node -> GenericRow([|documentTitle; node.InnerText|])
    | None -> GenericRow([|documentTitle; doc.SelectSingleNode("//poem").InnerText|])

let getDocuments path = System.IO.Directory.GetFiles(path, "*.xml")
                                      |> Seq.map (fun doc -> createXmlDoc doc)
                                      |> Seq.map (fun xml -> parseXml xml)

let main argv =

    let args = match argv with
                | [|documentPath; searchTerm|] -> {documentsPath = argv.[0]; searchTerm = argv.[1]; success = true}
                | _ -> {success = false; documentsPath = ""; searchTerm = ""}

    match args.success with
        | false ->
            printfn "Error, incorrect args. Expecting 'Path to documents' 'search term', got: %A" argv
            -1

        | true ->
            let spark = SparkSession.Builder().GetOrCreate()
            let documents = getDocuments args.documentsPath

Listing 7-8Reading the contents of each work as XML and retrieving the document and title in F#

```

```cs
private static List<GenericRow> GetDocuments(string path)
{
    var documents = new List<GenericRow>();

    foreach (var file in new DirectoryInfo(path).EnumerateFiles("*.xml", SearchOption.AllDirectories))
    {
        var doc = new XmlDocument();

        doc.Load(file.FullName);

        var playTitle = "";
        var title = doc.SelectSingleNode("//title");

        playTitle = title != null ? title.InnerText : doc.SelectSingleNode("//personae[@playtitle]").Attributes["playtitle"].Value;

        var play = doc.SelectSingleNode("//play");

        if (play != null)
        {
            documents.Add(new GenericRow(new[] {playTitle, play.InnerText}));
        }
        else
        {
            var poem = doc.SelectSingleNode("//poem");
            documents.Add(new GenericRow(new[] {playTitle, poem.InnerText}));
        }
    }

    return documents;
}

var spark = SparkSession
    .Builder()
    .GetOrCreate();

var documentPath = args[0];
var search = args[1];

var documentData = GetDocuments(documentPath);

Listing 7-7Reading the contents of each work as XML and retrieving the document and title in C#

```

既然我们已经将文档读入了。NET 应用程序，我们需要创建一个数据帧，以便 Apache Spark 可以处理这些文档。读取文件的替代方法。NET，然后创建一个 DataFrame，这将让 Apache Spark 读取 XML 文件，并用。NET，并以 Apache Spark 更友好的格式(如 Parquet 或 Avro)将它们写入磁盘。在这种情况下，因为大约有 50 个文档，所以我将创建一个 DataFrame 并将文档添加到其中，而不是再次写回文档。如果有成千上万的文档，那么我们需要考虑不同的方法。

在清单 [7-9](#PC14) 和 [7-10](#PC15) 中，我们创建了一个 DataFrame，它涉及到传递一个`IEnumerable<GenericRow>`和一个描述我们的行的模式。

```cs
let documents = spark.CreateDataFrame(documents, StructType([|StructField("title", StringType());StructField("content", StringType())|]))

Listing 7-10CreateDataFrame passing in our specific schema in F#

```

```cs
var documents = spark.CreateDataFrame(documentData, new StructType(new List<StructField>
{
    new StructField("title", new StringType()),
    new StructField("content", new StringType())
}));

Listing 7-9CreateDataFrame passing in our specific schema in C#

```

接下来我们要做的是产生 Spark。我们需要的 ML 对象。表 [7-2](#Tab2) 列出了对象以及我们将使用它们的目的。

表 7-2

Spark。机器学习应用程序所需的 ML 对象

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

目标

 | 

理由

 | 

培养

 | 

执行

 |
| --- | --- | --- | --- |
| Tokenizer | 将文档拆分成数据帧中的单词数组 | 是 | 是 |
| 哈希 | 将单词转换为每个单词的数字表示形式 | 是 | 是 |
| 综合资料的文件（intergrated Data File） | 使用这些文档建立一个模型，该模型描述了所有文档中的术语使用频率 | 是 | 不一旦用样本数据集“训练”了模型，我们就使用它，而不是每次都重新训练模型 |
| IDFModel | 这是与文档“匹配”的模型，包括每个术语在整个文档集中出现的频率 | 是 | 是 |

在清单 [7-11](#PC16) 和 [7-12](#PC17) 中，我们创建了在初始训练阶段和执行阶段使用的对象`Tokenizer`、`HashingTF`和`IDF`。我们将使用实际的文档创建`IDFModel`。

```cs
let tokenizer = Tokenizer().SetInputCol("content").SetOutputCol("words")
let hashingTF = HashingTF().SetInputCol("words").SetOutputCol("rawFeatures").SetNumFeatures(1000000)
let idf = IDF().SetInputCol("rawFeatures").SetOutputCol("features")

Listing 7-12Creating the Tokenizer, HashingTF, and IDF in F#

```

```cs
var tokenizer = new Tokenizer()
    .SetInputCol("content")
    .SetOutputCol("words");

var hashingTF = new HashingTF()
    .SetInputCol("words")
    .SetOutputCol("rawFeatures")
    .SetNumFeatures(1000000);

var idf = new IDF()
    .SetInputCol("rawFeatures")
    .SetOutputCol("features");

Listing 7-11Creating the Tokenizer, HashingTF, and IDF in C#

```

每个对象都使用一个`DataFrame`来处理，所以我们需要告诉对象使用哪个列。例如，要使用`Tokenizer`，我们告诉它将在“content”列中找到它的输入数据，它应该将它的输出数据写入到`Tokenizer`将创建的“words”列中。`HashingTF`将在“words”列中查找输入数据，并将数据输出到“rawFeatures”列。

在清单 [7-13](#PC18) 和 [7-14](#PC19) 中，我们将文档分成单个单词，然后分成向量，向量是每个单词的数字标识符。我们使用数字而不是字符串，因为我们需要运行一些计算，特别是计算每个文档与我们的搜索词相比的余弦相似性，而我们不能用字符串来做这些。

```cs
let featurized = tokenizer.Transform documents
                                |> hashingTF.Transform

Listing 7-14Transforming the documents into words and vectors in F#

```

```cs
var tokenizedDocuments = tokenizer.Transform(documents);
var featurizedDocuments = hashingTF.Transform(tokenizedDocuments);

Listing 7-13Transforming the documents into words and vectors in C#

```

如果我们在`HashingTF`返回的`DataFrame`上调用`Show`方法，那么它看起来会像这样

```cs
+---------+--------+---------+------------------+
|    title| content|  words|         rawFeatures|
+---------+--------+---------+------------------+
|The So...|The S...|[the, ...|(1000000,[522, ...|
|The Tw...|The T...|[the, ...|(1000000,[130, ...|

```

内容被分成一组单词，每个单词都有一个数字标识符。

现在我们有了可以使用的文档格式。我们需要通过将文档“适应”IDF 来“训练”模型。我们在清单 [7-15](#PC21) 和 [7-16](#PC22) 中展示了这一点。

```cs
let model = featurized
            |> idf.Fit

Listing 7-16“Fitting” the dataset to the IDF to create the model in F#

```

```cs
var idfModel = idf.Fit(featurizedDocuments);

Listing 7-15“Fitting” the dataset to the IDF to create the model in C#

```

现在我们有了我们需要的对象，我们有了已经在我们需要计算的文档数据集上训练过的模型，对于每个文档，它与所有其他文档相比有多大。为此，对于数据帧中的每一行，也就是每一个文档，我们遍历数据集中的每一个值，对数字求平方，然后求平方的平方根。在清单 [7-17](#PC23) 和 [7-18](#PC24) 中，我们遍历向量中的每个值，并计算归一化值，我们将在以后计算每个文档与我们的搜索词有多相似时使用该值。

```cs
let calcNormUDF = Functions.Udf<Row, double>(fun row -> row.Values.[3] :?> ArrayList
                                                     |> Seq.cast
                                                     |> Seq.map (fun item -> item * item)
                                                     |> Seq.sum
                                                     |> Math.Sqrt)

let normalizedDocuments = model.Transform featurized

                                            |> fun data -> data.Select(Functions.Col("features"), calcNormUDF.Invoke(Functions.Col("features")).Alias("norm"), Functions.Col("title"))

Listing 7-18Calculating the normalization number to use later on in F#

```

```cs
private static readonly Func<Column, Column> udfCalcNorm = Udf<Row, double>(row =>
    {
        var values = (ArrayList) row.Values[3];
        var norm = 0.0;

        foreach (var value in values)
        {
            var d = (double) value;
            norm += d * d;
        }

        return Math.Sqrt(norm);
    }
);

var transformedDocuments = idfModel.Transform(featurizedDocuments).Select("title", "features");
            var normalizedDocuments = transformedDocuments.Select(Col("features"), udfCalcNorm(transformedDocuments["features"]).Alias("norm"), Col("title"));

Listing 7-17Calculating the normalization number to use later on in C#

```

直到的 1.0 版。NET 中，不可能将向量从 JVM 转移到。尽管如此，在 1.0 版中，提供给 UDF 的数据是向量的内部表示。在 Apache Spark 中，有两种类型的向量，一种是 DenseVector，另一种是 SparseVector。如果您在 Scala 或 Python 中使用了一种 Vector 类型，那么您可以将它们作为 Vector 来使用。希望在未来的某个时候，你能够在。NET for Apache Spark，但在此之前，我们需要了解向量是如何在内部实现的。

DenseVector 是最容易使用的，因为它背后有一个 double 数组，即 double 数组。这里的 SparseVector 比较难处理，因为它不是一个包含所有值的数组，任何 0.0 的值都被排除在 SparseVector 之外，所以如果你想把 1.0，2.0，0.0，4.0 表示为一个 SparseVector，你会得到一个包含每个元素的索引列表的数组；如果值为 0.0，则省略索引。在我们的 SparseVector 示例中，我们有两个数组，一个包含以下索引 0、1、3，另一个包含值 1.0、2.0、3.0。当我们想要迭代 SparseVector 时，我们需要迭代每个索引。如果缺少索引值，我们知道该值是 0.0，但是如果该值在索引中，我们使用索引的位置来查找实际值。在我们的例子中，如果我们想知道 SparseVector 中第四个位置的值是什么，我们将进入索引并搜索值 3；记住这是一个从零开始的数组。值 3 位于数组或索引 2 的第三个位置，它指向值数组中的 3.0。

在表 [7-3](#Tab3) 中，我们可以看到单词是如何被拆分成记号的。

表 7-3

SparseVector 示例以及如何检索特定索引。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

矢量

 | 

索引

 | 

价值观念

 | 

索引 5 处的值

 |
| --- | --- | --- | --- |
| 0.0，0.0，0.1，0.0，0.0， *0.2* | 2、 *5* | 0.1， *0.2* | *0.2* |
| 0.1，0.2，0.3，0.4，0.0， *0.0* | 0, 1, 2, 3 | 0.1, 0.2, 0.3, 0.4 | *0.0* |

实际上，这意味着我们的 UDF 接收了一个包含四个对象的数组，如表 [7-4](#Tab4) 所示。

表 7-4

作为对象数组提供给 UDF 的 SparseVector 的详细信息

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

索引

 | 

类型

 | 

描述

 |
| --- | --- | --- |
| Zero | （同 Internationalorganizations）国际组织 | 此应用程序的起始偏移量将始终为 0 |
| one | （同 Internationalorganizations）国际组织 | 这个 SparseVector 表示的 Vector 中有多少项。SparseVector 可能包含 10 个值，但是 SparseVector 可以表示 Vector 中的数百万个项目 |
| Two | （同 Internationalorganizations）国际组织 | 向量中指向非 0.0 值的索引 |
| three | 两倍 | 向量中不为 0.0 的值 |

在清单 [7-19](#PC25) 和 [7-20](#PC26) 中，我们获取搜索词，创建一个数据帧，然后运行同样的过程，分割成单词，创建一个向量，并使用模型将向量转换成一组我们可以与原始文档进行比较的特征。我们唯一不需要做的是重建模型，因为我们有原始文档的模型，我们将不得不重用它；否则，我们的搜索词将与原始文档具有不同的权重。

```cs
let term = GenericRow([|"Montague and capulets"|])
let searchTerm = spark.CreateDataFrame([|term|], StructType([|StructField("content", StringType())|]) )

tokenizer.Transform searchTerm
    |> hashingTF.Transform
    |> model.Transform
    |> fun data -> data.WithColumnRenamed("features", "searchTermFeatures")
    |> fun data -> data.WithColumn("searchTermNorm", calcNormUDF.Invoke(Functions.Col("searchTermFeatures")))

Listing 7-20Converting the search term into a DataFrame that can be compared with the original documents in F#

```

```cs
var searchTerm = spark.CreateDataFrame(
    new List<GenericRow> {new GenericRow(new[] {search})},
    new StructType(new[] {new StructField("content", new StringType())}));

var tokenizedSearchTerm = tokenizer.Transform(searchTerm);

var featurizedSearchTerm = hashingTF.Transform(tokenizedSearchTerm);

var normalizedSearchTerm = idfModel
    .Transform(featurizedSearchTerm)
    .WithColumnRenamed("features", "searchTermFeatures")
    .WithColumn("searchTermNorm", udfCalcNorm(Column("searchTermFeatures")));

Listing 7-19Converting the search term into a DataFrame that can be compared with the original documents in C#

```

最后要做的事情是将原始文档和搜索词连接成一个数据帧，并计算两个向量的余弦相似度，如清单 [7-21](#PC27) 和 [7-22](#PC28) 所示。我们通过将向量中的每个值乘以数组中相同位置的第二个向量中的值来计算余弦相似度。然后，我们将结果除以我们之前计算的文档和搜索词的归一化乘积。还要注意，这是一个 SparseVector，所以我们需要做一些工作来识别特定偏移量处的值。

```cs
let cosineSimilarity (vectorA:Row, vectorB:Row, normA:double, normB:double):double =

    let indicesA = vectorA.Values.[2]  :?> ArrayList
    let valuesA = vectorA.Values.[3] :?> ArrayList

    let indicesB = vectorB.Values.[2] :?> ArrayList
    let valuesB = vectorB.Values.[3] :?> ArrayList

    let indexedA = indicesA |> Seq.cast |> Seq.indexed
    let indexedB = indicesB |> Seq.cast |> Seq.indexed |> Seq.map (fun item -> (snd item, fst item)) |> Map.ofSeq

    PrettyPrint.print indexedB

    let findIndex value = match indexedB.ContainsKey value with
                            | true -> indexedB.[value]
                            | false -> -1

    let findValue indexA =
                            let index =  findIndex indexA

                            match index with
                                | -1 -> 0.0
                                | _ -> unbox<double> (valuesB.Item(unbox<int> (index)))

    let dotProduct = indexedA
                       |> Seq.map (fun index -> (unbox<double>valuesA.[fst index]) * (findValue (unbox<int> indicesA.[fst index])))
                       |> Seq.sum

    normA * normB |> fun divisor -> match divisor with
                                                | 0.0 -> 0.0
                                                | _ -> dotProduct / divisor

let cosineSimilarityUDF = Functions.Udf<Row, Row, double, double, double>(fun vectorA vectorB normA normB -> cosineSimilarity(vectorA, vectorB, normA, normB))

Listing 7-22Calculating the cosine similarity using F#

```

```cs
private static readonly Func<Column, Column, Column, Column, Column> udfCosineSimilarity =
    Udf<Row, Row, double, double, double>(
        (vectorA, vectorB, normA, normB) =>
        {
            var indicesA = (ArrayList) vectorA.Values[2];
            var valuesA = (ArrayList) vectorA.Values[3];

            var indicesB = (ArrayList) vectorB.Values[2];
            var valuesB = (ArrayList) vectorB.Values[3];

            var dotProduct = 0.0;

            for (var i = 0; i < indicesA.Count; i++)
            {
                var valA = (double) valuesA[i];

                var indexB = findIndex(indicesB, 0, (int) indicesA[i]);

                double valB = 0;
                if (indexB != -1)
                {
                    valB = (double) valuesB[indexB];
                }
                else
                {
                    valB = 0;
                }

                dotProduct += valA * valB;
            }

            var divisor = normA * normB;

            return divisor == 0 ? 0 : dotProduct / divisor;
        });

Listing 7-21Calculating the cosine similarity using C#

```

在清单 [7-23](#PC29) 和 [7-24](#PC30) 中，我们有连接数据帧的最后一步，计算余弦相似度，按最相似到最不相似排序结果，然后打印出标题和相似度。

```cs
|> normalizedDocuments.CrossJoin
|> fun data -> data.WithColumn("similarity", cosineSimilarityUDF.Invoke(Functions.Col("features"), Functions.Col("searchTermFeatures"), Functions.Col("norm"), Functions.Col("searchTermNorm")))
|> fun matched -> matched.OrderBy(Functions.Desc("similarity")).Select("title", "similarity")
|> fun ordered -> ordered.Show(100, 1000)

Listing 7-24Joining the DataFrames and calculating the cosine similarity to generate our best matching results in F#

```

```cs
var results = normalizedDocuments.CrossJoin(normalizedSearchTerm);

results
    .WithColumn("similarity", udfCosineSimilarity(Column("features"), Column("searchTermFeatures"), Col("norm"), Col("searchTermNorm")))
    .OrderBy(Desc("similarity")).Select("title", "similarity")
    .Show(10000, 100);

Listing 7-23Joining the DataFrames and calculating the cosine similarity to generate our best matching results in C#

```

在表 [7-5](#Tab5) 中，我用各种搜索词运行程序，这些是结果，我认为这些结果惊人地准确。

表 7-5

与莎士比亚全集进行比对时的搜索词及其结果

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

搜索词

 | 

位置

 | 

标题

 | 

类似

 |
| --- | --- | --- | --- |
| “树林里的恋人们毒死了自己” | one | 仲夏夜之梦 | 0.04105529867838565 |
|   | Two | 如你所愿 | 0.02845396350570514 |
|   | three | 《爱的徒劳》 | 0.014176769638970023 |
| "女巫用匕首沾满鲜血" | one | 麦克白的悲剧 | 0.08824800070165366 |
|   | Two | 错误的喜剧 | 0.025993297039907045 |
|   | three | 亨利六世的第二部分 | 0.007198784643312808 |

## 摘要

在这一章中，我们研究了 Spark。ML API，尽管它远不如。NET 版本的 Apache Spark APIs 仍然有用，并且正在积极开发以增加覆盖率。

使用 Spark 有一些复杂之处。ML API in。NET，比如不得不与 raw `SparseVector`一起工作，但希望这些进入的障碍应该很快被消除。