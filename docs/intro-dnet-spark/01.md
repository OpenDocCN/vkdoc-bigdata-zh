# 一、了解 Apache Spark

Apache Spark 是一个数据分析平台，它使大数据变得可访问，并将大规模数据处理带入每个开发人员的生活。使用 Apache Spark，读取本地机器上的单个 CSV 文件就像读取数据湖中的一百万个 CSV 文件一样容易。

## 一个例子

让我们看一个例子。清单 [1-1](#PC1) (C#)和[1-2](#PC2)(F #版本)中的代码从一组 CSV 文件中读取，并计算有多少记录匹配特定条件。该代码读取特定路径中的所有 CSV 文件，因此我们读取的文件数量实际上是无限的。

尽管本章中的示例是功能完整的示例，但是它们需要一个可以工作的 Apache Spark 实例，无论是在本地还是在集群上。我们在第 [2](02.html) 章介绍了 Apache Spark 的设置和运行。NET for Apache Spark 第三章。

```cs
open Microsoft.Spark.Sql

[<EntryPoint>]
let main argv =

    let path = argv.[0]
    let spark = SparkSession.Builder().GetOrCreate()

    spark.Read().Option("header", "true").Csv(path)
     |> fun dataframe -> dataframe.Filter(Functions.Col("name").EqualTo("Ed Elliott")).Count()
     |> printfn "There are %d row(s)"

    0

Listing 1-2Counting how many rows match a filter in one or a million CSV files in F#

```

```cs
using System;
using System.Linq;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;

namespace Introduction_CSharp
{
    class Program
    {
        static void Main(string[] args)
        {

            var path = args.FirstOrDefault();

            var spark = SparkSession
                .Builder()
                .GetOrCreate();

            var dataFrame = spark.Read().Option("header", "true").Csv(path);
            var count = dataFrame.Filter(Col("name") == "Ed Elliott").Count();
            Console.WriteLine($"There are {count} row(s)");
        }
    }
}

Listing 1-1Counting how many rows match a filter in one or a million CSV files in C#

```

执行这两个程序中的任何一个都会显示与过滤器匹配的行数:

```cs
» dotnet run --project ./Listing0-1 "/Users/ed/sample-data/1.csv"
There are 1 row(s)
» dotnet run --project ./Listing0-2 "/Users/ed/sample-data/1.csv"
There are 1 row(s)

```

如果我们对单个文件使用这种方法，那就没问题，代码看起来非常高效，但是当相同的代码可以在包含许多节点和数 Pb 数据的集群中高效运行时，您就可以看到 Apache Spark 有多么强大了。

## 核心使用案例

Apache Spark 在大数据处理领域是独一无二的，因为它允许数据处理、分析以及机器学习。通常，您可以使用 Apache Spark:

*   将数据转换为 ETL 或 ELT 数据管道的一部分

*   分析从一个小文件到数百万个文件中数 Pb 数据的数据集

*   创建机器学习(ML)应用程序来实现人工智能

### 转变您的数据

Apache Spark 可以读取和写入 Java 虚拟机支持的任何文件格式或数据库，这意味着我们可以从 JDBC 连接读取和写入文件。Apache Spark 开箱即用，能够读取各种文件格式，比如 CSV 或 Parquet。但是，您总是可以引用其他 JAR 文件来添加对其他文件类型的支持，例如，crealytics 的“spark-excel”插件( [`https://github.com/crealytics/spark-excel`](https://github.com/crealytics/spark-excel) )允许您在 Apache Spark 中读写 XLSX 文件。

为了展示 Apache Spark 在处理时的强大功能，并展示它是如何从头开始构建性能的，我参与了一个项目，在这个项目中，我们将读取一个巨大的 parquet 文件，其中包含一个流行的国际网站的所有 Adobe 点击流数据。在我们的例子中，数据是一个包含用户在网站上所有行为的文件；对于一个经常访问的网站，该文件可能有数 GB，包含一系列事件，包括无效数据。我的团队的任务是高效地读取数百万行的整个文件，并检索一个特定操作的最小子集。在 Apache Spark 之前，我们可能会将整个文件放入数据库，然后过滤掉我们不想要的行，或者使用像微软的 SSIS 这样的工具，它会读入整个文件。当我们在 Apache Spark 中实现它时，我们为我们想要的特定行类型编写了一个过滤器。Apache Spark 从文件中读取数据，并使用谓词下推将过滤器传递给读取 parquet 文件的驱动程序，因此，在最早的时候，无效的行就被过滤掉了。这个项目向我们展示了 Apache Spark 所展示的性能水平和易用性，这是我们的团队以前从未见过的。

清单 [1-3](#PC4) (C#)和 [1-4](#PC5) (F#)中的代码将演示如何从数据源读取数据，将数据过滤到您需要的行，并展示如何将数据写出到一个新文件中，这对于 Apache Spark 来说再简单不过了。

```cs
open Microsoft.Spark.Sql
open System

[<EntryPoint>]
let main argv =

    let writeResults (x:DataFrame) =
        x.Write().Mode("overwrite").Parquet("output.parquet")
        printfn "Wrote: %u rows" (x.Count())

    let spark = SparkSession.Builder().GetOrCreate()
    spark.Read().Parquet("1.parquet")
    |> fun p -> p.Filter(Functions.Col("Event_Type").EqualTo(Functions.Lit(999)))
    |> fun filtered -> writeResults filtered

    0 // return an integer exit code

» dotnet run --project ./ Listing0-4
Wrote: 10 rows

Listing 1-4Reading, filtering, and writing data back out again in F#

```

```cs
using System;
using Microsoft.Spark.Sql;

namespace TransformingData_CSharp
{
    class Program
    {
        static void Main(string[] args)
        {
            var spark = SparkSession
                .Builder()
                .GetOrCreate();

            var filtered = spark.Read().Parquet("1.parquet")
                .Filter(Functions.Col("event_type") == Functions.Lit(999));

            filtered.Write().Mode("overwrite").Parquet("output.parquet");
            Console.WriteLine($"Wrote: {filtered.Count()} rows");
        }
    }
}

» dotnet run --project ./ Listing0-3
Wrote: 10 rows

Listing 1-3Reading, filtering, and writing data back out again in C#

```

### 分析你的数据

Apache Spark 包含了您期望从数据库中获得的数据分析能力，如聚合、窗口和 SQL 函数，您可以使用公共 API 如`data.GroupBy(Col("Name")).Count()`来访问这些功能。有趣的是，您也可以编写 Spark SQL，这意味着您可以使用 SQL 查询来访问您的数据。Spark SQL 使 Apache Spark 面向更广泛的受众，包括开发人员以及分析师和数据科学家。无需学习 Scala、Python、Java、R 以及现在的 C#或 F#就能使用 Apache Spark 的强大功能是一个引人注目的特性。

清单 [1-5](#PC6) 和 [1-6](#PC7) 显示了另一个例子，我们生成三个数据集，将这些数据集联合在一起，然后聚合并显示结果。NET，然后在清单 [1-7](#PC8) 中，我们演示了相同的结果，但是没有使用。NET 代码，我们将一个 SQL 查询传递给 Apache Spark，并执行该查询来创建一个我们可以使用的结果集；请注意，有一些 Apache Spark 环境，如 Databricks 笔记本，我们可以只编写 SQL 而不编写任何应用程序代码。

```cs
open Microsoft.Spark.Sql
open System

[<EntryPoint>]
let main argv =
    let spark = SparkSession.Builder().GetOrCreate()
    spark.Range(100L).WithColumn("Name", Functions.Lit("Ed"))
    |> fun d -> d.Union(spark.Range(100L).WithColumn("Name", Functions.Lit("Bert")))
    |> fun d -> d.Union(spark.Range(100L).WithColumn("Name", Functions.Lit("Lillian")))
    |> fun d -> d.GroupBy(Functions.Col("Name")).Count()
    |> fun d -> d.Show()

    0

Listing 1-6Create three datasets, union, aggregate, and count in F#

```

```cs
using System;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;

namespace TransformingData_CSharp
{
    class Program
    {
        static void Main(string[] args)
        {
            var spark = SparkSession

                .Builder()
                .GetOrCreate();

            var data = spark.Range(100).WithColumn("Name", Lit("Ed"))
                .Union(spark.Range(100).WithColumn("Name", Lit("Bert")))
                .Union(spark.Range(100).WithColumn("Name", Lit("Lillian")));

            var counts = data.GroupBy(Col("Name")).Count();
            counts.Show();
        }
    }
}

Listing 1-5Create three datasets, union, aggregate, and count in C#

```

最后，在清单 [1-7](#PC8) 中，我们将使用 Spark SQL 来实现相同的结果。

```cs
using System;
using Microsoft.Spark.Sql;

namespace TransformingData_SQL
{
    class Program
    {
        static void Main(string[] args)
        {
            var spark = SparkSession
                .Builder()
                .GetOrCreate();

            var data = spark.Sql(@"
                WITH users

                AS (
                    SELECT ID, 'Ed' as Name FROM Range(100)
                    UNION ALL
                    SELECT ID, 'Bert' as Name FROM Range(100)
                    UNION ALL
                    SELECT ID, 'Lillian' as Name FROM Range(100)
                ) SELECT Name, COUNT(*) FROM users GROUP BY Name
            ");
            data.Show();
        }
    }
}

Listing 1-7Create three datasets, union, aggregate, and count in Spark SQL

```

Apache Spark 在所有三个实例中执行的代码是相同的，并产生以下输出:

```cs
» dotnet run --project ./Listing0-7
+-------+--------+
|   Name|count(1)|
+-------+--------+
|   Bert|     100|
|Lillian|     100|
|     Ed|     100|
+-------+--------+

```

### 机器学习

Apache Spark 的最后一个核心用例是编写机器学习(ML)应用。今天，有很多不同的环境可以编写 ML 应用程序，比如 Scikit-Learn、TensorFlow 和 PyTorch。然而，在 ML 应用程序中使用 Apache Spark 的好处是，如果您已经使用 Apache Spark 处理了数据，那么您将获得相同的熟悉的 API，更重要的是，您可以重用现有的基础设施。

要了解在 Apache Spark 中使用 ML API 可以做什么，请参见 [`https://spark.apache.org/docs/latest/ml-guide.html`](https://spark.apache.org/docs/latest/ml-guide.html) 。

## 。Apache Spark 的 NET

Apache Spark 是用 Scala 编写的，运行在 Java 虚拟机(JVM)上，但是有大量开发人员的主要语言是 C#，其次是 F#。那个。NET for Apache Spark 项目旨在将 Apache Spark 的全部功能引入到。NET 开发人员。微软将该项目作为开源项目启动，在开放环境中开发，并接受拉式请求、问题和功能请求。

那个。NET for Apache Spark project 在。NET CLI 代码和 JVM。工作方式是有一个 Java 类，用 Scala 写的；名为`DotnetRunner`的 Java 类创建一个 TCP 套接字，然后`DotnetRunner`运行一个 dotnet 程序，**你的程序**创建一个`SparkSession`。`SparkSession`与 TCP 套接字建立连接，将请求转发给 JVM 并返回响应。你可以想到。NET for Apache Spark 库作为。NET 代码和 JVM。

微软团队做出了一个重要的早期决策，这影响了我们如何从. NET 使用 Apache Spark。Apache Spark 最初是从所谓的 RDD API 开始的。RDD API 允许用户访问 Apache Spark 使用的底层数据结构。当 Apache Spark 版发布时，它包含了一个新的 DataFrame API。DataFrame API 有几个额外的好处，比如一个新的`"catalyst"`优化器，这意味着使用 DataFrame API 比原来的 RDD API 更有效。让 Apache Spark 优化查询，而不是尝试使用 RDD API 自己优化调用，也简单得多。DataFrame API 为 Python 和 R 以及现在的. NET 带来了同等的性能。以前的 RDD API 对于 Scala 或 Java 代码要比 Python 快得多。使用新的 DataFrame API，在大多数情况下，Python 或 R 代码与 Scala 和 Java 代码一样快。

微软团队决定只为新的 DataFrame API 提供支持，这意味着现在不可能使用来自。NET for Apache Spark。老实说，我不认为这是一个重要的问题，它当然不是采用。NET for Apache Spark。这种只支持后来的 API 的情况一直延续到 ML 库，这里有两个用于 ML 的 API，MLLib 和 ML。Apache Spark 团队不赞成 MLLib，而支持 ML 库，所以在。NET for Apache Spark，我们也只实现了 ML 版本的 API。

## 特征奇偶校验

那个。NET for Apache Spark project 于 2019 年 4 月首次向公众发布，包含了 Apache Spark 中也提供的许多核心功能。然而，有相当多的功能缺失，甚至从 DataFrame API 中也是如此，这忽略了可能不会实现的 API，如 RDD API。自最初发布以来，微软团队和外部贡献者已经增加了功能的数量。与此同时，Apache Spark 团队也发布了更多的功能，所以在某些方面，Microsoft project 正在追赶 Apache 团队，所以目前并不是所有的功能都可以在。NET 项目。在过去的一年多时间里，差距一直在缩小，我完全预计在未来一年左右的时间里，差距会越来越小，功能对等将在某个时候存在。

如果您试图使用。NET for Apache Spark project，并且缺少一些功能，这对您来说是一个障碍，您可以选择几个选项来实现缺少的功能，我在附录 b 中对此进行了介绍。

## 摘要

Apache Spark 是一个引人注目的数据处理项目，它使得查询大型分布式数据集变得非常简单。。NET for Apache Spark 将这种能力带到了。NET 开发人员，就我而言，对使用 C#和 F#创建 ETL、ELT、ML 和各种数据处理应用程序的可能性感到兴奋。