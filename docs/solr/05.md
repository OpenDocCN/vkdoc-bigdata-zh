# 五、索引数据

如您所知，Solr 会为您摄取的数据创建一个倒排索引，因此将数据添加到 Solr 的过程称为索引。在 Solr 中，您主要做的是索引和搜索，为了使数据可搜索，必须对其进行索引。

在第 2 章中，您使用 Solr 管理控制台索引了样本文档。在本章中，您将从深度和广度上了解索引过程。我所说的深度是指对过程的详细理解。我所说的宽度是指在索引文档之前需要做出的各种决定，比如文档格式、索引工具、索引频率、文档预处理和文本分析。前一章介绍了文本分析，所以本章介绍了其他方面。

你可能急于了解搜索过程，因为这是开发搜索引擎的最终目标，你可能想浏览这一章。如果您不熟悉 Solr，您可以这样做，并在搜索过程就绪后回来详细阅读。但是，如果您正致力于将系统投入生产，就不应该忽略索引过程。索引不仅仅是必要的先决条件。你必须记住，如果你把垃圾放进去，你就会把垃圾弄出来。

索引过程可以像添加文档以使内容可搜索一样简单，但是您不应该将您的应用程序局限于此。应该充分利用 Solr 和其他补充技术在数据清理、丰富和元数据提取方面的潜力。你将在本章中了解这些方法。此外，[第 11 章](11.html)介绍了一些提取隐藏在非结构化数据中的金块的高级方法。您可以遵循迭代和增量模型来引入这些功能。

本章涵盖以下主题:

*   文档索引工具
*   索引操作
*   为 XML、JSON 和 CSV 等文本文档编制索引
*   索引二进制文档和提取元数据
*   从数据库导入数据
*   预处理和文档丰富
*   索引性能
*   编写定制处理器
*   了解经常出现的问题

## 索引工具

Solr 通过 HTTP 公开了一个 RESTful 服务，因此任何 REST 客户端都可以搜索和索引文档。您只需要知道正确的端点和适用的参数。访问服务的客户端可以是您最喜欢的浏览器，比如 Chrome 或 Firefox，甚至是 Wget 工具。浏览器和 Wget 工具很适合评估和简单的请求，但是您通常需要更复杂的工具。

您选择的工具取决于您的索引需求、数据源、文档格式和性能要求。Solr 支持各种文档格式，包括 XML 和 JSON，一些工具支持一组特定的文档格式或数据源。本节介绍一些常用的索引工具。

### 附言

Solr 5.0 引入了一个 bash 脚本，可以在*nix 环境中执行该脚本来索引文档。这个脚本可以索引 Solr 原生格式(Solr 为 XML 和 JSON 指定并理解的格式)的文档，可以索引 CSV 文档，甚至可以执行简单的 web 爬行。它还允许您指定包含文档的目录路径。这个脚本存在于 Solr 发行版的`bin`目录中，您可以通过使用`-help`选项运行它来找到它的使用细节:

`$ bin/post -help`

下面是一个将`example/exampledocs`目录中的`money.xml`文件索引到`hellosolr`内核的例子:

`$ bin/post -c hellosolr ../example/exampledocs/money.xml`

### SimplePostTool

如果你运行的是早于 Solr 5.0 的版本或者你的平台是 Windows，你不用担心。您可以依赖在`example/exampledocs`目录中作为`post.jar`提供的`SimplePostTool`包，这是一个 Java 可执行文件。前面的`bin/post`脚本在内部使用这个 JAR 来提供方便的方法来索引文档。您可以通过如下方式运行 JAR 来查找使用细节:

`$ java -jar example/exampledocs/post.jar -h`

让我们像以前一样使用 SimplePostTool 来索引相同的文档:

`java -Dc=hellosolr -jar post.jar money.xml`

在这个例子中，我们不提供`money.xml`的相对路径，因为`post.jar`存在于同一个目录中。

### 卷曲

另一种强大的文档索引方式是`curl`实用程序。在本书中，你将使用 curl 来索引文档。请参阅 [`http://curl.haxx.se/docs/manpage.html`](http://curl.haxx.se/docs/manpage.html) 或*nix 手册中的 curl 用法详情:

`$ man curl`

### solr java 库

Solr 提供了一个 SolrJ Java 客户端来从您的程序访问服务器。它提供了创建文档、索引文档、形成查询、执行搜索和遍历结果的方法。对于索引，如果您从主数据源(如内容管理系统(CMS ))读取数据，您可能更喜欢使用 SolrJ。对于从关系数据库导入数据，Solr `DataImportHandler` contrib 模块是一个很好的起点。然而，因为它是单线程的，所以人们确实使用 SolrJ 来编写定制的导入程序，以处理大量的数据。

Note

Solr 的每个版本都捆绑了 SolrJ 库，Solr 提供的新特性更新了 Solr 库。使用 SolrJ 时，确保使用匹配版本的库作为服务器。

### 其他图书馆

除了 SolrJ，其他客户端库也可用于 Java。如果您的 Java 应用程序使用 Spring 框架，您甚至可以尝试 Spring Data Solr。客户端库也可用于其他语言。Python 有十几个客户端库。你可以参考 Solr wiki， [`https://wiki.apache.org/solr/IntegratingSolr`](https://wiki.apache.org/solr/IntegratingSolr) ，获得完整的客户端库列表。

## 索引过程

如果数据是结构化的并且格式良好，那么索引过程可能会很简单，但是如果数据是非结构化的，并且可以在不同的格式和数据源中使用，那么索引过程可能会有点复杂。例如，在 CSV 文件中索引文档只需要触发一个上传按钮或发布文本流。

Solr 支持包括 XML 和 JSON 在内的多种文档格式作为索引的输入:您可以用 Solr 指定的结构甚至随机格式来格式化数据并发布它。您定义的结构化文件将包含一组文档，每个文档都指定字段和应该索引到该字段的内容。输入文件中的字段名应映射到`schema.xml`中定义的字段名。

有趣的是，您可能有一个 XML 文档，但是结构和元素可能与原生 Solr 格式不同。对于这样的场景，`DataImportHandler` contrib 模块允许您定义 XML 元素和 Solr 字段之间的映射，并且您可以通过调用处理程序端点来启动索引过程。

此外，数据可以在任何不同的数据源中获得(例如，本地文件系统、数据库或网页)。以下步骤将帮助您根据您的索引需求理解和开发索引过程:

Text extraction: In this process, you extract the text for indexing. The text can be acquired, for example, by reading files, querying databases, crawling web pages, or reading RSS feeds. Extraction can be performed by your Java client application or Solr components. `DataImportHandler` is a contrib module that can be used for reading data from databases or XML files, for example. The Solr Cell framework, built on Apache Tika, can directly extract data from files in Office, Word, and PDF formats, as well as other proprietary formats.   Document preparation: The extracted text should be transformed into a Solr document for ingestion. The prepared document should adhere to the native format specified, for example, for XML or JSON. As a better alternative, you can use the SolrJ client to create a document for ingestion. If data is directly ingested using one of the Solr frameworks having support for automatic transformation, this step might not be required.   Post and commit: During this process, you post the document to the appropriate Solr endpoint with required parameters. Solr-provided extraction capabilities are performed based on the endpoint you invoke. You may optionally like to trigger a commit to persist the added documents immediately.   Document preprocessing: You might want to do cleanup, enrichment, or validation of text received by Solr. Solr provides a large number of `UpdateRequestProcessor` implementations for performing these tasks. It prebundles the processor implementation for common tasks such as deduplication and language detection, and allows you to write custom processors. You can even do the custom processing in the client program during document preparation, if you are not interested in writing Solr plug-ins.   Field analysis: Field analysis converts the input stream into terms. This step refers to the analysis chain of `analyzers`, `tokenizers` and token `filters` that are applied on the `fieldType` definition, which you read about in the previous chapter.   Index: The terms output from field analysis are finally indexed; the inverted index is created. These indexed terms are used for matching and ranking in search requests. After you trigger the post operation (mentioned in Step 3), the preprocessing and field analysis defined in Solr will be triggered automatically and documents will be indexed.  

图 [5-1](#Fig1) 描述了分度步骤。您可以看到，数据可以直接索引，也可以使用客户端应用程序索引。

![A978-1-4842-1070-3_5_Fig1_HTML.gif](img/A978-1-4842-1070-3_5_Fig1_HTML.gif)

图 5-1。

Solr indexing process

现在让我们更详细地理解索引过程和相关的 Solr 组件。假设您想要索引一个 XML 文档。为了索引它，您向`/update`端点发出请求。如果你穿过`solrconfig.xml`，你会发现它被映射到`UpdateRequestHandler`:

`<requestHandler name="/update" class="solr.UpdateRequestHandler" />`

对`/update`的任何请求都由`UpdateRequestHandler`处理。类似地，其他处理程序用于处理不同类型的内容流——例如，`DataImportHandler`用于从数据库、定制 XML、RSS 和 atom 提要导入数据；`ExtractingRequestHandler`用于从二进制文档中提取数据；等等。每个端点被映射到一个特定的处理器，就像`/update`被映射到`UpdateRequestHandler`一样。您甚至可以通过扩展`ContentStreamHandlerBase`来编写自己的处理程序。

当向`UpdateRequestHandler`发出请求时，将 XML 作为内容流发送，同时发送其他参数来处理请求。现在，我们来探索一下`UpdateRequestHandler`。

### 更新请求处理程序

`UpdateRequestHandler`支持对 XML、JSON、CSV 和 javabin 格式的文档进行索引。对于每个更新请求，您可以指定内容类型。基于这些信息，`UpdateRequestHandler`通过调用相应的加载器来解析文档，然后通过调用更新链中注册的处理器来索引文档。

您可以通过提供 MIME `Content-Type`或传递请求参数`update.contentType`来通知`UpdateRequestHandler`文档的格式。如果没有提供任何此类信息，处理程序会尝试自动识别内容类型。如果它检测到内容是 XML 或 JSON 文档，它会继续处理。否则，它将引发异常。根据您的文档格式，您需要指定相应的内容类型，如表 [5-1](#Tab1) 所示。

表 5-1。

Document Formats and Corresponding Content Type

<colgroup><col> <col> <col></colgroup> 
| 文档格式 | 内容类型 | 特征 |
| --- | --- | --- |
| 可扩展置标语言 | `application/xml, text/xml` | 接受 Solr 定义的任意格式`XML`。功能齐全。 |
| 数据 | `application/json, text/json` | 接受 Solr 定义的任意格式`JSON`。功能齐全。不允许注释。 |
| 战斗支援车 | `application/csv, text/csv` | 标准`CSV`格式。能力有限。 |
| 爪哇滨 | `application/javabin` | 更快。 |

默认情况下，索引操作的响应格式与文档的内容类型相同。如果您想获得不同格式的响应，您需要通过使用`wt`参数来提供响应格式。

### UpdateRequestProcessorChain

Solr 提供了在文档被索引之前修改文档的条款。您可以对文本执行任何处理，如清理、丰富和验证。Solr 允许您修改一个字段的值(例如，设置缺省值)，创建一个新字段(例如，添加当前时间戳)，删除一个字段，甚至过滤文档使其不被索引(例如，在重复文档的情况下)。

这些操作由`UpdateRequestProcessorFactory`实例执行，这些实例可以链接在一起，并在更新处理程序中配置。每当处理程序收到更新请求时，它就执行这个链，这个链运行所有已配置的处理器，然后索引文档。在`solrconfig.xml`中可以定义多个链，但是只能将一个链分配给一个处理程序来执行。

除了作为 core Solr 一部分的`UpdateRequestProcessor`，一些处理器也可以作为 contrib 模块使用，比如 UIMA。您甚至可以编写自己的处理器实现，根据您的定制需求修改文档。以下是如何向更新处理程序注册处理器的示例:

`<updateRequestProcessorChain name="mychain" default="true">`

`<processor class="solr.TimestampUpdateProcessorFactory">`

`<str name="fieldName">timestamp</str>`

`</processor>`

`<processor class="solr.CustomUpdateRequestProcessorFactory">`

`<lst name="name">`

`<str name="name1">value</str>`

`<str name="name2">value</str>`

`</lst>`

`</processor>`

`<processor class="solr.LogUpdateProcessorFactory" />`

`<processor class="solr.RunUpdateProcessorFactory" />`

`</updateRequestProcessorChain>`

那么这个`updateRequestProcessorChain`是干什么的呢？这个链按顺序运行注册的处理器。它运行`TimestampUpdateProcessorFactory`，将当前时间戳设置为`timestamp`字段的默认值。该工厂的输出输入到`CustomUpdateRequestProcessorFactory`。假设这是您的定制处理器，它会根据配置进行一些处理并更新文档。接下来是`LogUpdateProcessorFactory`用更新信息更新日志。最后应该运行的是`RunUpdateProcessorFactory`，因为它最终会更新倒排索引。

Note

如果`RunUpdateProcessorFactory`没有在您的链中注册，将不会发生索引更新。如果它没有被注册为最后一个组件，它后面的处理器将没有任何作用。

要执行任何预处理，链必须在更新处理程序中注册。如果您想为每个请求运行处理，您可以在`solrconfig.xml`中设置它，如下所示:

`<requestHandler name="/myupdate " class="solr.UpdateRequestHandler">`

`<lst name="defaults">`

`<str name="update.chain">mychain</str>`

`</lst>`

`</requestHandler>`

您也可以通过用参数`update.chain`传递名称来为特定请求注册链，如下所示:

`$ curl` `http://localhost:8983/solr/hellosolr/update?update.chain=mychain`

Note

Solr 提供的`UpdateRequestProcessors`的详细列表可以在 Solr-Start 项目、 [`www.solr-start.com/info/update-request-processors/`](http://www.solr-start.com/info/update-request-processors/) 中找到。也可以参考 [`http://lucene.apache.org/solr/5_3_1/solr-core/org/apache/solr/update/processor/UpdateRequestProcessorFactory.html`](http://lucene.apache.org/solr/5_3_1/solr-core/org/apache/solr/update/processor/UpdateRequestProcessorFactory.html) 的 Solr Javadoc。

### UpdateRequestProcessor 与分析器/令牌化器

在前一章中，你学习了分析器和记号赋予器。但是如果`UpdateRequestProcessor`的工作方式确实类似于索引时间分析器，您可能会想为什么需要两个特性。接下来的几段将揭示它们之间的主要区别，并解释为什么我们两者都需要。

分析器背后的思想是修改输入流以生成用于匹配的术语(例如小写和词干)，因为它通过维护标记的位置和偏移信息来提供更好的控制。它可以在索引和查询时应用。实现对被索引的文档进行预处理，例如清理、丰富新字段中的信息以及验证。它仅在索引文档时应用，在搜索时不起作用。

在讨论下一个区别之前，让我们回顾一下 Solr 中文本是如何索引和存储的。

在`schema.xml`中，您如下定义一个字段:

`<field name="title" type="string" indexed="true" stored="true" />`

在这个字段定义中，`indexed="true"`意味着应该为字段创建一个倒排索引，`stored="true"`意味着字段文本应该以不倒排的方式逐字存储在索引中。只能检索存储的字段进行显示。

回到差别，在文档被`RunUpdateProcessorFactory`提交用于索引之后，一个分析器进入画面。它只能转换被索引的文本，而不能转换被存储的文本(未转换的文本值保持不变)。但是一个`UpdateRequestProcessor`接收到了`SolrInputDocument`，它的修改适用于索引和存储的文本。如果您将相同的逻辑放入组件和索引中并检索一些文本，分析器检索的文本将不会被修改，而`UpdateRequestProcessor`的文本将被修改。例如，如果您编写一个算法，根据售出的产品数量来评估产品的受欢迎程度，分析器将为搜索、执行函数查询和获取范围方面索引受欢迎程度；但是如果你想查看受欢迎程度(显示给用户)，你需要写`UpdateRequestProcessor`。

在每个字段的基础上应用分析器。它不允许您从一个字段获取数据并将其添加到另一个字段。是的，在某种程度上你可以通过使用`copyField`来实现，但是灵活性仍然有限。此外，分析器不能应用于非文本原始字段。

## 索引操作

到目前为止，您已经对 Solr 支持的文档格式有了很好的理解。在本节中，您将对不同格式的文档进行索引。本节涵盖了 XML 文档的所有索引操作。对于其他格式，它为您提供了足够的信息来帮助您管理索引。

### XML 文档

在本节中，您将学习如何使用 Solr 的原生 XML 格式添加、删除和更新文档。所谓 Solr 的原生格式，我指的是 Solr 定义的一组标签和属性。要使用 XML 本地格式索引文档，您需要以 Solr 指定的格式准备文档，并将这些文档发送到服务器。

对索引所做的任何更改只有在触发 commit 后才可见，这将结束操作。让我们来看看每一项操作。

#### 增加

你已经在第二章中看到了一个向 Solr 添加文档的例子。`add`命令将提供的文档集索引到 Solr。提醒一下，每个文档都可以比作关系数据库中的一个元组。下面是一个索引 XML 文档的请求示例:

`$ curl` `http://localhost:8983/solr/hellosolr/update`

`-H "Content-Type: text/xml"`

`--data-binary ’<add commitWithin="5000" overwrite="true">`

`<doc>`

`<field name="id">apl1001</field>`

`<field name="product">iPad</field>`

`<field name="model">nano</field>`

`<field name="manufacturer">apple</field>`

`</doc>`

`<doc boost="2.0">`

`<field name="id">apl1002</field>`

`<field name="product">iPhone</field>`

`<field name="model" boost="2.0">iPhone 6</field>`

`<field name="manufacturer">apple</field>`

`<field name="color">gold</field>`

`<field name="color">silver</field>`

`</doc>`

`</add>’`

以下是 Solr 支持的 XML 元素和属性，使用其原生格式对文档进行索引。

*   `add`:该根标签定义了向 Solr 添加文档的操作。它可以构成一个或多个要添加的文档。
    *   `commitWithin`:如果您想确保在指定的时间内自动触发提交，请以毫秒为单位设置该属性的值。或者，您可以调用提交作为请求参数。
    *   `overwrite`:可选属性，默认设置为`true`，用相同的`uniqueKey`覆盖现有文档。如果不希望现有文档被覆盖，将其设置为`false`，那些文档将被忽略。
*   这个标签定义了一个 Solr 文档，并组成了一组字段。
    *   `boost`:使用这个可选的属性来提升文档的索引时间。
*   `field`:定义一个 Solr 字段。对于多值字段，将其多次添加到文档中。
    *   `name`:这个强制属性应该对应`schema.xml`中的一个字段名或者动态字段，除非你运行的是无模式。
    *   `boost`:这个可选属性给字段一个索引时间提升。

在这个例子中，我们为文档 ID `apl1002`定义了两次`color`字段。这就是向多值字段添加值的方法。如果这个字段在`schema.xml`中没有被定义为多值，Solr 将抛出一个异常。

#### 更新

如果添加一个已经存在的文档会怎样？如果指定了`overwrite="true"`，文档将被覆盖；否则，新文档将被忽略。覆盖操作会索引新文档，删除旧文档。如果您想要更新特定字段的值并保留其他字段的值，或者想要向字段的现有值添加一个值，该怎么办？

在内部，Lucene 没有更新文档的功能。如果您想要更新一个字段的值，您需要删除文档并重新添加它。如果添加只有特定字段的文档，其他字段的值将被删除。要更新一个字段，您需要从 Solr 或您的主数据源获取所有其他字段的值，准备一个新文档，并添加它。

但是不用担心；Solr 的原子更新特性通过获取现有值消除了准备文档的痛苦。您需要执行以下操作来执行原子更新:

*   在`schema.xml`中，将所有字段设置为`stored="true"`，以便 Solr 可以在内部执行一个获取并准备一个新文档。使用`copyField`填充的所有字段可以保持不存储。
*   在`solrconfig.xml`中，注册`<updateLog/>`。通过这种设置，原子更新可以确保获得索引文档的最新版本。
*   在索引文档时，传递一个附加属性`update`。它的值可以是下列值之一:
    *   `set`:设置字段的值。如果存在旧值，它将被覆盖。如果要删除现有值，将值设置为`null`。
    *   `add`:向字段的现有值添加新值。该字段应该是多值的。
    *   `remove`:从字段中删除指定的值。该字段应该是多值的。
    *   `removeregex`:删除所有匹配指定正则表达式模式的值。该字段应该是多值的。
    *   `inc`:将数值字段的值增加所提供的值。该字段应为单值和数字。

以下示例将`product`字段的值设置为`iPod`，并将银色、金色和粉色添加到索引中，用于具有唯一 ID `apl1001`的文档:

`<add>`

`<doc>`

`<field name="id">apl1001</field>`

`<field name="product" update="set">iPod</field>`

`<field name="color" update="add">silver</field>`

`<field name="color" update="add">gold</field>`

`<field name="color" update="add">pink</field>`

`</doc>`

`</add>`

#### 删除

`delete`命令标记要删除的文档。在下一次提交时，这些文档将从索引中永久删除，并且不再可搜索。您可以通过指定文档的唯一 ID 或指定查询来将文档标记为删除。以下是在 Solr 中删除文档的示例。

Delete by ID: deletes the documents with id apl1001 and apl1002

`<delete>`

`<id>apl1001</id>`

`<id>apl1002</id>`

`</delete>`

Delete by query: deletes all the documents that contain iPad as the product name

`<delete>`

`<query>product:iPad</query>`

`</delete>`

Delete by ID and query combined: deletes the document with id apl1001 and all the products of type iPad

`<delete>`

`<id>apl1001</id>`

`<query>product:iPad</query>`

`</delete>`

正如您可以在搜索时使用`*:*`选择所有文档一样，您也可以用同样的方式删除所有文档。

Delete all the documents in ‘hellosolr’ core

`$ curl` `http://localhost:8983/solr/hellosolr/update`

`-H "Content-Type: text/xml"`

`--data-binary ’<delete><query>*:*</query></delete>’`

#### 犯罪

在 SQL 数据库中，您在更新后执行提交。类似地，Solr 也需要提交，只有在内核上触发了提交之后，更改才是可搜索的。您可以按如下方式执行提交:

`$ curl``http://localhost:8983/solr/hellosolr/update`T2】

`--data-binary ’<commit/>’`

此外，您可以通过向更新处理程序传递一个额外的`commit`参数来触发提交:

`$ curl` `http://localhost:8983/solr/hellosolr/update?commit=true`

Solr 支持两种类型的提交:硬提交和软提交。

##### 硬提交

硬提交使您的数据可搜索，并将更改保存到磁盘。前面的命令会触发硬提交。您还可以配置`solrconfig.xml`在指定的持续时间(以毫秒为单位)之后或者当指定数量的文档被添加到核心时自动提交文档。

Perform hard commit when 50,000 documents are added or every 5 minutes, whichever occurs earlier

`<autoCommit>`

`<maxDocs>50000</maxDocs>`

`<maxTime>300000</maxTime>`

`</autoCommit>`

##### 软提交

因为硬提交会将文档保存到辅助存储中，所以操作成本很高。Solr 4.0 引入了软提交的概念，这使得添加的文档可以立即被搜索到，但是需要依赖硬提交来实现持久性。软提交有助于实现接近实时的搜索，但也带来了一个代价:如果系统在下一次硬提交之前崩溃，更改将会丢失。您可以按如下方式配置软提交:

Perform soft commit when 5,000 documents are added or every 5 seconds, whichever occurs earlier

`<autoSoftCommit>`

`<maxDocs>5000</maxDocs>`

`<maxTime>5000</maxTime>`

`</autoSoftCommit>`

您通常喜欢更频繁地执行软提交，但是要确保它们不会太频繁，以至于在第一次提交完成之前另一次提交就开始了。您应该减少执行硬提交的频率，但是持续时间不应该太长，因为崩溃会导致数据丢失。这些值应该根据您的要求适当设置。

#### 使最优化

Lucene 索引由更小的称为段的块组成。在添加文档的过程中，Solr 会在每次硬提交时创建新的段，并不时地合并它们。当段数增加时，查询需要更多的时间；执行合并加快了查询速度，但这是一个需要大量磁盘交换的高成本操作。Lucene 根据合并策略自动合并段，但是如果你想强制合并，你可以调用`optimize`。它执行硬提交，然后将数据段合并成一个数据段。因为这是一项开销很大的操作，所以应该减少执行频率(例如，作为夜间作业)。

`$ curl` `http://localhost:8983/solr/hellosolr/update?optimize=true`

`$ curl` `http://localhost:8983/solr/hellosolr/update`

`-H "Content-Type: text/xml" --data-binary ’<optimize/>’`

不调用`optimize`，可以考虑在`solrconfig.xml`中将合并因子设置为一个较低的值:

`<mergeFactor>10</mergeFactor>`

#### 反转

与数据库类似，您可以回滚未提交的更改:

`$ curl` `http://localhost:8983/solr/hellosolr/update?rollback=true`

`$ curl` `http://localhost:8983/solr/hellosolr/update`

`-H "Content-Type: text/xml" --data-binary ’<rollback/>’`

### JSON 文档

Solr 使您能够在 Solr 指定的结构以及您的定制结构中索引 JSON 文档。在 Solr 指定的 JSON 结构中索引文档的过程与前面描述的 XML 文档的过程相同。您需要做的就是将内容类型指定为`application/json`或`text/json`，并提供 JSON 数据。

这里，我们使用与前面 XML 文档中相同的数据来索引 JSON 文档:

`$ curl``http://localhost:8983/solr/hellosolr/update`T2】

`{`

`"id": "apl1001",`

`"product": "iPad",`

`"model":"nano",`

`"manufacturer":"apple"`

`},`

`{`

`"id": "apl1002",`

`"product": "iPhone",`

`"model": {`

`"value": "iPhone 6",`

`"boost": 2.0`

`},`

`"color": ["gold", "silver"]`

`}`

`]’`

同样，您可以执行其他操作，如删除或提交，如下所示:

`$ curl -X POST -H ’Content-Type: application/json’ ’``http://localhost:8983/solr/hellosolr/update’`T2】

`{`

`"commit": {},`

`"delete": { "query":"*:*" }`

`}’`

如果 JSON 数据不是 Solr 指定的样式，而是遵循任意结构，那么可以通过在更新请求中传递额外的映射参数来索引它。有关附加参数的详细信息，请参考 Solr 官方文件 [`https://cwiki.apache.org/confluence/display/solr/Uploading+Data+with+Index+Handlers`](https://cwiki.apache.org/confluence/display/solr/Uploading+Data+with+Index+Handlers) 。

### CSV 文档

如果您的数据是 CSV 格式的，那么建立索引就非常简单。您不需要遵循 Solr 指定的格式，因为值只是用逗号分隔。您可以分两步索引一个简单的 CSV 文件:

Map the values to Solr fields either by specifying comma-separated field names on the first line of the file or by specifying the comma separated names as the value of the `fieldnames` parameter in the request.   Set the content type as `text/csv` or `application/csv`. Alternatively, you can make a request without passing content-type information, if you call `/update/csv` handler instead of `/update`.  

下面是一个索引`books.csv`的请求示例，它随 Solr 包一起提供:

`$ curl` `http://localhost:8983/solr/hellosolr/update/csv`

`--data-binary @books.csv -H ’Content-Type:text/plain’`

以下是重要的参数，您可以随请求一起传递:

*   `separator`:如果逗号不是您的默认分隔符，请将这个附加参数与适用的分隔符一起传递。
*   `skip`:如果您不想索引 CSV 中的所有字段，请指定要跳过的字段的逗号分隔列表。
*   `skipLines`:指定您不想索引的第一行的数量。
*   字段的文本包含逗号是很常见的，这也是默认的分隔符。您可以为文件指定一个封装器，并用它包围该值。此外，您可以指定特定于字段的封装器。
*   `split`:为多值字段索引数据，需要`split`布尔参数。指定`split=true`在所有`multiValued`字段上启用拆分，指定`f.<fieldname>.split=true`在特定字段上应用拆分。此外，您需要提供分隔符，这可以使用特定于字段的分隔符来完成。

下面是一个几乎完全成熟的 CSV 请求，它对我们用于 XML 和 JSON 的相同数据进行索引，但采用 CSV 格式:

`$ curl``http://localhost:8983/solr/hellosolr/update?commit=true&split=true&f.color.separator=,&f.color.encapsulator`T2】

`-H "Content-Type: text/xml"`

`--data-binary ’`

`id,product,model,manufacturer,color`

`apl1001,iPad,nano,apple,`

`apl1002,iPhone,Person,iPhone 6,apple,"gold,silver"’`

### 索引丰富的文档

如果您有为书籍、期刊或杂志编制索引的业务需求，那么这些文档很可能是 PDF 或 Word 格式的。Solr 提供了一个称为 Solr Cell(以前称为内容提取库)的框架，它以二进制格式从文件中提取文本和元数据。这个框架由一个名为`ExtractingRequestHandler`的处理程序公开。

在索引富文档时，您需要指定 MIME 类型。如果没有提供信息，Solr 将尝试自动检测文档类型。它使用 Tika 解析器来识别文档类型并提取内容和元数据。提取的内容被添加到内容字段，元数据被添加到根据诸如 Dublin Core 的规范定义的字段。

Apache Tika 是一个专注于从一千多种文件类型中检测和提取元数据和文本的项目。该项目首页 [`https://tika.apache.org/`](https://tika.apache.org/) 提供了更多详情。

以下是索引 PDF 文件的步骤:

Add dependencies to `lib` directives in `solrconfig.xml`: `<lib dir="../../../contrib/extraction/lib" regex=".*\.jar" />` `<lib dir="../../../dist/" regex="solr-cell-\d.*\.jar" />`   Configure the `ExtractionRequestHandler` in `solrconfig.xml`: `<requestHandler name="/update/extract"` `class="org.apache.solr.handler.extraction.ExtractingRequestHandler">`   `<lst name="defaults">`     `<str name="lowernames">true</str>`     `<str name="uprefix">others_</str>`     `<str name="captureAttr">true</str>`     `<str name="fmap.a">url</str>`     `<str name="fmap.div">others_</str>`   `</lst>` `</requestHandler>` The following points describe the specified parameters: `lowernames`: Setting this Boolean parameter to true, maps the field names to names with lowercase and underscores, if required. For example, “`Author-Name`” will be mapped to “`author_name`”. `uprefix`: The fields that are undefined in Solr are prefixed with the value specified here. You can define a dynamic field with the same pattern to handle the contents appropriately. The next step, demonstrates a dynamic field to ignore the undefined fields. `captureAttr`: Setting this Boolean parameter to true will even index the values of attributes extracted from XHTML elements. `fmap.<source_field>`: This parameter allows you to rename the field. The specified source field gets renamed to the value of the parameter. In this configuration, a field with name “a” will get renamed to “url”, for example.   Define appropriate fields in `schema.xml`: `<field name="id" type="string" indexed="true" stored="true"` `required="true"/>` `<field name="content" type="string" indexed="true" stored="true"` `multiValued="true"/>` `<field name="author" type="string" indexed="true" stored="true"` `multiValued="true"/>` `<field name="title" type="text" indexed="true" stored="true"/>` `<dynamicField name="others_*" type="ignored" />`   Index the document: `$ curl ’` `http://localhost:8983/solr/hellosolr/update/extract?literal.id=doc1&commit=true’` `-F` `myfile=@example/exampledocs/solr-word.pdf`  

## 示例

`DataImportHandler`是一个 contrib 模块，允许您从各种来源导入数据。如果 RDBMS 是您的主要数据存储，或者您需要从任意 XML 导入数据，那么`DataImportHandler`是一个很好的起点。只需使用 XML 配置，您就可以让一切就绪。

对于数据库，您可以从多个表中导入数据，方法是执行连接并展平结果以将其作为 Solr 文档进行索引。您还可以嵌套查询，为父查询提取的每一行执行子查询。一旦获取了一行，就可以在索引之前应用转换器来修改文档。`DataImportHandler`允许您执行以下两种类型的更新:

*   完全导入:完全导入类似于完全转储，从表中提取所有记录并对其进行索引。
*   Delta import:这将执行增量更新，只获取自上次导入以来添加/修改过的文档。

`DataImportHandler`不限于数据库。有了它，您可以用以下任何一种格式来索引文档:

*   任意 XML:使用 XSLT 进行转换。
*   电子邮件:使用 JavaMail API。
*   Word/PDF 文档:使用 Apache Tika。这是使用 Solr Cell 索引文档的替代方法。

### 从 RDBMS 导入

在继续下一步之前，您需要理解数据导入的重要元素:

*   dataSource: `dataSource d`定义从哪个数据源读取数据以及如何连接。
*   实体:生成单据的`entity`。例如，您可以创建一个从表中读取每一行并创建一个文档的`entity`。
*   处理器:它从数据源中提取内容，并在转换后将其添加到索引中(如果有的话)。缺省值是`SqlEntityProcessor`，它适用于关系数据库。`entity`元素支持`processor`属性，该属性允许您指定适用的处理器名称。
*   transformer:允许您转换文档(例如，拆分数据或剥离 HTML)。

下面是从 HSQLDB 中的`items`表导入数据的步骤:

Add dependencies to the `lib` directives in `solrconfig.xml`: `<lib dir="../../../dist/" regex="solr-dataimporthandler-.*\.jar" />`   Define a handler in `solrconfig.xml`: `<requestHandler name="/dataimport"` `class="org.apache.solr.handler.dataimport.DataImportHandler">`     `<lst name="defaults">`       `<str name="config">data-config.xml</str>`     `</lst>` `</requestHandler>` `data-config.xml` contains all the information regarding which source to read the data from, how to read the data, and how to map it with Solr fields and apply any transformation to the document, if needed.   Create `data-config.xml` in the `conf` directory: `<dataConfig>` `<dataSource driver="org.hsqldb.jdbcDriver"` `url="jdbc:hsqldb:example/example-DIH/hsqldb/ex"` `user="sa" batchSize="10000"/>`     `<document name="products">`         `<entity name="item" query="select * from item">`             `<field column="ID" name="id" />`             `<field column="NAME" name="name" />`             `<entity name="feature"` `query="select description from feature where item_id=’${item.ID}’">`                 `<field name="features" column="description" />`             `</entity>`             `<entity name="item_category"` `query="select CATEGORY_ID from item_category where item_id=’${item.ID}’">`                 `<entity name="category"` `query="select description from category` `where id = ’${item_category.CATEGORY_ID}’">`                     `<field column="description" name="cat" />`                 `</entity>`             `</entity>`         `</entity>`     `</document>` `</dataConfig>`   Define the fields in `schema.xml`: `<field name="id" type="string" indexed="true" stored="true"` `required="true"/>` `<field name="name" type="string" indexed="true" stored="true"` `multiValued="false"/>` `<field name="features" type="string" indexed="true" stored="true"` `multiValued="true"/>` `<field name="description" type="string" indexed="true" stored="true"` `multiValued="true"/>`   Trigger the data import: `$ curl` `http://localhost:8983/solr/hellosolr/dataimport?command=full-import`  

## 文档预处理

正如您已经知道的，一个`UpdateRequestProcessor`可以修改或删除一个字段，创建一个新的字段，甚至跳过一个被索引的文档。在这一节中，您将看到一些重要的`UpdateRequestProcessors`，它们可以用来修改和丰富文档。本节提出了一个问题或需求，然后解释了在这种情况下您将使用的更新处理器。要使用处理器，必须在`updateRequestProcessorChain`中注册，如下所示:

`<updateRequestProcessorChain name="customchain">`

`<!--`

`Your custom processor goes here.`

`-->`

`<processor class="solr.LogUpdateProcessorFactory" />`

`<processor class="solr.RunUpdateProcessorFactory" />`

`</updateRequestProcessorChain>`

请记住,`RunUpdateProcessorFactory`应该始终是您在链中注册的最后一个处理器。

这个`updateRequestProcessorChain`必须添加到处理程序的更新链中，或者应该为每个请求提供`update.chain`参数。您可以将它添加到请求处理程序中，如下所示:

`<requestHandler name="/update" class="solr.XmlUpdateRequestHandler" >`

`<lst name="defaults">`

`<str name="update.chain">customchain</str>`

`</lst>`

`</requestHandler>`

### 语言检测

多语言数据是常见的，语言检测 contrib 模块在这里派上了用场。它有助于检测输入文档的语言，并允许您将语言信息存储在单独的字段中。Solr 提供了`updateRequestProcessorChain`的两个实现:

*   使用阿帕奇 Tika。
*   `LangDetectLanguageIdentifierUpdateProcessorFactory`使用在 [`https://github.com/shuyo/language-detection`](https://github.com/shuyo/language-detection) 可用的语言检测项目。

在本节中，您将看到如何使用`LangDetect`项目来配置语言检测。以下是步骤:

Add dependencies to the `lib` directives in `solrconfig.xml`: `<lib dir="../../../contrib/langid/lib" regex=".*\.jar" />` `<lib dir="../../../dist/" regex="solr-langid-\d.*\.jar" />`   Register the `LangDetect` processor to the `UpdateRequestProcessorChain` in `solrconfig.xml`: `<processor` `class="org.apache.solr.update.processor.LangDetectLanguageIdentifierUpdateProcessorFactory">`   `<lst name="defaults">`     `<str name="langid.fl">title,abstract,content</str>`     `<str name="langid.langField">lang</str>`   `</lst>` `</processor>`   Define the fields in `schema.xml`: `<field name="title" type="string" indexed="true" stored="true" />` `<field name="abstract" type="string" indexed="true" stored="true" />` `<field name="content" type="string" indexed="true" stored="true" />` `<field name="lang" type="string" indexed="true" stored="true" />`  

现在，当您将该处理器添加到链中，并将该链注册到更新处理程序并索引文档时，`lang`字段将根据在文档的`title`、`abstract`和`language`字段中检测到的语言自动填充。

### 生成唯一 ID

如果被索引的文档没有惟一的 ID，并且您希望 Solr 自动为每个文档分配一个 ID，那么您需要在更新链中配置`UUIDUpdateProcessorFactory`。以下是步骤:

Register the `UUIDUpdateProcessorFactory` to `UpdateRequestProcessorChain` in `solrconfig.xml`: `<processor class="solr.UUIDUpdateProcessorFactory">`   `<str name="fieldName">id</str>` `</processor>`   Add the `id` field of type String or UUID to hold the unique IDs generated by the processor, in `schema.xml`: `<field name="id" type="string" indexed="true" stored="true" required="true"/>`   Optionally, register the `id` field as a `uniqueKey`: `<uniqueKey>id</uniqueKey>`  

如果被索引的文档不包含`id`值，将生成一个随机 ID 并索引到该字段。如果文档包含一个值，那么 Solr 不会生成它，而是使用文档提供的值。

### 重复数据删除

`UniqueKey`可以确保没有重复的文档被编入索引，但是它根据字段中的值来识别重复的文档。如果您的需求更复杂呢？假设你有一个包含汽车特征信息的系统；制造商、型号和年份创建了一个唯一的实体，因此您希望确保没有两个文档是使用相同的制造商、型号和年份信息进行索引的。可以用`SignatureUpdateProcessorFactory`来实现这个功能。

当您添加文档时，`SignatureUpdateProcessor`根据`fields`参数中提供的一组字段，使用`signatureClass`中配置的签名生成算法生成一个签名，并将其写入`signatureField`。对于精确的重复检测，可以使用`MD5Signature`或 [`Lookup3Signature`](https://wiki.apache.org/solr/Lookup3Signature) 实现，对于模糊或近似的重复检测，可以使用 [`TextProfileSignature`](https://wiki.apache.org/solr/TextProfileSignature) 。以下是配置重复数据删除的步骤:

Register the `SignatureUpdateProcessorFactory` to `UpdateRequestProcessorChain` in `solrconfig.xml`: `<processor class="solr.processor.SignatureUpdateProcessorFactory">`   `<bool name="enabled">true</bool>`   `<str name="signatureField">signature</str>`   `<bool name="overwriteDupes">false</bool>`   `<str name="fields">make,model,year</str>`   `<str name="signatureClass">solr.processor.Lookup3Signature</str>` `</processor>`   Add a signature field in `schema.xml`: `<field name="make" type="string" stored="true" indexed="true" multiValued="false"/>` `<field name="model" type="string" stored="true" indexed="true" multiValued="false"/>` `<field name="year" type="string" stored="true" indexed="true" multiValued="false"/>` `<field name="signature" type="string" stored="true" indexed="true" multiValued="false"/>`  

您还可以将签名添加到现有字段，如`id`，而不是添加到单独的字段`signature`。

### 文档到期

在许多情况下，您会希望文档只在有限的时间范围内有效。例如，对于电子商务网站上的闪购，您希望文档在特定时间或特定时间段后过期。Solr 提供了自动删除过期文档的`DocExpirationUpdateProcessorFactory`。过期文档列表是根据配置的过期字段确定的。自动删除由工厂分叉的后台线程完成，该线程每 N 秒唤醒一次并执行一次`deleteByQuery`。

该处理器提供两种功能。首先，它计算文档的到期日期，并根据提供的生存时间(TTL)值将其填充到一个字段中。TTL 表示所提供的文档具有有限的寿命，并且应该在 N 秒后过期。处理器允许您以两种方式提供 TTL 信息:

*   `_ttl_`请求参数:更新请求中所有文档过期的持续时间。
*   `_ttl_`字段:该字段的值为该文档提供 TTL，并覆盖`_ttl_`请求参数。

为了澄清任何混淆，`_ttl_`字段是每个文档的，`_ttl_`请求参数是全局的。基于所提供的 TTL 信息，处理器计算来自`NOW`的到期日期，并将其存储在到期字段中。以下步骤配置文档到期时间:

Register the `DocExpirationUpdateProcessorFactory` to `UpdateRequestProcessorChain` in `solrconfig.xml`: `<processor class="solr.processor.DocExpirationUpdateProcessorFactory">`   `<str name="expirationFieldName">_expire_me_at_</str>` `</processor>` This populates the `_expire_me_at_` field of the document, with the expiry time based on the `_ttl_` parameter. No automatic deletion will happen. You either need to delete the documents manually or you can hide the documents from Solr requests by adding a filter query such as `fq=-_expire_me_at_:[* TO NOW]`. Along with setting the expiration time, the second functionality this processor provides is automatic deletion of a document. If you want to delete the documents automatically, you can add `autoDeletePeriodSeconds` to the processor. This triggers the deletion thread every N seconds: `<processor class="solr.processor.DocExpirationUpdateProcessorFactory">`   `<int name="autoDeletePeriodSeconds">300</int>`   `<str name="expirationFieldName">_expire_at_</str>` `</processor>`   Add `_expire_me_at_ field` in `schema.xml`: `<field name="_expire_me_at_" type="string" stored="true" indexed="true" multiValued="false"/>`   Index the documents. When you index the documents, provide the TTL information for expiration to work: `$ curl -X POST -H ’Content-Type: application/xml’ ’` `http://localhost:8983/solr/hellosolr/update?commit=true` `&_ttl_=+4HOURS’ -d ’<add>` `<doc>`   `<field name="id">1</field>`   `<field name="title">This title will persist for 4 hours</field>`   `<field name="abstract">This abstract will persist for 4 hours</field>`   `<field name="content">This content will persist for 4 hours</field>` `</doc>` `<doc>`   `<field name="id">2</field>`   `<field name="title">This title will persist for 30 minutes</field>`   `<field name="abstract">This abstract will persist for 30 minutes</field>`   `<field name="content">This content will persist for 30 minutes</field>`   `<field name="_ttl_">+30MINUTES</field>` `</doc>` `<add>’`  

## 索引性能

除了我们所讨论的一切，索引性能是一个需要考虑的关键因素。企业搜索需要处理不同数量的数据，这些数据可以大致分为小型、中型或大型。除了体积，另一个重要因素是速度。一些搜索引擎需要高速索引，以处理大量数据或支持接近实时的搜索需求。

对 Solr 索引性能的深入探究超出了本书的范围。本节提供了索引性能的基本概述；第 10 章提供了更多细节。

根据数据量和索引速度要求，您需要做出必要的设计决策和定制，其中一些如下所示:

*   Solr 架构:基于数据量、可伸缩性需求和容错需求，应该设计一个合适的架构。
*   索引工具:索引工具有不同的功能；一个提供简单性，另一个提供快速索引。例如，`DataImportHandler`是单线程的，如果您要索引大量数据，您可能希望有一个定制的多线程实现。
*   CPU 和内存优化:许多 Solr 特性都可以优化 CPU 和内存利用率。`solrconfig.xml`文件还提供了定制资源利用的配置。

表 [5-2](#Tab2) 提供了关于 Solr 架构和索引工具的信息，可以根据数据量考虑这些工具。

表 5-2。

Data Volume Decisions

<colgroup><col> <col> <col></colgroup> 
| 数据卷宗 | 太阳能建筑 | 索引工具 |
| --- | --- | --- |
| 小的 | 独立复制 | SimplePostTool 数据导入处理程序 |
| 中等 | 共享和复制的 solrcloud | 索勒 |
| 大型 | 索尔鲁德 | 索勒 |

以下是一些可以用来优化索引性能的措施:

*   执行批量写入，并根据可用资源找到最佳的批量大小。
*   仅存储您真正需要存储的字段。
*   仅索引真正需要搜索的字段。
*   如果可能的话，避免昂贵的预处理和文本分析。
*   避免频繁的硬提交。

## 定制组件

Solr 为大多数常见问题和重要用例提供了现成的实现，包括它的每个组件，比如`RequestHandler and UpdateRequestProcessor`。在本章中，您看到了用于索引 XML、从数据库导入和从 PDF 文件提取的`RequestHandler`的实现；以及针对生成唯一 ID 和重复数据删除等需求的`UpdateRequestProcessor`实现。但是您可能需要一个 Solr 没有提供的特性，或者您可能想要编写一个定制的实现。对于这样的场景，Solr 允许您通过扩展现有的基类并将它们插入 Solr 来开发定制组件。

在本节中，您将学习扩展`UpdateRequestProcessor`并插入一个自定义逻辑。

### 自定义更新 request 处理器

到目前为止，你一直使用 Solr 提供的`UpdateRequestProcessors`或者作为 contrib 模块。Solr 还允许你编写自己的`UpdateRequestProcessor`并链接到`updateRequestProcessorChain`。下面是编写定制处理器的步骤:

Extend the `UpdateRequestProcessorFactory`.   Override the `getInstance()` method to create an instance of `CustomUpdateProcessor`. If you want to do some initialization on core load, you can override the `init()` method.   Your `CustomUpdateProcessor` should extend the abstract class `UpdateRequestProcessor`.   Override the `processAdd()` method, to hook in your code to process the document being indexed. This is the method where all the action happens. Its argument `AddUpdateCommand` has the `getSolrInputDocument()` method, which contains a reference to the `SolrInputDocument`.   Get the values from the fields in `SolrInputDocument`, do your processing, update or delete the value of those fields, or add the result to a new field altogether.   `super.processAdd()` should always be the last statement of the method.  

下面是一个定制更新处理器的例子，如果一个文档的下载次数超过 100 万，它会添加`popular`字段并将其值设置为`true`。这需要进行以下更改:

Write your custom code in Java: Custom implementation of UpdateRequestProcessorFactory `package com.apress.solr.pa.chapter05` `.processor;` `import java.io.IOException;` `import org.apache.solr.common.SolrInputDocument;` `import org.apache.solr.common.util.NamedList;` `import org.apache.solr.request.SolrQueryRequest;` `import org.apache.solr.response.SolrQueryResponse;` `import org.apache.solr.update.AddUpdateCommand;` `import org.apache.solr.update.processor.UpdateRequestProcessor;` `import org.apache.solr.update.processor.UpdateRequestProcessorFactory;` `public class CustomUpdateProcessorFactory extends UpdateRequestProcessorFactory` `{` `/**` `* Initialize your factory.` `* This method is not mandatory.` `*/` `public void init(NamedList args) {`         `super.init(args);` `}` `@Override` `public UpdateRequestProcessor getInstance(SolrQueryRequest req,` `SolrQueryResponse rsp, UpdateRequestProcessor nxt)`   `{`     `return new CustomUpdateProcessor(nxt);`   `}` `}` `class CustomUpdateProcessor extends UpdateRequestProcessor` `{`   `public CustomUpdateProcessor ( UpdateRequestProcessor nxt) {`     `super( nxt );`   `}`   `@Override`   `public void processAdd(AddUpdateCommand cmd) throws IOException {`     `SolrInputDocument doc = cmd.getSolrInputDocument();`     `Object obj = doc.getFieldValue( "downloadcount" );`     `if( obj != null ) {`       `int dc = Integer.parseInt( String.valueOf(obj) );`       `if( dc > 1000000 ) {`         `doc.setField("popular", true);`       `}`     `}`     `// you must make this call`     `super.processAdd(cmd);`   `}` `}`   Add the executable JAR of the program to `lib` and register the processor in `solrconfig.xml`: `<lib dir="../lib" regex="solr-practical-approach-*.jar" />` `<requestHandler name="/update" class="solr.UpdateRequestHandler">`   `<lst name="defaults">`     `<str name="update.chain">customprocessor</str>`   `</lst>` `</requestHandler>` `<updateRequestProcessorChain name="customprocessor">`   `<processor class="com.apress.solr.pa.chapter``05``.processor.CustomUpdateProcessorFactory" />`   `<processor class="solr.LogUpdateProcessorFactory" />`   `<processor class="solr.RunUpdateProcessorFactory" />` `</updateRequestProcessorChain>`   Add the required field to `schema.xml`: `<field name="id" type="string" indexed="true" stored="true" required="true"/>` `<field name="downloadcount" type="int" indexed="true" stored="true" />` `<field name="popular" type="boolean" indexed="true" stored="true" default="false" />`   Index documents: `$ curl -X POST -H ’Content-Type: application/xml’` `’` `http://localhost:8983/solr/apress/update?commit=true` `’ -d ’<add>` `<doc>`   `<field name="id">123</field>`   `<field name="downloadcount">1200000</field>` `</doc>` `</add>’`  

当您索引此文档时，值`true`将自动被索引并存储在`popular`字段中。

## 经常出现的问题

本节探讨为文档编制索引时遇到的常见问题。

### 将多个字段复制到单值字段

为文档编制索引时，请确保不要将多个字段添加到单值字段中。这将引发一个异常。例如，如果您试图索引下面的文档，该文档包含多个`text`字段的值，并且在`schema.xml`中被定义为`multiValued="false"`，Solr 将抛出一个异常。因此，您要么需要将文本字段设置为`multiValued="true"`，要么只为每个文档提供一个文本元素。

schema.xml:

`<field name="text" type="string" indexed="true" stored="true" multiValued="false"/>`

XML document:

`<add>`

`<doc>`

`<field name="id">1</field>`

`<field name="text">Red Wine</field>`

`<field name="text">White Wine</field>`

`</doc>`

`</add>`

#### 文档缺少必需的唯一键字段

如果您在`schema.xml`中定义了`UniqueKey`，则必须为该字段提供一个值。如果被索引的文档不包含该字段，Solr 将抛出以下异常:

`Document is missing mandatory uniqueKey field: <field>`

如果您的用例不需要惟一键，那么为`schema.xml`中的`uniqueKey`元素提供一个附加属性:

`<uniqueKey required="false">id</uniqueKey>`

#### 数据未编入索引

如果您的索引过程没有索引数据，那么检查错误和异常的最佳位置是日志文件。如果日志文件中没有任何信息，并且您已经修改了处理器链，可能的原因是您在链接处理器时遗漏了某些内容。确保`RunUpdateProcessorFactory`是链中的最后一个处理器。如果您已经编写了自定义处理器，请检查它是否将引用传递给了链中的下一个处理器。

#### 索引很慢

由于多种原因，索引可能会很慢。以下是可以提高索引性能的因素，使您能够相应地调整您的设置:

*   内存:如果分配给 JVM 的内存很低，垃圾收集将被更频繁地调用，索引将会很慢。
*   索引字段:索引字段的数量影响索引大小、内存需求和合并时间。仅索引您希望可搜索的字段。
*   合并因素:合并段是一项开销很大的操作。合并因子越高，索引越快。
*   提交频率:提交频率越低，索引速度越快。
*   批量大小:在每个请求中索引的文档越多，索引就越快。

请记住，这些因素都有利弊。考虑系统的内存和搜索要求，为索引过程提供最佳设置。

#### OutOfMemoryError—Java 堆空间

[第 2 章](02.html)介绍了`OutOfMemoryError`，但是我想再讨论一下，因为索引任务是内存密集型的，如果没有适当的计划，你可能会得到一个异常。当您索引文档时，Solr 将它们全部放入内存，因此文档大小很重要。为了避免这个错误，您可以增加分配给 JVM 的内存。另一个解决方案是以较小的块来索引文档。如果您使用的是`DataImportHandler`，`it`提供了`batchSize`属性来批量获取数据并减少内存使用。

另一个要考虑的重要因素是提交时间。Solr 将自上次提交以来的所有文档索引保存在堆内存中，并在提交时释放它。要修复`OutOfMemoryError`，您可以在`solrconfig.xml`中更改`autoCommit`的频率，以在更短的持续时间或添加更少的文档时触发，如下所示:

`<autoCommit>`

`<maxDocs>100000</maxDocs>`

`<maxTime>60000</maxTime>`

`</autoCommit>`

`<autoSoftCommit>`

`<maxDocs>5000</maxDocs>`

`<maxTime>5000</maxTime>`

`</autoSoftCommit>`

## 摘要

在本章中，您学习了如何为文档编制索引。您已经看到 Solr 支持包括 XML 和 JSON 在内的各种输入格式，以及包括文件系统和数据库在内的各种输入源。为了索引数据，你需要某种工具来做索引。您了解了 Solr 附带的工具、操作系统提供的实用程序以及其他可用于索引文档的库。

我们并不总是幸运地拥有正确的数据格式和结构。Solr 使您能够在文档被索引之前对其进行预处理和丰富。它提供了一组处理器，可用于转换被索引的文档。如果捆绑或贡献的处理器不符合您的需要，您可以编写自己的处理器并将其添加到链中。您学习了链接进程和编写定制的处理器。

文档被编入索引后，就可以进行搜索查询了。在下一章，你将学到你一直在等待的东西:如何在 Solr 中搜索文档。