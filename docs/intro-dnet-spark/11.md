# 十一、DeltaLake

Delta Lake 是 Apache Spark 的扩展，由 Apache Spark 背后的公司 Databricks 创建，并作为一个独立的开源项目发布。Delta Lake 的目标是在企业环境中高效地写入数据湖，无论您拥有哪种类型的数据湖，无论是 Azure 数据湖存储、AWS S3 还是 Hadoop。Delta Lake 将关系数据库(如 Microsoft SQL Server 或 Oracle)的 ACID 属性带到了远程文件系统(如数据湖)中。

## 酸

当我们使用 RDBMS(如 Microsoft SQL Server 或 Oracle)时，我们会讨论 ACID 属性，以及我们如何能够向数据库发送读写请求，RDBMS 会为我们处理 ACID 属性。酸的性质可以描述为

*   **原子性**–读取和写入发生在它们的事务中，要么完全完成，要么完全失败。

*   **一致性**–数据绝不会处于损坏状态，并且必须有效。数据还必须通过任何数据约束，以确保数据不仅从格式的角度来看是有效的，而且符合预期，例如是否允许空值。

*   **隔离**–一个事务不能影响另一个正在进行的事务。

*   **持久性**–一旦事务被提交，那么即使系统停止运行，它也会保持提交状态。

为了理解这对 Apache Spark 意味着什么，让我们看看之前发生了什么，当你将一些数据写到文件系统时会发生什么，在这个例子中，是 Azure 数据湖存储。

假设您有一个 Apache Spark 应用程序，它将使用 parquet 格式写入一些数据。在写入数据的选项中，您指定“覆盖”，这将导致任何数据被覆盖。在这种情况下，您需要确保您是唯一写入数据的进程，因为任何其他进程都会覆盖您的数据。除非您有一些外部进程导致一个且只有一个作业写入特定文件夹，否则您会发现需要解决复杂的计时问题。

另一个潜在的问题是，如果 Apache Spark 正在写入一个目录，而写入中途失败了，那么会发生什么。读者应该怎么做？他们会知道数据不完整吗？即使他们意识到数据不完整，也不可能回到以前被覆盖的数据。

Apache Spark 的另一个问题是从包含大量文件的目录中读取。评估要读入的文件列表是非常昂贵的，并且会降低从数据湖中读取的作业的速度。

对于传统 RDBMS 中的表，我们可以读取、插入、更新、删除或合并，这是插入、更新和删除的组合。有了数据湖，我们有时可以通过添加新文件来添加文件，或者我们可以覆盖文件，但我们不能打开一个 parquet 文件，找到一些行，然后更新或删除它们。我们在 RDBMS 中处理表的传统方式不能转化为基于文件的数据湖。

引入 DeltaLake 是为了解决这些问题。Delta Lake 将 ACID 属性从 RDBMS 带到了基于文件的数据湖，并能够对数据湖中的文件运行插入、更新、删除甚至合并语句，同时还修复了同一目录中大量文件的缓慢读取性能，并修复了多个写入程序的问题以及当写入程序失败并留下不完整数据时会发生什么情况。Delta Lake 改进了所有这一切，并使您能够回滚到以前的数据版本。

这种类型的并发控制称为“多版本并发控制”或 MVCC，在这种控制中，原始数据保持不变，但提供一些其他方法来存储版本信息。

## 三角洲日志

Delta Lake 之所以有效，是因为它创建了所谓的“Delta Log”，这是一组 JSON 文件，允许 Apache Spark 不仅读取数据湖中的数据，还读取关于哪些文件与数据的哪个版本相关的版本信息。增量日志位于名为 _delta_log 的文件夹中，由具有已知命名系统的 JSON 文件组成，并使用特定于每种类型的数据湖的锁来确保 JSON 文件被写入一次，从而允许多个写入者写入他们的数据，然后按顺序写入 JSON 文件，因此一个事务不能在实际失败或被部分覆盖时看起来像另一个事务成功了。

JSON 增量日志文件描述了数据的状态。在清单 [11-1](#PC1) 中，我们可以看到当文件夹被转换为 delta 格式时，第一个 JSON 文件被写入数据湖。清单显示了最初的操作，即“CONVERT ”,然后 12 个文件被添加到表中。增量日志还包括写入数据的模式和组成第一个版本的文件的路径。

```cs
{
    "commitInfo": {
        "timestamp": 1603400609668,
        "operation": "CONVERT",
        "operationParameters": {
            "numFiles": 12,
            "partitionedBy": "[]",
            "collectStats": false
        },
        "operationMetrics": {
            "numConvertedFiles": "12"
        }
    }
}
{
    "protocol": {
        "minReaderVersion": 1,
        "minWriterVersion": 2
    }
}
{
    "metaData": {
        "id": "8942a94c-506b-488c-8247-0da4e861a37a",
        "format": {
            "provider": "parquet",
            "options": {}
        },
        "schemaString": "{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}",
        "partitionColumns": [],
        "configuration": {},
        "createdTime": 1603400609648
    }
}
{
    "add": {

        "path": "part-00011-707f035c-4ddb-461f-9d52-bc1f41f1f08c-c000.snappy.parquet",
        "partitionValues": {},
        "size": 804,
        "modificationTime": 1603400607000,
        "dataChange": true
    }
}
{
    "add": {
        "path": "part-00000-707f035c-4ddb-461f-9d52-bc1f41f1f08c-c000.snappy.parquet",
        "partitionValues": {},
        "size": 796,
        "modificationTime": 1603400607000,
        "dataChange": true
    }
}

Listing 11-1The JSON delta log file

```

### 阅读日期

为了从这种 delta 格式中读取数据，Apache Spark 转到“_delta_log”文件夹，按名称顺序读取每个 JSON 文件，然后评估哪些数据文件应该包含在返回的 DataFrame 中。让一组 JSON 文件告诉 Apache Spark 要读取哪些文件，意味着 Apache Spark 不必枚举所有文件，这样会增加性能开销。

Apache Spark 写入数据的顺序至关重要。首先，编写实际的 parquet 文件，然后更新 JSON 文件。这意味着如果 Apache Spark 写数据文件，然后崩溃，delta 格式不会处于不一致的状态；增量日志中没有引用的拼花文件将被忽略。

### 更改数据

在本节中，我们将了解 Delta Lake 如何修改可供读取的数据。我们将首先概述 Delta Lake 的所有不同特性，然后以一个 Delta Lake 应用程序示例结束，该示例展示了所有的附加特性。我们将先在 C#中演示使用 Delta Lake，然后在 F#中演示。

#### 追加数据

最直接的操作是向现有的数据集添加更多的数据，Apache Spark 通过编写新的数据文件，然后向 JSON 添加更多的“add”指令来实现，我们在清单 [11-1](#PC1) 中看到了这些指令。在清单 [11-2](#PC2) 中，我们可以看到在运行一个“附加”之后，我们在哪里获得下一个 JSON 文件，以及要添加到数据帧中的下一组文件的详细信息。

```cs
{
    "commitInfo": {
        "timestamp": 1603740783553,
        "operation": "WRITE",
        "operationParameters": {
            "mode": "Append",
            "partitionBy": "[]"
        },
        "readVersion": 0,
        "isBlindAppend": true,
        "operationMetrics": {
            "numFiles": "12",
            "numOutputBytes": "5780",
            "numOutputRows": "50"
        }
    }
}
{
    "add": {
        "path": "part-00000-29d1078d-45f1-40f2-8058-1dc16eee7bf2-c000.snappy.parquet",
        "partitionValues": {},
        "size": 481,
        "modificationTime": 1603740783000,
        "dataChange": true
    }
}
{
    "add": {
        "path": "part-00001-1b7ad20f-a037-4ca4-a786-be6400e5b3b1-c000.snappy.parquet",
        "partitionValues": {},
        "size": 481,
        "modificationTime": 1603740783000,
        "dataChange": true
    }
}

Listing 11-2Appending more data causes additional “add” directives to be added via a JSON delta log file

```

如果我们现在让 Apache Spark 读取，那么它会做的是评估第一个 JSON 文件，找到所有“add”指令，然后评估第二个 JSON 文件，评估第二个文件中的“add”指令，并从所有底层的 parquet 文件创建一个数据帧。

#### 覆盖数据

当我们想要覆盖数据以便我们正在写入的数据成为完整的数据集时，Apache Spark 将该写入标记为覆盖，并且忽略所有以前的 parquet 文件。

在清单 [11-3](#PC3) 中，我们可以看到“Overwrite”操作使用“remove”指令从 JSON delta 日志中删除了 parquet 文件，并使用“add”指令添加了新文件。这意味着可以在任何时间点从底层的 parquet 文件中读取数据，这是不会改变的。

```cs
{
    "commitInfo": {
        "timestamp": 1603741047813,
        "operation": "WRITE",
        "operationParameters": {
            "mode": "Overwrite",
            "partitionBy": "[]"
        },
        "readVersion": 1,
        "isBlindAppend": false,
        "operationMetrics": {
            "numFiles": "12",
            "numOutputBytes": "5780",
            "numOutputRows": "50"
        }
    }
}
{
    "remove": {
        "path": "part-00005-6bd2f7c1-a364-4028-9846-3da01fd36f7f-c000.snappy.parquet",
        "deletionTimestamp": 1603741047812,
        "dataChange": true
    }
}
{
    "remove": {

        "path": "part-00001-437c2c31-ff49-489b-aff8-274f3b3de4b2-c000.snappy.parquet",
        "deletionTimestamp": 1603741047813,
        "dataChange": true
    }
}
{
    "add": {
        "path": "part-00000-061e39ea-a20c-4671-8abb-adae938d2115-c000.snappy.parquet",
        "partitionValues": {},
        "size": 481,
        "modificationTime": 1603741047000,
        "dataChange": true
    }
}
{
    "add": {
        "path": "part-00001-8428416f-4c93-4d5d-bbcb-830905e1221f-c000.snappy.parquet",
        "partitionValues": {},
        "size": 481,
        "modificationTime": 1603741047000,
        "dataChange": true
    }
}

Listing 11-3The “remove” directives are causing Apache Spark to ignore the underlying files

```

#### 更改数据

到目前为止，我们已经研究了如何追加额外的文件或覆盖整个增量表，这相对来说比较简单。如果我们想编辑增量表中的数据怎么办？德尔塔湖是怎么做到的？

如果我们要删除一行或更新一行，那么就复杂多了。Delta Lake 将读取表的当前状态，然后读入数据以识别要更改或删除的行。一旦 Delta Lake 知道哪些特定的行需要删除或更改，它将创建一个新的文件，其中包含应该保留的行以及任何更新的行。任何应该删除的行都不会写入新文件。当写入新文件时，新文件的“添加”指令和前一文件的“移除”指令被写入增量日志文件。

这意味着，如果您有一个由一个包含一百万行的 parquet 文件组成的 Delta 表，并且您更改了其中的一行，那么其他 999，999 行将与新修改的文件一起被重写。这是一种浪费，但这是唯一可用的选择，因为拼花地板不是可更新的格式。实际上，如果您不使用 Delta Lake 格式，您仍然会产生很高的成本，因为您必须覆盖这些文件，所以即使它不是最佳的，它也不比其他可能的解决方案差。

在清单 [11-4](#PC4) 中，我们看到了增量日志上的更新语句的结果。

```cs
{
    "commitInfo": {
        "timestamp": 1603742054064,
        "operation": "UPDATE",
        "operationParameters": {
            "predicate": "(id#529L > 500)"
        },
        "readVersion": 1,
        "isBlindAppend": false,
        "operationMetrics": {
            "numRemovedFiles": "6",
            "numAddedFiles": "6",
            "numUpdatedRows": "499",
            "numCopiedRows": "1"
        }
    }
}
{
    "remove": {
        "path": "part-00006-d4e299fc-0ae4-48d8-9252-b00b3b78584d-c000.snappy.parquet",
        "deletionTimestamp": 1603742053905,
        "dataChange": true
    }
}
{

    "remove": {
        "path": "part-00010-d4e299fc-0ae4-48d8-9252-b00b3b78584d-c000.snappy.parquet",
        "deletionTimestamp": 1603742053905,
        "dataChange": true
    }
}
{
    "add": {
        "path": "part-00004-426a4814-9962-4aa5-81da-e38480d86c5c-c000.snappy.parquet",
        "partitionValues": {},
        "size": 493,
        "modificationTime": 1603742054000,
        "dataChange": true
    }
}
{
    "add": {
        "path": "part-00005-b6e1b947-3109-4951-af4a-f01a894642ff-c000.snappy.parquet",
        "partitionValues": {},
        "size": 493,
        "modificationTime": 1603742054000,
        "dataChange": true
    }
}

Listing 11-4Result of the delta log after running an update statement

```

update 语句运行并删除任何包含匹配行的文件，并使用任何新数据写入一个新文件。有趣的是，delta 日志对人类来说可读性有多强。在本例中，它甚至记录了更新操作和用于查找要修改的行的过滤器。

#### 检查站

如果对以 delta 格式存储的数据集做了很多修改，您可能会发现枚举所有这些 JSON 文件会变得很慢。Delta Lake 的另一个特性是，它使用“检查点”文件以一种读取速度更快的格式存储一个特定版本的状态。当 Delta Lake 认为合适时，它将创建一个具有当前状态的 parquet 文件，并在“_last_checkpoint”文件中记录检查点的版本。如果您将检查点文件作为一个 parquet 文件读取并显示数据，那么您将看到如下所示的内容:

```cs
+----+--------------------+------+--------------------+--------+----------+
| txn|                 add|remove|            metaData|protocol|commitInfo|
+----+--------------------+------+--------------------+--------+----------+
|null|[part-00007-d4e29...|  null|                null|    null|      null|
|null|[part-00010-d4e29...|  null|                null|    null|      null|
|null|[part-00009-d4e29...|  null|                null|    null|      null|
|null|[part-00002-d4e29...|  null|                null|    null|      null|
|null|[part-00004-d4e29...|  null|                null|    null|      null|
|null|[part-00006-d4e29...|  null|                null|    null|      null|
|null|[part-00008-d4e29...|  null|                null|    null|      null|
|null|[part-00003-d4e29...|  null|                null|    null|      null|
|null|[part-00000-d4e29...|  null|                null|    null|      null|
|null|[part-00011-d4e29...|  null|                null|    null|      null|
|null|[part-00001-d4e29...|  null|                null|    null|      null|
|null|                null|  null|                null|  [1, 2]|      null|
|null|                null|  null|[3434f91f-fb53-4e...|    null|      null|
|null|[part-00005-d4e29...|  null|                null|    null|      null|
+----+--------------------+------+--------------------+--------+----------+

```

#### 历史

由于数据被写入各个 parquet 文件，然后增量日志被用来记录数据在任何时间点的状态，我们还可以查看增量表的历史记录，并选择数据，就像它是特定版本或特定时间一样。

要查看可用的历史记录，我们可以手动检查 JSON 增量日志，该日志由位于 [`https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html#:~:text=The%20Delta%20Lake%20Transaction%20Log%20at%20the%20File%20Level&text=Each%20commit%20is%20written%20out`](https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html%2523:%257E:text%253DThe%2520Delta%2520Lake%2520Transaction%2520Log%2520at%2520the%2520File%2520Level%2526text%253DEach%2520commit%2520is%2520written%2520out,json%2520%252C%2520the%2520following%2520as%2520000002) 的数据块、JSON % 20% 2C % 20 以下%20as%20000002 或位于[*https://github.com/delta-io/delta/blob/master/PROTOCOL.md*](https://github.com/delta-io/delta/blob/master/PROTOCOL.md)*的协议本身记录。*

Delta Lake 还提供了一个 API，我们可以使用对象或 SQL 请求来调用它。当我们使用 API 时，我们得到了一个数据帧，其中包含了增量表的历史细节。

如果我们查看数据帧的内容，我们会看到

*   版本

*   时间戳

*   创建版本的用户

*   写、删除、更新等操作

*   工作或笔记本详细信息

*   集群 ID

*   操作的详细信息，例如使用的过滤器以及添加和删除的文件数量

#### 真空

因为每次进行更改时，都会添加更多的文件，所以对于经常更新的表来说，数据的大小可能会变得过于昂贵，无法永久存储。为了迎合这一点，Delta Lake 提供了一种方法来设置保留历史长度，然后运行 Vacuum，这将删除任何不再需要提供历史的文件。

如果您指定一段时间，例如七天，这是默认值，那么创建过去七天的历史所需的任何文件都会保留。这可能意味着您有一个七天前的文件，它仍然用于提供增量表的当前版本。如果您认为只有删除或更新文件中的数据时文件才过时，如果文件中的数据没有更改，则文件仍然有效，即使您指定要清空超过七天的数据。

我们可以随时发出 Vacuum 命令，但是我们应该注意，如果我们不运行 Vacuum 命令，文件将无限期地停留在那里。

真空清理数据文件。任何日志文件都会保留到检查点之后，这是在每十次提交之后自动发生的。

#### 合并

有了 Delta Lake 格式和修改现有文件的能力，Apache Spark 团队还为 Delta Lake 引入了 merge，这意味着我们可以有一个可以

*   更新行

*   删除行

*   插入新行

Merge 是一个令人兴奋的特性，因为我们可以让 Apache Spark 一次运行多个操作，而不是手动运行更新、插入和删除。

#### 图式进化

Delta Lake 格式包括底层数据的模式，这意味着如果我们试图用额外的列追加一个新的数据文件，追加将会失败。要解决这个问题，您可以包含“mergeSchema”选项，该选项会用新列自动更新模式。对于任何旧行，新列将为空。但是，对于任何新追加的数据，该列都将有数据。

#### 时间旅行

表的历史记录允许我们查看存在哪个版本，然后，通过 DataFrame API，我们可以指定选项来控制版本，要么是“timestampAsOf ”,就像是特定时间一样提取数据，要么是“versionAsOf ”,就像是特定版本一样提取数据。

这种快速回到过去的能力对于故障排除非常方便。通过能够回滚到特定的日期和时间，然后重新运行以前中断的数据管道，它已经为我个人节省了几次重新加载大量数据的时间。

## DeltaLake 应用示例

希望到现在为止，你已经对 DeltaLake 有了很好的理解。因此，在这一节中，我们将研究如何将 Delta Lake 代码添加到 Apache Spark 实例中，并创建我们的。NET 用于使用 Delta Lake 格式的 Apache Spark 应用程序。

### 配置

DeltaLake 不是核心阿帕奇星火项目的一部分。核心团队已经创建了它，但是它被视为第三方组件。因为它没有附带 Apache Spark，所以我们需要做的事情很少。

首先，我们需要确保 Apache Spark 实例加载了 Delta Lake 的 JAR 文件。为此，我修改了＄SPARK _ HOME/conf/SPARK-defaults . conf 文件，并添加了以下代码行:

```cs
spark.jars.packages io.delta:delta-core_2.12:0.7.0

```

这将导致 JAR 文件被下载并包含在随后启动的每个 Apache Spark 实例中。

现在 Apache Spark 有了 Delta Lake 格式，我们需要包含。NET 对象，因为它们也不是微软。Spark NuGet 包。DeltaLake。NET 对象在微软。所以您需要将它添加到您的项目中。

最后，为了使用 Delta Lake SQL 扩展，我们需要告诉 Apache Spark 使用配置选项“spark.sql.extensions”启用扩展的 SQL 命令，该选项设置为“io . Delta . SQL . deltasparksessionextension”。

### c sharp . c sharp . c sharp . c sharp

这一节将通过一个例子来说明如何使用 Delta Lake 扩展。NET for Apache Spark。首先，我们将通过 C#的例子，然后是 F#的例子。

在清单 [11-5](#PC7) 中，我们创建了`SparkSession`，但是使用配置选项指定我们想要使用`DeltaSparkSessionExtensions`。

```cs
var spark = SparkSession.Builder()
    .Config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .GetOrCreate();

Listing 11-5Create the SparkSession passing in the details of the Delta extension we wish to load

```

然后，作为一次性练习，我们创建 DeltaLake 表。对于 Delta Lake，有一个名为 DeltaTable 的静态类，它为我们提供了一些有用的方法来获取对 Delta 表的引用，并将 parquet 文件转换为 Delta Lake 格式。在清单 [11-6](#PC8) 中，我们使用 DeltaTable。IsdeltaTable 来查看 Delta 表是否存在，或者我们是否必须编写一个 parquet 文件，然后将其转换为 Delta Lake 格式。

```cs
if (!DeltaTable.IsDeltaTable("parquet.`/tmp/delta-demo`"))
{
    spark.Range(1000).WithColumn("name", Lit("Sammy")).Write().Mode("overwrite").Parquet("/tmp/delta-demo");
    DeltaTable.ConvertToDelta(spark, "parquet.`/tmp/delta-demo`");
}

Listing 11-6Converting a parquet file to Delta Lake

```

注意，在清单 [11-6](#PC8) 中，我们显式地将一个 parquet 文件转换成 Delta Lake 格式，但是我们也可以使用“Delta”格式编写一个 DataFrame。

在清单 [11-7](#PC9) 中，我们使用`Delta.ForPath`获取对增量表的引用，然后通过使用`ToDF()`将对增量表的引用转换成`DataFrame`。

```cs
var delta = DeltaTable.ForPath("/tmp/delta-demo");
delta.ToDF().OrderBy(Desc("Id")).Show();

Listing 11-7Using DeltaTable to get a reference to the DataFrame

```

向增量表追加数据就像使用其他格式一样简单。我们指定模式为“追加”；我们在清单 [11-8](#PC10) 中展示了这一点。

```cs
spark.Range(5, 500 ).WithColumn("name", Lit("Lucy")).Write().Mode("append").Format("delta").Save("/tmp/delta-demo");

Listing 11-8Appending data to a Delta table

```

如果我们想要更新增量表中的数据，我们需要使用对`DeltaTable`的引用，而不是从`DeltaTable.ToDF`返回的`DataFrame`。在清单 [11-9](#PC11) 中，我展示了如何使用`DeltaTable`引用来更新增量表。在本例中，我们找到 id 大于 500 的任何行，然后将 id 列设置为值 999。

```cs
delta.Update(Expr("id > 500"), new Dictionary<string, Column>()
{
    {"id", Lit(999)}
});

Listing 11-9Updating a Delta table

```

从增量表中删除是通过调用`DeltaTable.Delete`然后传递一个过滤器来完成的。如果您没有提供过滤器，那么每一行都会被删除。清单 [11-10](#PC12) 显示了`Delete`的操作。

```cs
delta.Delete(Column("id").EqualTo(999));

Listing 11-10Deleting from a Delta table

```

为了从表中查看历史，我们使用 DeltaTable。History()方法，该方法返回一个数据帧，因此我们可以调用 Show 或做任何您需要做的过滤。这很有用，因为您可以过滤数据帧以找到一个特定的更新，然后使用版本和/或时间详细信息在特定更新时从表中读取。清单 [11-11](#PC13) 展示了如何请求一个增量表的历史。

```cs
delta.History().Show(1000, 10000);

Listing 11-11Requesting the history from a Delta table

```

现在你有了可用的历史；您可以使用“timestampAsOf”和“version of”选项来指定您想要的增量表的确切版本，而不是最新版本。在清单 [11-12](#PC14) 中，我们展示了如何读取增量表，就好像它是一个特定的版本或时间。

```cs
spark.Read().Format("delta").Option("versionAsOf", 0).Load("/tmp/delta-demo").OrderBy(Desc("Id")).Show();

spark.Read().Format("delta").Option("timestampAsOf", "2021-10-22 22:03:36").Load("/tmp/delta-demo").OrderBy(Desc("Id")).Show();

Listing 11-12Reading from the Delta table using time travel

```

我们要看的下一个操作是合并操作。当我们在中合并数据时，我们使用增量表作为目标，使用数据帧作为源。为了方便起见，通常最好将两个表都用别名。在清单 [11-13](#PC15) 中，我展示了一个示例合并操作，我将 Delta 表别名为“target ”,将 DataFrame 别名为“source ”,因此当提供过滤器来显示匹配哪些行时，我们可以以简单的字符串格式“target.id = source.id”提供过滤器。当我们提供了过滤器后，我们可以选择在行匹配时提供两个动作，在行不匹配时提供一个动作。当一行匹配时，我们还可以选择提供第二个过滤器，更新匹配的行或删除它们。

当我们不匹配任何行时，我们可以插入所有列或者提供一个列列表。在这种情况下，因为我在增量表和数据帧中有相同的模式，所以 InsertAll 工作正常。

```cs
var newData = spark.Range(10).WithColumn("name", Lit("Ed"));

delta.Alias("target")
        .Merge(newData.Alias("source"), "target.id = source.id")
        .WhenMatched(newData["id"].Mod(2).EqualTo(0)).Update(new Dictionary<string, Column>()
                                                            {
                                                                {"name", newData["name"]}
                                                            })
        .WhenMatched(newData["id"].Mod(2).EqualTo(1)).Delete()
        .WhenNotMatched().InsertAll()
    .Execute();

Listing 11-13The Delta Lake merge operation

```

清单 [11-13](#PC15) 中的示例背后的逻辑是

*   在增量表中找到与数据帧中的 id 相匹配的任何一行。

*   如果有任何行匹配并且它们是偶数，`Mod(2).EqualTo(0)`，更新该行并将名称列设置为源数据帧中相关 id 的名称列的值。

*   如果任何行匹配并且是奇数，`Mod(2).EqualTo(1)`，删除该行。

*   如果源数据帧中的任何 id 在目标增量表中尚不存在，则将源数据帧中的所有列插入到目标增量表中。

最后，在清单 [11-14](#PC16) 中，我们展示了要演示的最后一个操作是`Vacuum`方法，如果可以的话，它会整理任何不需要支持当前版本和保留期内任何版本的旧数据文件。

```cs
delta.Vacuum(1F)

Listing 11-14Delta table Vaccum

```

### FSharp

这一节将通过一个例子来说明如何使用 Delta Lake 扩展。NET for Apache Spark 使用 F#。

在清单 [11-15](#PC17) 中，我们创建了`SparkSession`，但是使用配置选项指定我们想要使用`DeltaSparkSessionExtensions`。

```cs
let spark = SparkSession.Builder()
            |> fun builder -> builder.Config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
            |> fun builder -> builder.GetOrCreate()

Listing 11-15Create the SparkSession passing in the details of the Delta extension we wish to load

```

然后，作为一次性练习，我们创建 DeltaLake 表。对于 Delta Lake，有一个名为 DeltaTable 的静态类，它为我们提供了一些有用的方法来获取对 Delta 表的引用，并将 parquet 文件转换为 Delta Lake 格式。在清单 [11-16](#PC18) 中，我们使用 DeltaTable。IsdeltaTable 来查看 Delta 表是否存在，或者我们是否必须编写一个 parquet 文件，然后将其转换为 Delta Lake 格式。

```cs
let delta = match DeltaTable.IsDeltaTable("parquet.`/tmp/delta-demo`") with
                | false -> spark.Range(1000L)
                                |> fun dataframe -> dataframe.WithColumn("name", Functions.Lit("Sammy"))
                                |> fun dataframe -> dataframe.Write()
                                |> fun writer -> writer.Mode("overwrite").Parquet("/tmp/delta-demo")
                           DeltaTable.ConvertToDelta(spark, "parquet.`/tmp/delta-demo`")
                | _ -> DeltaTable.ForPath("/tmp/delta-demo")

Listing 11-16Converting a parquet file to Delta Lake

```

注意，在清单 [11-16](#PC18) 中，我们显式地将一个拼花文件转换成 Delta Lake 格式，但是我们也可以使用“Delta”格式编写一个数据帧。如果我们不需要创建增量表，我们使用`Delta.ForPath`来获取对增量表的引用，然后通过使用`ToDF()`将对增量表的引用转换成`DataFrame`。

向增量表追加数据就像使用其他格式一样简单。我们指定模式为“追加”；我们在清单 [11-17](#PC19) 中展示了这一点。

```cs
spark.Range(5L, 500L)
    |> fun dataframe -> dataframe.WithColumn("name", Functions.Lit("Lucy"))
    |> fun dataframe -> dataframe.Write()
    |> fun writer -> writer.Mode("append").Format("delta").Save("/tmp/delta-demo")

Listing 11-17Appending data to a Delta table

```

如果我们想要更新增量表中的数据，我们需要使用对`DeltaTable`的引用，而不是从`DeltaTable.ToDF`返回的`DataFrame`。在清单 [11-18](#PC20) 中，我展示了如何使用`DeltaTable`引用来更新增量表。在本例中，我们找到 id 大于 500 的任何行，然后将 id 列设置为值 999。

```cs
delta.Update(Functions.Expr("id > 500"), Dictionary<string, Column>(dict [("id", Functions.Lit(999))]))

Listing 11-18Updating a Delta table

```

从增量表中删除是通过调用`DeltaTable.Delete`然后传递一个过滤器来完成的。如果您没有提供过滤器，那么每一行都会被删除。清单 [11-19](#PC21) 显示了`Delete`的操作。

```cs
delta.Delete(Functions.Col("id").EqualTo(999))

Listing 11-19Deleting from a Delta table

```

为了从表中查看历史，我们使用 DeltaTable。History()方法，该方法返回一个数据帧，因此我们可以调用 Show 或做任何您需要做的过滤。这很有用，因为您可以过滤数据帧以找到一个特定的更新，然后使用版本和/或时间详细信息在特定更新时从表中读取。清单 [11-20](#PC22) 展示了如何请求一个增量表的历史。

```cs
delta.History()
        |> fun dataframe -> dataframe.Show()

Listing 11-20Requesting the history from a Delta table

```

现在你有了可用的历史；您可以使用“timestampAsOf”和“version of”选项来指定您想要的增量表的确切版本，而不是最新版本。在清单 [11-21](#PC23) 中，我们展示了如何读取增量表，就好像它是一个特定的版本或时间。

```cs
spark.Read()
    |> fun reader -> reader.Format("delta")
    |> fun reader -> reader.Option("versionAsOf", 0L)
    |> fun reader -> reader.Load("/tmp/delta-demo")
    |> fun dataframe -> dataframe.OrderBy(Functions.Desc("id"))
    |> fun ordered -> ordered.Show()

spark.Read()
    |> fun reader -> reader.Format("delta")
    |> fun reader -> reader.Option("timestampAsOf", "2022-01-01")
    |> fun reader -> reader.Load("/tmp/delta-demo")
    |> fun dataframe -> dataframe.OrderBy(Functions.Desc("id"))
    |> fun ordered -> ordered.Show()

Listing 11-21Reading from the Delta table using time travel

```

我们要看的下一个操作是合并操作。当我们在中合并数据时，我们使用增量表作为目标，使用数据帧作为源。为了方便起见，通常最好将两个表都用别名。在清单 [11-22](#PC24) 中，我展示了一个示例合并操作，我将 Delta 表别名为“target ”,将 DataFrame 别名为“source ”,因此当提供过滤器来显示匹配哪些行时，我们可以以简单的字符串格式“target.id = source.id”提供过滤器。当我们提供了过滤器后，我们可以选择在行匹配时提供两个动作，在行不匹配时提供一个动作。当一行匹配时，我们还可以选择提供第二个过滤器，更新匹配的行或删除它们。

当我们不匹配任何行时，我们可以插入所有列或者提供一个列列表。在这种情况下，因为我在增量表和数据帧中有相同的模式，所以 InsertAll 工作正常。

```cs
let newData = spark.Range(10L)
                 |> fun dataframe -> dataframe.WithColumn("name", Functions.Lit("Ed"))
                 |> fun newData -> newData.Alias("source")

delta.Alias("target")
  |> fun target -> target.Merge(newData, "source.id = target.id")
  |> fun merge -> merge.WhenMatched(newData.["id"].Mod(2).EqualTo(0))
  |> fun evens -> evens.Update(Dictionary<string, Column>(dict [("name", newData.["name"])]))
  |> fun merge -> merge.WhenMatched(newData.["id"].Mod(2).EqualTo(0))
  |> fun odds -> odds.Delete()
  |> fun merge -> merge.WhenNotMatched()
  |> fun inserts -> inserts.InsertAll()
  |> fun merge -> merge.Execute()

Listing 11-22The Delta Lake merge operation

```

清单 [11-22](#PC24) 中的示例背后的逻辑是

*   在增量表中找到与数据帧中的 id 相匹配的任何一行。

*   如果有任何行匹配并且它们是偶数，`Mod(2).EqualTo(0)`，更新该行并将名称列设置为源数据帧中相关 id 的名称列的值。

*   如果任何行匹配并且是奇数，`Mod(2).EqualTo(1)`，删除该行。

*   如果源数据帧中的任何 id 在目标增量表中尚不存在，则将源数据帧中的所有列插入到目标增量表中。

最后，在清单 [11-23](#PC25) 中，我们展示了要演示的最后一个操作是`Vacuum`方法，如果可以的话，它会整理任何不需要支持当前版本和保留期内任何版本的旧数据文件。

```cs
delta.Vacuum(1F)

Listing 11-23Delta table Vaccum

```

## 摘要

在本章中，我们已经了解了 Delta Lake，以及它如何帮助我们在数据湖中创建数据应用程序。我在生产中使用过 Delta Lake，它的好处非常明显，比如能够将表回滚到特定的时间点，以及更新、删除和合并现有 Delta 表中的数据。