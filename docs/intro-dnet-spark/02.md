# 二、配置 Spark

为了开发 Apache Spark 应用程序的. NET，我们需要在开发机器上安装 Apache Spark，然后进行配置。NET for Apache Spark，以便我们的应用程序正确执行。当我们在生产中运行 Apache Spark 应用程序时，我们将使用一个集群，或者类似于 YARN 集群，或者使用一个完全托管的环境，比如 Databricks。当我们开发应用程序时，我们在本地使用相同版本的 Apache Spark，就像我们在许多机器的集群上运行时一样。在我们的开发机器上拥有相同的版本意味着当我们开发和测试代码时，我们可以确信在生产中运行的代码是相同的。

在这一章中，我们将介绍正确运行所需的各种组件；Apache Spark 是一个 Java 应用程序，所以我们需要安装和配置正确的 Java 版本，然后下载和配置 Apache Spark。只有当我们有正确版本的 Java 和 Apache Spark 运行时，我们才能够用 C#或 F#编写一个在 Apache Spark 上执行的. NET 应用程序。

## 选择您的软件版本

在本节中，我们将首先帮助您选择应该使用哪个版本的 Apache Spark 和哪个版本的 Java。尽管这看起来应该是一个简单的选择，但是有一些特定的要求，并且正确地做到这一点对于顺利开始是至关重要的。

### 选择 Apache Spark 的版本

在这一节中，我们将看看如何选择 Apache Spark 的版本。Apache Spark 是一个积极开发的开源项目，新版本经常出现，有时甚至一个月出现多次。然而。NET for Apache Spark project 并不支持每个版本，要么是因为它不支持它，要么是因为开发团队还没有添加。

当我们运行 Apache Spark 应用程序的. NET 时，我们需要理解我们需要。NET 代码，它运行在特定版本的。NET 框架或者。NET 核心。那个。NET for Apache Spark 代码与有限的 Apache Spark 版本兼容，根据您拥有的 Apache Spark 版本，您可能需要 Java 8 或 Java 11。

要帮助选择您需要的组件版本，请访问的主页。NET 为 Apache Spark 项目， [`https://github.com/dotnet/spark`](https://github.com/dotnet/spark) ，还有一个板块“支持的 Apache Spark”；水流。NET for Apache Spark 版本`"v1.0.0"`支持这些版本的 Apache Spark:

*   2.3.*

*   2.4.0

*   2.4.1

*   2.4.3

*   2.4.4

*   2.4.5

*   3.0.0

注意不支持 2.4.2，Apache Spark 的 3.0.0 在。NET for Apache Spark v1.0.0 于 2020 年 10 月发布。在可能的情况下，你应该尽可能把这两个项目的最高版本作为目标，今天，也就是 2020 年 11 月，我将开始一个新的项目。NET for Apache Spark 1 . 0 . 0 版和 Apache Spark 3.0 版。不幸的是，我们在这里写的任何具体建议都会很快过时。在撰写本文和回顾本章之间，建议从使用。NET for Apache Spark 版本 v0.12.1 和 v1.0.0。

一旦您选择了要使用的 Apache Spark 代码版本，请访问该版本的发行说明，如 [`https://spark.apache.org/docs/3.0.0/`](https://spark.apache.org/docs/2.4.6/) 或 [`https://spark.apache.org/docs/3.0.0/`](https://spark.apache.org/docs/3.0.0/) 。发行说明包括支持哪个版本的 Java VM 的详细信息。如果您尝试在一个不受支持的 JVM 版本上运行，那么您的应用程序将会失败，所以在这里您确实需要小心。

当您下载 Apache Spark 时，您有几个选项。您可以下载源代码并自己编译，我们不在这里讨论，但是您可以从 [`https://spark.apache.org/docs/latest/building-spark.html`](https://spark.apache.org/docs/latest/building-spark.html) 获得如何从源代码编译的说明。您也可以选择使用预构建的 Hadoop 或不使用 Hadoop 进行下载，但是您需要提供自己的 Hadoop 安装。通常，对于开发机器，我会下载带有预构建 Hadoop 的 Apache Spark 版本，因此您不必维护 Hadoop 的实例。但是在某些情况下，您可能希望使用不带 Hadoop 的版本，并单独安装 Hadoop。一个例子是，如果你从你的开发实例中，想要读写文件到 Azure 数据湖存储，那么你将需要一个单独的 Hadoop 实现作为预构建的实现。在撰写本文时，预构建的 Hadoop 实现不支持 Azure 数据湖协议。在实践中，您可能会发现，当您在集群上运行时，您针对本地文件进行开发，并且只需要读写像 Azure Data Lake 存储这样的东西。

总结一下:

*   **为 Apache Spark 版本**选择. NET 理想情况下，获得最新版本。

*   **选择 Apache Spark 版本**——支持的最新版本。NET for Apache Spark。

*   **选择 Java VM 版本**–选择您选择的 Apache Spark 支持的最新版本。

*   **选择带或不带 Hadoop 的下载**–除非您有使用独立 Hadoop 的特定要求，否则请使用预构建 Hadoop 的 Apache Spark 版本。

在下一节中，我们将探讨如何选择 Java 的版本，这并不总是像第一次出现时那样简单。

### 选择 Java 版本

Sun Microsystems 最初开发了 Java，任何人都可以免费使用。2019 年 4 月，甲骨文改变了 Java 的许可方式，因此从 2019 年 4 月起，甲骨文 8 版以上的 Java 版本不再免费用于生产或非个人使用。更令人困惑的是，Oracle 还发布了一个名为 OpenJDK 的 Java 版本，它没有这些限制；许多人选择使用 OpenJDK 版本的 Java，而不是 Oracle 版本的 Java。要了解有关这些许可变更的更多信息，请参见 [`www.oracle.com/java/technologies/javase/jdk-faqs.html`](http://www.oracle.com/java/technologies/javase/jdk-faqs.html) 。

谈到 Java 版本，两种不同的方案指向同一个逻辑版本；Java 1.8 和 Java 8，虽然看起来很不一样，但都是同一个版本。

与 Oracle OpenJDK 相比，您从 Oracle JDK 获得的是支持，这是许多组织的基本要求。这篇堆栈溢出帖子的前两个答案描述了这个问题，并帮助指导您选择使用哪种风格的 Java:[`https://stackoverflow.com/questions/52431764/difference-between-openjdk-and-adoptium-adoptopenjdk`](https://stackoverflow.com/questions/52431764/difference-between-openjdk-and-adoptium-adoptopenjdk)。

## 配置 Apache Spark 和。macOS 上 Apache Spark 的. NET

在这一节中，我们将介绍如何获得 Apache Spark 运行开发机器的本地实例；一旦我们有了支持的版本的工作安装。NET，我们可以创建我们的第一个。NET 应用程序在下一章。在后面的小节中，我们将看看如何在 Windows 和 Linux 上配置 Apache Spark。

### 配置已安装的 Java

在安装 Java 之前，有必要检查您是否已经安装并配置了正确的 Java 版本，或者您已经安装了正确的版本，但是配置了单独的版本。要查看 macOS 上有哪些版本的 Java，运行`/usr/libexec/java_home -V`。在这种情况下，输出显示了 Java 8 和 13 JDKs:

```cs
» /usr/libexec/java_home -V
Matching Java Virtual Machines (2):
    13.0.1, x86_64:   "OpenJDK 13.0.1"    /Library/Java/JavaVirtualMachines/openjdk-13.0.1.jdk/Contents/Home
    1.8.0_232, x86_64:    "AdoptOpenJDK 8"    /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home

```

在我的例子中，我需要 Java 8，这样我就可以运行 Apache Spark“3 . 0 . 0”，所以我检查两个 Java 实例的哪个版本被设置为默认版本:

```cs
» java -version
openjdk version "1.8.0_232"
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_232-b09)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.232-b09, mixed mode)

```

如果选择了不正确的版本，那么我需要使用工具`"/usr/libexec/java_home"`更新我的 JAVA_HOME 环境变量，并向该工具传递我想要使用的 JAVA 版本:

```cs
» export JAVA_HOME=$(/usr/libexec/java_home -v 13)
» java -version
openjdk version "13.0.1" 2019-10-15
OpenJDK Runtime Environment (build 13.0.1+9)
OpenJDK 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)

```

或者

```cs
» export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)
» java -version
openjdk version "1.8.0_232"
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_232-b09)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.232-b09, mixed mode)

```

### 安装 Java

如果您没有 Apache Spark 可以使用的 Java 版本，我们将需要下载一个 JDK 版本。在本节中，我们将下载并安装 AdoptOpenJDK 8 JDK，因此我们转到发布页面( [`https://adoptopenjdk.net/releases.html`](https://adoptopenjdk.net/releases.html) )并选择

*   open JDK 8(lt)

*   热点

*   苹果

*   JDK。包装

这些选项下载一个 pkg 文件，然后您可以使用 GUI 或命令行`installer -pkg jdk.pkg`安装该文件。如果您下载了这个包，这个包将安装并配置 Java。如果您启动一个新的终端并运行`java -version`，检查现在显示的是正确的版本；如果它没有显示正确的版本，请按照上一节中的步骤操作。

### 下载和配置 Apache Spark

现在您已经有了正确的 Java 工作版本，可以下载 Apache Spark 了。进入首页( [`https://spark.apache.org/downloads.html`](https://spark.apache.org/downloads.html) ，选择你想要的版本和包类型，然后下载；我下载并复制我的到我的主目录，然后运行

```cs
» tar -xvf spark-3.0.0-bin-hadoop2.7.tgz

```

该命令将文件提取到~/ spark-3.0.0-bin-hadoop2.7/，因此下一步是设置几个环境变量:

*   飞奔回家

*   小路

我们将`SPARK_HOME`设置为我们刚刚提取的目录，我将更新我的。zshrc 的新文件夹，并更新我的变量。zshrc 要包括“`$SPARK_HOME/bin`”；确保在更新路径之前设置了`$SPARK_HOME`。

### 试验

要验证 Apache Spark 的安装是否正常，运行`spark-shell`，这是一个运行命令的 REPL。如果您运行`spark-shell`，您应该看到显示的 spark 标志和一个命令提示符，您可以在这里运行 spark 命令。

```cs
» spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1595401509136).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_262)
Type in expressions to have them evaluated.
Type :help for more information.

scala>

```

如果您得到`scala>`提示符，那么这是一切正常的好迹象，但是让我们看看是否可以运行 spark 命令来做一些本地处理:

```cs
scala> spark.sql("select * from range(10)").withColumn("ID2", col("ID")).show
+---+---+
| id|ID2|
+---+---+
|  0|  0|
|  1|  1|
|  2|  2|
|  3|  3|
|  4|  4|
|  5|  5|
|  6|  6|
|  7|  7|
|  8|  8|
|  9|  9|
+---+---+

```

我们在这里做的是通过运行一些 spark SQL 调用`range`函数来创建一个`DataFrame`。`range`函数根据我们的要求创建尽可能多的行，然后我们使用 withColumn 创建第二列，其值与第一列中的值相同。最后，我们使用`show`来显示`DataFrame`的内容。

退出 REPL 就像退出维姆；使用`:q`。

### 覆盖默认配置

要配置 Apache Spark，我们可以使用$ Spark _ HOME/conf；中的配置文件。这些是一组控制 Apache Spark 如何工作的文本文件。当您第一次下载 Apache Spark 时，只有模板配置文件，没有实际的配置文件:

```cs
~/spark-3.0.0-bin-hadoop2.7/conf » ls
docker.properties.template   metrics.properties.template  spark-env.sh.template
fairscheduler.xml.template   slaves.template
log4j.properties.template    spark-defaults.conf.template

```

如果我们想要配置任何配置文件，我们应该首先通过删除将模板文件复制到实际文件中。模板"从文件名的末尾开始:

```cs
» cp ./spark-defaults.conf.template ./spark-defaults.conf
» cp ./log4j.properties.template ./log4j.properties

```

然后，您可以使用您最喜欢的编辑器打开配置文件并编辑它们。对于 Apache Spark 的开发实例，我会将控制台日志记录的 log4j 部分改为只显示错误；显示警告会产生很多我们通常可以忽略的输出。如果您更改默认文件内容，如下所示:

```cs
# Set everything to be logged to the console
log4j.rootCategory=INFO, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

```

到

```cs
# Set only errors to be logged to the console
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

```

这通常是一个好的开始。在 spark-defaults.conf 文件中，配置 Apache Spark 可以在您的开发机器上使用多少内存通常是一个好主意；例如，我的机器有 16GB 的 RAM，所以我将 Apache Spark 配置为使用 6GB 的 executor 内存。我还广泛使用了 Databricks 中的 Delta Lake 库，因此通过将它添加到我的默认配置中，我每次都可以访问它，并且不需要记住使用附加库启动 Apache Spark:

```cs
spark.executor.memory=6g
spark.jars.packages=io.delta:delta-core_2.12:0.7.0

```

一旦我更改了配置文件，重新运行 spark-shell 来验证更改是否有效总是值得的。这一次，当我运行 spark-shell 时，我可以看到日志记录变得不那么详细了，并且我的 delta 包已经被加载了:

```cs
» ./spark-shell
Ivy Default Cache set to: /Users/ed/.ivy2/cache
The jars for the packages stored in: /Users/ed/.ivy2/jars
:: loading settings :: url = jar:file:/Users/ed/spark-3.0.0-bin-without-hadoop/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
io.delta#delta-core_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-4987d518-30f9-4696-ac0e-1b20ed99f224;1.0
      confs: [default]
      found io.delta#delta-core_2.12;0.7.0 in central
      found org.antlr#antlr4;4.7 in central
      found org.antlr#antlr4-runtime;4.7 in local-m2-cache
      found org.antlr#antlr-runtime;3.5.2 in central
      found org.antlr#ST4;4.0.8 in central
      found org.abego.treelayout#org.abego.treelayout.core;1.0.3 in spark-list

      found org.glassfish#javax.json;1.0.4 in central
      found com.ibm.icu#icu4j;58.2 in central
:: resolution report :: resolve 210ms :: artifacts dl 8ms
      :: modules in use:
      com.ibm.icu#icu4j;58.2 from central in [default]
      io.delta#delta-core_2.12;0.7.0 from central in [default]
      org.abego.treelayout#org.abego.treelayout.core;1.0.3 from spark-list in [default]
      org.antlr#ST4;4.0.8 from central in [default]
      org.antlr#antlr-runtime;3.5.2 from central in [default]
      org.antlr#antlr4;4.7 from central in [default]
      org.antlr#antlr4-runtime;4.7 from local-m2-cache in [default]
      org.glassfish#javax.json;1.0.4 from central in [default]
      ---------------------------------------------------------------------
      |                  |            modules            ||   artifacts   |
      |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
      ---------------------------------------------------------------------
      |      default     |   8   |   0   |   0   |   0   ||   8   |   0   |
      ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-4987d518-30f9-4696-ac0e-1b20ed99f224
      confs: [default]
      0 artifacts copied, 8 already retrieved (0kB/7ms)

```

如果您能够运行 Apache Spark REPL，并且能够运行 Spark 命令，那么您很可能能够运行您的。当我们在下一章创建我们的第一个应用程序时。

## 配置 Apache Spark 和。NET for Apache Spark on Windows

在这一节中，我们将介绍如何获得 Apache Spark 运行开发机器的本地实例；一旦我们有了支持的版本的工作安装。NET，我们可以创建我们的第一个。NET 应用程序在下一章。

### 配置已安装的 Java

在安装 Java 之前，有必要检查您是否已经安装并配置了正确的 Java 版本，或者您已经安装了正确的版本，但是配置了单独的版本。要想知道你的 Windows 机器上有哪些版本的 Java，看看你的`program files`目录中有哪些`java.exe`文件。在这种情况下，输出显示了 Java 8 和 11 JDKs:

```cs
C:\>cd %ProgramFiles%

C:\Program Files>attrib java.exe /s
A                    C:\Program Files\AdoptOpenJDK\jdk-11.0.8.10-hotspot\bin\java.exe
A                    C:\Program Files\AdoptOpenJDK\jdk-8.0.262.10-hotspot\bin\java.exe
A                    C:\Program Files\AdoptOpenJDK\jdk-8.0.262.10-hotspot\jre\bin\java.exe

```

您可以选择是为整个系统设置正确的 Java 版本，还是在每次需要运行应用程序时设置 Java 版本。关于这个选择没有硬性的规则，但是我通常在运行时设置 Java 的版本，除非我 100%使用单一版本的 Java VM。

要在运行时设置 Java 的版本，需要设置两个环境变量:JAVA_HOME 和 PATH。JAVA_HOME 指向 JAVA 版本的根文件夹，PATH 包含 java.exe 所在文件夹的路径:

```cs
C:\Program Files>SET JAVA_HOME=C:\Program Files\AdoptOpenJDK\jdk-8.0.262.10-hotspot

C:\Program Files>SET PATH=%JAVA_HOME%\bin;%PATH%

```

然后，您必须使用命令提示符来启动使用 Java VM 的应用程序，您可以在命令提示符中设置环境变量。

如果你更喜欢为你的整个系统设置版本，如果你总是使用相同的版本会更简单，你应该进入 Windows 设置并搜索“编辑系统环境变量”,使用控制面板小程序为`JAVA_HOME`创建一个指向你的 java 目录的“系统变量”,并将包含 java.exe 的 java bin 文件夹的路径添加到`PATH`环境变量的开头。

当您在控制台会话中或使用 Windows 设置对话框在系统范围内配置了环境变量(这需要重新启动)后，您可以通过运行`java -version`来测试您是否配置了正确的 Java 版本:

```cs
C:\Program Files>java -version
openjdk version "1.8.0_262"
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_262-b10)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.262-b10, mixed mode).

```

如果您决定在每次运行应用程序时都配置`JAVA_HOME`和`PATH`，那么创建一个为您完成工作的批处理文件是很有用的，因此您只需在运行应用程序之前调用它。

### 安装 Java

如果您没有 Apache Spark 可以使用的 Java 版本，我们将需要下载一个 JDK 版本。在本节中，我们将下载并安装 AdoptOpenJDK 8 JDK，因此我们转到发布页面( [`https://adoptopenjdk.net/releases.html`](https://adoptopenjdk.net/releases.html) )并选择

*   open JDK 8(lt)

*   热点

*   Windows 操作系统

*   JDK .msi 文件

这些选项下载一个 MSI 文件，然后您可以安装该文件。如果您下载了这个包，这个包将安装并配置 Java。如果您启动一个新的终端并运行`java -version`，检查现在显示的是正确的版本；如果它没有显示正确的版本，请按照上一节中的步骤操作。

### 下载和配置 Apache Spark

现在你已经有了正确的 Java 工作版本，可以从主页( [`https://spark.apache.org/downloads.html`](https://spark.apache.org/downloads.html) )下载 Apache Spark，选择你想要的版本和包类型，然后下载。

当 Apache Spark 完成下载后，因为该文件是一个 gzipped tar 归档文件，所以您将无法提取它，除非您使用类似 WinZip 或 7-Zip 的文件。我们将首先使用 7-Zip 来解压缩 tar 归档文件:

```cs
>"c:\Program Files\7-Zip\7z.exe" x spark-3.0.0-bin-hadoop2.7.gz

```

7-Zip 将创建一个名为 spark-3.0.0-bin-hadoop2.7 的文件，这是一个 tar 归档文件；然后我们可以再次使用 7-Zip 来获得实际的文件

```cs
>"c:\Program Files\7-Zip\7z.exe" x spark-3.0.0-bin-hadoop2.7 -ospark

```

我们最终得到一个名为 spark 的目录，其中包含我们想要的实际文件夹。如果您切换到实际的 spark 目录并运行`dir`，您应该会看到这些文件和文件夹:

```cs
>dir
 Volume in drive C is Windows
 Volume Serial Number is C024-E5C2

 Directory of C:\Users\ed\Downloads\spark\spark-3.0.0-bin-hadoop2.7

05/30/2020  12:02 AM    <DIR>          .
05/30/2020  12:02 AM    <DIR>          ..
05/30/2020  12:02 AM    <DIR>          bin
05/30/2020  12:02 AM    <DIR>          conf
05/30/2020  12:02 AM    <DIR>          data
05/30/2020  12:02 AM    <DIR>          examples
05/30/2020  12:02 AM    <DIR>          jars
05/30/2020  12:02 AM    <DIR>          kubernetes
05/30/2020  12:02 AM            21,371 LICENSE
05/30/2020  12:02 AM    <DIR>          licenses
05/30/2020  12:02 AM            42,919 NOTICE
05/30/2020  12:02 AM    <DIR>          python
05/30/2020  12:02 AM    <DIR>          R
05/30/2020  12:02 AM             3,756 README.md
05/30/2020  12:02 AM               187 RELEASE
05/30/2020  12:02 AM    <DIR>          sbin
05/30/2020  12:02 AM    <DIR>          yarn

```

下一步是将 spark 文件夹移动到您想要保存它的位置。你可以把代码放在你选择的任何地方；然而，我倾向于创建一个 c:\spark 文件夹，并将每个版本复制到该文件夹中。在这种情况下，对于 Apache Spark 3.0.0，这意味着我的 Spark 文件夹将是 c:\ Spark \ spark-3.0.0-bin-hadoop2.7。与安装和配置 Java 一样，我们还需要创建一个名为`SPARK_HOME`的环境变量，并更新`PATH`以包含 Spark-3 . 0 . 0-bin-Hadoop 2.7 目录中 bin 文件夹的路径。如果您一次只使用一个版本的 Apache Spark，那么您可以通过使用 Windows 设置并搜索“编辑系统环境变量”来更改系统环境变量，并使用控制面板小程序为`SPARK_HOME`创建一个“系统变量”,并编辑`PATH`的环境变量以包含我们之前提取的 bin 文件夹。

如果您确实想创建一个 c:\spark 文件夹，那么这是一个保存批处理脚本的好地方，用来配置您希望使用的 Apache Spark 的每个版本；对于每个版本的 Apache Spark，在 Windows 上，我创建了一个名为`c:\spark\version.cmd`的文件，因此，在本例中为`c:\spark\3-0-0.cmd`，并包含 Java 和 Apache Spark 环境变量:

```cs
SET JAVA_HOME=C:\Program Files\AdoptOpenJDK\jdk-8.0.262.10-hotspot
SET SPARK_HOME=c:\spark\spark-3.0.0-bin-hadoop2.7
SET PATH=%JAVA_HOME%\bin;%SPARK_HOME%\bin;%PATH%

```

### 连接器

当您在 Windows 上运行时，预构建的 Hadoop 版本(允许读写文件)缺少一些核心功能。要读取或写入文件，我们需要一个额外的 exe 调用 winutils。您可以从 Hadoop 源代码手动编译它，也可以下载一个预构建的。如果你想从 Hadoop 源代码构建，那么请看这里的说明: [`https://github.com/steveloughran/winutils`](https://github.com/steveloughran/winutils) 。如果您决定下载一个预建的，那么访问页面( [`https://github.com/cdarlint/winutils`](https://github.com/cdarlint/winutils) )并为您的 Hadoop 版本选择一个 winutils 版本。在前面的例子中，我们下载了 Apache Spark 3 . 0 . 0 版和 Hadoop 版的`spark-3.0.0-bin-hadoop2.7.gz,`；目前，GitHub 上 winutils repo 中 Hadoop 2.7 的最新版本是 2.7.7。如果你进入那个文件夹，你可以下载 winutils.exe。Apache Spark 不需要任何其他文件。配置 Apache Spark 实例最简单的方法是将您的 winutils.exe 文件复制到您的`%SPARK_HOME%\bin`目录，并设置一个名为`HADOOP_HOME`的附加环境变量，如果您只打算使用 Apache Spark 的单一版本，则使用系统环境变量，或者将它包含在您的特定版本批处理脚本中:

```cs
SET JAVA_HOME=C:\Program Files\AdoptOpenJDK\jdk-8.0.262.10-hotspot
SET SPARK_HOME=c:\spark\spark-3.0.0-bin-hadoop2.7
SET HADOOP_HOME=%SPARK_HOME%
SET PATH=%JAVA_HOME%\bin;%SPARK_HOME%\bin;%PATH%

```

### 试验

要验证 Apache Spark 的安装是否正常，运行`spark-shell`，这是一个运行命令的 REPL。如果您运行`spark-shell`，您应该看到显示的 spark 标志和一个命令提示符，您可以在这里运行 spark 命令。我第一次运行 Java 和 Apache Spark 时，还必须允许访问 Windows 防火墙。

```cs
» spark-shell
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1595570221598).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_262)
Type in expressions to have them evaluated.
Type :help for more information

scala>

```

如果您得到`scala>`提示符，那么这是一切正常的好迹象，但是让我们看看是否可以运行 spark 命令来做一些本地处理:

```cs
scala> spark.sql("select * from range(10)").withColumn("ID2", col("ID")).show
+---+---+
| id|ID2|
+---+---+
|  0|  0|
|  1|  1|
|  2|  2|
|  3|  3|
|  4|  4|
|  5|  5|
|  6|  6|
|  7|  7|
|  8|  8|
|  9|  9|
+---+---+

```

我们在这里做的是通过运行一些 spark SQL 调用`range`函数来创建一个`DataFrame`。`range`函数根据我们的要求创建尽可能多的行，然后我们使用 withColumn 创建第二列，其值与第一列中的值相同。最后，我们使用`show`来显示`DataFrame`的内容。

退出 REPL 就像退出维姆；使用`:q`。

在 Windows 上退出 REPL 时，默认情况下，日志记录级别配置为显示可以忽略的错误。如果您看到诸如“ERROR ShutdownHookManager:Exception while deleting Spark temp dir”这样的错误，我们将在下一节中配置日志记录来隐藏这个良性错误。

### 覆盖默认配置

要配置 Apache Spark，我们可以使用位于`%SPARK_HOME%\conf`文件夹中的配置文件。这些是一组控制 Apache Spark 如何工作的文本文件。当您第一次下载 Apache Spark 时，只有模板配置文件，没有实际的配置文件:

```cs
C:\spark\spark-3.0.0-bin-hadoop2.7\conf>dir
 Volume in drive C is Windows
 Volume Serial Number is C024-E5C2

 Directory of C:\spark\spark-3.0.0-bin-hadoop2.7\conf

05/30/2020  12:02 AM    <DIR>          .
05/30/2020  12:02 AM    <DIR>          ..
05/30/2020  12:02 AM               996 docker.properties.template
05/30/2020  12:02 AM             1,105 fairscheduler.xml.template
05/30/2020  12:02 AM             2,025 log4j.properties.template
05/30/2020  12:02 AM             7,801 metrics.properties.template
05/30/2020  12:02 AM               865 slaves.template
05/30/2020  12:02 AM             1,292 spark-defaults.conf.template
05/30/2020  12:02 AM             4,221 spark-env.sh.template

```

如果我们想要配置任何配置文件，我们应该首先将模板文件复制到实际文件中，我们需要 spark-defaults.conf 和 log4j.properties:

```cs
C:\spark\spark-3.0.0-bin-hadoop2.7\conf>copy spark-defaults.conf.template spark-defaults.conf
        1 file(s) copied.
C:\spark\spark-3.0.0-bin-hadoop2.7\conf>copy log4j.properties.template log4j.properties
        1 file(s) copied.

```

然后，您可以使用您最喜欢的编辑器打开配置文件并编辑它们。对于 Apache Spark 的开发实例，我会将控制台日志记录的 log4j 部分改为只显示错误；显示警告会产生很多我们通常可以忽略的输出。如果您更改默认文件内容，如下所示:

```cs
# Set everything to be logged to the console
log4j.rootCategory=INFO, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

```

到

```cs
# Set only errors to be logged to the console
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

```

我们还需要确保这两行出现在 log4j.properties 文件中；否则，我们每次退出 Apache Spark 时都会看到一个错误，这不会导致任何副作用，只会引起混乱和怀疑:

```cs
log4j.logger.org.apache.spark.util.ShutdownHookManager=OFF
log4j.logger.org.apache.spark.SparkEnv=ERROR

```

这通常是一个好的开始。在 spark-defaults.conf 文件中，配置 Apache Spark 可以在您的开发机器上使用多少内存通常是一个好主意；例如，我的机器有 16GB 的 RAM，所以我将 Apache Spark 配置为使用 6GB 的 executor 内存。我还广泛使用了 Databricks 中的 Delta Lake 库，因此通过将它添加到我的默认配置中，我每次都可以访问它，并且不需要记住使用附加库启动 Apache Spark:

```cs
spark.executor.memory=6g
spark.jars.packages=io.delta:delta-core_2.12:0.7.0

```

一旦我更改了配置文件，重新运行 spark-shell 来验证更改是否有效总是值得的。这一次，当我运行 spark-shell 时，我可以看到日志记录变得不那么详细了，并且我的 delta 包已经被加载了:

```cs
C:\spark\spark-3.0.0-bin-hadoop2.7\conf>spark-shell
Ivy Default Cache set to: C:\Users\ed\.ivy2\cache
The jars for the packages stored in: C:\Users\ed\.ivy2\jars
:: loading settings :: url = jar:file:/C:/spark/spark-3.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
io.delta#delta-core_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-01c3fcde-c454-4d1c-a62a-4a6a55bc3838;1.0
        confs: [default]
        found io.delta#delta-core_2.12;0.7.0 in central
        found org.antlr#antlr4;4.7 in central
        found org.antlr#antlr4-runtime;4.7 in central
        found org.antlr#antlr-runtime;3.5.2 in central
        found org.antlr#ST4;4.0.8 in central
        found org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central

        found org.glassfish#javax.json;1.0.4 in central
        found com.ibm.icu#icu4j;58.2 in central
:: resolution report :: resolve 1390ms :: artifacts dl 32ms
        :: modules in use:
        com.ibm.icu#icu4j;58.2 from central in [default]
        io.delta#delta-core_2.12;0.7.0 from central in [default]
        org.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]
        org.antlr#ST4;4.0.8 from central in [default]
        org.antlr#antlr-runtime;3.5.2 from central in [default]
        org.antlr#antlr4;4.7 from central in [default]
        org.antlr#antlr4-runtime;4.7 from central in [default]
        org.glassfish#javax.json;1.0.4 from central in [default]
        -------------------------------------------------------------------
        |                |            modules            ||   artifacts   |
        |      conf      | number| search|dwnlded|evicted|| number|dwnlded|
        -------------------------------------------------------------------
        |     default    |   8   |   0   |   0   |   0   ||  8    |   0   |
        -------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-01c3fcde-c454-4d1c-a62a-4a6a55bc3838
        confs: [default]
        0 artifacts copied, 8 already retrieved (0kB/31ms)
20/07/24 06:10:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http:// win10.jfdetya2p5vexax3rmqk4rrt4a.ax.internal.cloudapp.net:4040
Spark context available as 'sc' (master = local[*], app id = local-1595571047482).
Spark session available as 'spark'.
Welcome to

      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_262)
Type in expressions to have them evaluated.
Type :help for more information.

scala>

```

如果您能够运行 Apache Spark REPL，并且能够运行 Spark 命令，那么您很可能能够运行您的。当我们在下一章创建我们的第一个应用程序时。

## 配置 Apache Spark 和。NET for Apache Spark on Linux(Ubuntu)

在这一节中，我们将介绍如何获得 Apache Spark 运行开发机器的本地实例；一旦我们有了支持的版本的工作安装。NET，我们可以创建我们的第一个。NET 应用程序在下一章。

### 配置已安装的 Java

在安装 Java 之前，有必要检查您是否已经安装并配置了正确的 Java 版本，或者您已经安装了正确的版本，但是配置了单独的版本。要查看 Ubuntu 上有哪些 Java 版本，请使用`update-alternatives`工具:

```cs
$ sudo update-alternatives --config java
There are 2 choices for the alternative java (providing /usr/bin/java).
  Selection    Path                                            Priority   Status
------------------------------------------------------------
* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode
  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode
  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode

```

`update-alternatives`显示我们已经安装了 Java 11 和 Java 8。但是，Java 11 是默认的，它不支持当前最新版本的。NET for Apache Spark，需要 Java 8。

您可以选择通过传入 Java 8 运行时的选择号来更改缺省值，也可以在运行 Apache Spark 应用程序之前创建一个脚本，该脚本总是配置正确的 Java 版本:

```cs
#!/bin/bash

export JAVA_HOME=$(update-java-alternatives --list java-1.8.0-openjdk-amd64 | awk '{print $3}')
export PATH=$JAVA_HOME/bin:$PATH

```

在我们创建了这个 shell 文件之后，我们可以对它进行源代码处理，然后运行 java -version 来检查脚本是否成功运行:

```cs
$ source ./spark.sh
$ java -version
openjdk version "1.8.0_252"
OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)
OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)

```

在这种情况下，我们现在看到我们在终端会话中设置的 Java 是 Java 8，这是正确的。

### 安装 Java

如果您没有 Apache Spark 可以使用的 Java 版本，我们将需要下载一个 JDK 版本。在本节中，我们将使用 OpenJDK 的版本 8；要使用`apt-get`进行安装，请运行`sudo apt-get install openjdk-8-jdk`。

如果您启动一个新的终端并运行`java -version`，检查现在显示的是正确的版本；如果它没有显示正确的版本，请按照上一节中的步骤操作。

### 下载和配置 Apache Spark

现在您已经有了正确的 Java 工作版本，可以下载 Apache Spark 了。进入首页( [`https://spark.apache.org/downloads.html`](https://spark.apache.org/downloads.html) ，选择你想要的版本和包类型，然后下载；我下载并复制我的到我的主目录，然后运行

```cs
$ tar -xvf spark-3.0.0-bin-hadoop2.7.tgz

```

该命令将文件提取到~/ spark-3.0.0-bin-hadoop2.7/，因此下一步是设置几个环境变量:

*   飞奔回家

*   小路

我们将`SPARK_HOME`设置为我们刚刚提取的目录，我将更新我的。bashrc 并更新我们的变量。巴沙尔要包括`"$SPARK_HOME/bin`；确保在更新路径之前设置了`$SPARK_HOME`。

### 测试安装

要验证 Apache Spark 的安装是否正常，运行`spark-shell`，这是一个运行命令的 REPL。如果您运行`spark-shell`，您应该看到显示的 spark 标志和一个命令提示符，您可以在这里运行 spark 命令。

```cs
$ spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1595401509136).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_262)
Type in expressions to have them evaluated.
Type :help for more information.

scala>

```

如果您得到`scala>`提示符，那么这是一切正常的好迹象，但是让我们看看是否可以运行 spark 命令来做一些本地处理:

```cs
scala> spark.sql("select * from range(10)").withColumn("ID2", col("ID")).show
+---+---+
| id|ID2|
+---+---+
|  0|  0|
|  1|  1|
|  2|  2|
|  3|  3|
|  4|  4|
|  5|  5|
|  6|  6|
|  7|  7|
|  8|  8|
|  9|  9|
+---+---+

```

我们在这里做的是通过运行一些 spark SQL 调用`range`函数来创建一个`DataFrame`。`range`函数根据我们的要求创建尽可能多的行，然后我们使用 withColumn 创建第二列，其值与第一列中的值相同。最后，我们使用`show`来显示`DataFrame`的内容。

退出 REPL 就像退出维姆；使用`:q`。

### 覆盖默认配置

要配置 Apache Spark，我们可以使用`$SPARK_HOME/conf`中的配置文件；这些是一组控制 Apache Spark 如何工作的文本文件。当您第一次下载 Apache Spark 时，只有模板配置文件，没有实际的配置文件:

```cs
~/spark-3.0.0-bin-hadoop2.7/conf » ls
docker.properties.template   metrics.properties.template  spark-env.sh.template
fairscheduler.xml.template   slaves.template
log4j.properties.template    spark-defaults.conf.template

```

如果我们想要配置任何配置文件，我们应该首先通过删除将模板文件复制到实际文件中。模板"从文件名的末尾开始:

```cs
$ cp ./spark-defaults.conf.template ./spark-defaults.conf
$ cp ./log4j.properties.template ./log4j.properties

```

然后，您可以使用您最喜欢的编辑器打开配置文件并编辑它们。对于 Apache Spark 的开发实例，我会将控制台日志记录的 log4j 部分改为只显示错误；显示警告会产生很多我们通常可以忽略的输出。如果您更改默认文件内容，如下所示:

```cs
# Set everything to be logged to the console
log4j.rootCategory=INFO, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

```

到

```cs
# Set only errors to be logged to the console
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

```

这通常是一个好的开始。在 spark-defaults.conf 文件中，配置 Apache Spark 可以在您的开发机器上使用多少内存通常是一个好主意；例如，我的机器有 16GB 的 RAM，所以我将 Apache Spark 配置为使用 6GB 的 executor 内存。我还广泛使用了 Databricks 中的 Delta Lake 库，因此通过将它添加到我的默认配置中，我每次都可以访问它，并且不需要记住使用附加库启动 Apache Spark:

```cs
spark.executor.memory=6g
spark.jars.packages=io.delta:delta-core_2.12:0.7.0

```

一旦我更改了配置文件，重新运行 spark-shell 来验证更改是否有效总是值得的。这一次，当我运行 spark-shell 时，我可以看到日志记录变得不那么详细了，并且我的 delta 包已经被加载了:

```cs
$ ./spark-shell

Ivy Default Cache set to: /Users/ed/.ivy2/cache
The jars for the packages stored in: /Users/ed/.ivy2/jars
:: loading settings :: url = jar:file:/Users/ed/spark-3.0.0-bin-without-hadoop/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
io.delta#delta-core_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-4987d518-30f9-4696-ac0e-1b20ed99f224;1.0
      confs: [default]
      found io.delta#delta-core_2.12;0.7.0 in central
      found org.antlr#antlr4;4.7 in central
      found org.antlr#antlr4-runtime;4.7 in local-m2-cache
      found org.antlr#antlr-runtime;3.5.2 in central
      found org.antlr#ST4;4.0.8 in central
      found org.abego.treelayout#org.abego.treelayout.core;1.0.3 in spark-list
      found org.glassfish#javax.json;1.0.4 in central
      found com.ibm.icu#icu4j;58.2 in central
:: resolution report :: resolve 210ms :: artifacts dl 8ms
      :: modules in use:
      com.ibm.icu#icu4j;58.2 from central in [default]
      io.delta#delta-core_2.12;0.7.0 from central in [default]
      org.abego.treelayout#org.abego.treelayout.core;1.0.3 from spark-list in [default]
      org.antlr#ST4;4.0.8 from central in [default]
      org.antlr#antlr-runtime;3.5.2 from central in [default]
      org.antlr#antlr4;4.7 from central in [default]
      org.antlr#antlr4-runtime;4.7 from local-m2-cache in [default]
      org.glassfish#javax.json;1.0.4 from central in [default]
      ---------------------------------------------------------------------
      |                  |            modules            ||   artifacts   |
      |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
      ---------------------------------------------------------------------
      |      default     |   8   |   0   |   0   |   0   ||   8   |   0   |
      ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-4987d518-30f9-4696-ac0e-1b20ed99f224
      confs: [default]
      0 artifacts copied, 8 already retrieved (0kB/7ms)

```

## 点网络工作者配置

在简介中，我们介绍了的体系结构。NET for Apache Spark，这就是。NET 代码使用 TCP 套接字连接到 JVM 代码，因此。NET 代码可以调用 JVM 中的代码。有一个例外，那就是用户定义函数或 UDF。UDF 通过在另一个单独的特定于微软的进程中运行来工作，JVM 连接到该进程并从中来回发送消息。如果我们使用 UDF，那么我们还需要从。NET 并配置一个环境变量来指向这个可执行文件。我们将在第四章中介绍更多关于 UDF 的内容，你可能永远都不需要使用 UDF。

来配置。NET for Apache Spark，所以 UDF 工作，你首先需要访问项目的 releases 页面( [`https://github.com/dotnet/spark/releases`](https://github.com/dotnet/spark/releases) )，选择你要使用的版本和操作系统，下载并解压到某个地方，然后创建`DOTNET_WORKER_DIR`环境变量指向与微软的目录。Spark。工人可执行文件。

## 常见错误疑难解答

在这一节中，我们将讨论运行 Apache Spark 时出现的常见错误。

### 不支持的类文件主要版本

当运行 Apache Spark 命令时，您会看到一条消息“Java . lang . illegalargumentexception:Unsupported class file major version 58”，末尾的数字是不相关的。该消息意味着 Java 类版本与当前运行时不兼容。这通常是由于在不正确的 Java 版本上运行 Apache Spark 造成的。Apache Spark 不验证它运行在哪个版本的 Java 上。Apache Spark 试图运行，如果版本错误，就会失败。使用“`java -version"`检查您配置了哪个版本的 Java，并且在 Apache Spark 输出中，它应该打印出它认为自己使用的是哪个版本的 Java。除非您运行 Apache Spark，并且它输出了适用于您的 Apache Spark 版本的正确 Java 版本，否则您的程序总是会失败。

### 退出 Spark 时出现异常

当在 Windows 上退出 Apache Spark 应用程序时，您会看到“错误关闭 HookManager:删除 Spark 临时目录时出现异常”。这是一个可以安全忽略的错误，如果您用这两行更新% SPARK _ HOME % \ conf \ log4j . properties，那么错误消息将被隐藏:

```cs
log4j.logger.org.apache.spark.util.ShutdownHookManager=OFF
log4j.logger.org.apache.spark.SparkEnv=ERROR

```

### 无法运行 Spark 外壳

如果你在 Windows 上，你试图运行`spark-shell`，但是你得到一个错误“系统找不到指定的路径”，这可能是由`JAVA_HOME`环境变量末尾的分号或者其他一些关于`JAVA_HOME`的错误引起的。为了确保您的`JAVA_HOME`变量是正确的，尝试`dir "%JAVA_HOME%\bin\java.exe"`，如果这没有显示 java.exe 存在，运行`echo %JAVA_HOME%\bin\java.exe`并确保整个路径指向一个 java.exe。

如果 Apache Spark 出现另一个错误，那么 Stack Overflow 是一个提问的好地方，或者如果您联系 Apache Spark 邮件列表，有一个很好的社区可以提供帮助。要查看所有可用的社区帮助，请参见 [`https://spark.apache.org/community.html`](https://spark.apache.org/community.html) 。

## 摘要

要让 Apache Spark 在您的开发机器上运行，需要安装和配置正确的 Java 版本，现在是 Java 8。当。NET for Apache Spark project 支持 Apache Spark 3.0，它将是 Java 11 及其未来版本，很可能支持更高版本的 Java。

整个过程是安装和配置 Java，然后下载和配置 Apache Spark。如果你用的是 Windows，那么你也应该下载 winutils.exe。如果你希望使用用户定义的函数，那么你还需要微软的。承载您的 UDF 代码的. NET 辅助进程。

如果在本章结束时，您可以运行 spark-shell，并且您使用的是正确的 Java 版本，那么您的状态很好，应该会很兴奋地编写您的第一个。NET for Apache Spark 在下一章。